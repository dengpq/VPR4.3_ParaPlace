#include <stdio.h>
#include <math.h>
#include <assert.h>
#include <string.h>
#include <pthread.h>


#include "util.h"
#include "globals.h"
#include "place.h"
#include "read_place.h"
#include "draw.h"
#include "place_and_route.h"
#include "net_delay.h"
#include "path_delay.h"
#include "timing_place_lookup.h"
#include "timing_place.h"
#include "path_delay2.h"
#include "vpr_utils.h"

#define SMALL_NET 4    /* Cut off for incremental bounding box updates. */
/* 4 is fastest -- I checked. */

#define FROM        0  /* What blocks connected to a net has moved? */
#define TO          1
#define FROM_AND_TO 2

#define ERROR_TOL   0.001
#define MAX_MOVES_BEFORE_RECOMPUTE 1000000
/* Attention, for place_parallel.c, the ERROR_TOL *
 * and MAX_MOVES_BEFORE_RECOMPUTE had changed to  *
 * #define ERROR_TOL       0.0025
 * #define MAX_MOVES_BEFORE_RECOMPUTE 50000*/

#define EMPTY -1

#define _XOPEN_SOURCE  600
#define T_CONSTANT_GENERATOR  -1000  /* Essentially -ve infinity */
#define PROB  10
#define TIMING_UPDATE  5
#define PARITION_UPDATE  80  /* why? */

/* Partitioning parameter.
 * The grid will be partitioned into x_partition rows *
 * and y_partition columns.                *
 * Feel free to change the numbers below.  *
 * For 8-threads, I'd like to set 4x2 regions. *
 * FIXME, when I set regions, the number of horizontal regions
 * should >= the number of vertical ones. That is *
 * x_partition[NUM_OF_THREADS] >= y_partition[NUM_OF_THREADS]. */
int x_partition[64] = {
    1, 1, 1, 2, 1, /*  1-5   */
    3, 0, 4, 3, 0, /*  6-10  */
    0, 0, 0, 0, 3, /* 11-15  */
    4, 0, 3, 0, 4, /* 16-20  */
    0, 2, 0, 4, 5, /* 21-25  */
    0, 0, 0, 0, 0, /* 26-30  */
    0, 0, 0, 0, 0, /* 31-35  */
    6, 0, 0, 0, 0, /* 36-40  */
    0, 0, 0, 0, 0, /* 41-45  */
    0, 0, 0, 7, 0, /* 46-50  */
    0, 0, 0, 0, 0, /* 51-55  */
    0, 0, 0, 0, 6, /* 56-60  */
    0, 0, 7, 8   /* 61-64 */
};

int y_partition[64] = {
    1, 2, 3, 2, 5, /*  1-5  */
    2, 0, 2, 3, 0, /*  6-10 */
    0, 0, 0, 0, 5, /* 11-15 */
    4, 0, 6, 0, 5, /* 16-20 */
    0, 11, 0, 6, 5,/* 21-25 */
    0, 0, 0, 0, 0, /* 26-30 */
    0, 0, 0, 0, 0, /* 31-35 */
    6, 0, 0, 0, 0, /* 36-40 */
    0, 0, 0, 0, 0, /* 41-45 */
    0, 0, 0, 7, 0, /* 46-50 */
    0, 0, 0, 0, 0, /* 51-55 */
    0, 0, 0, 0, 10,/* 56-60 */
    0, 0, 9, 8     /* 61-64 */
};


/********************** Variables local to place.c ***************************/
/* [0..num_nets-1]  0 if net never connects to the same blocks more than *
 *  once, otherwise it gives the number of duplicate connections.        */
static int* duplicate_pins;

/* [0..num_nets-1][0..num_unique_blocks-1]  Contains a list of blocks with *
 * no duplicated blocks for ONLY those nets that had duplicates.           */
static int** unique_pin_list;

/* Cost of a net, and a temporary cost of a net used during move assessment. */
static double* net_cost = NULL;      /* double new_cost[0..num_nets-1] */
static double* temp_net_cost = NULL; /* double temp_new_cost[0..num_nets-1] */

/* [0..num_nets-1][1..num_pins-1]. What is the value of the timing   */
/* driven portion of the cost function. These arrays will be set to  */
/* (criticality * Tdel) for each point to point connection. */
static double** point_to_point_timing_cost = NULL;
static double** temp_point_to_point_timing_cost = NULL;


/* [0..num_nets-1][1..num_pins-1]. The value of the Tdel */
/* for each connection in the circuit */
static double**  point_to_point_delay_cost = NULL;
static double**  temp_point_to_point_delay_cost = NULL;


/* [0..num_blocks-1][0..pins_per_clb-1]. Indicates which pin on the net */
/* this blocks corresponds to, this is only required during timing-driven */
/* placement. It is used to allow us to update individual connections on */
/* each net. That is <pin_number, net_number>. */
static int** net_pin_index = NULL;

/* [0..num_nets-1], store the bounding_box coordinates of each net. */
static bbox_t* bb_coords = NULL;

/* [0..num_nets-1], store the number of blocks on each of a net's bounding_box *
 * (to allow efficient updates), respectively.                                 */
static bbox_t* bb_num_on_edges = NULL;

/* Stores the maximum and expected occupancies, plus the cost, of each   *
 * region in the placement.  Used only by the NONLINEAR_CONG cost        *
 * function.  [0..num_region-1][0..num_region-1].  Place_region_x and    *
 * y give the situation for the x and y directed channels, respectively. */
static place_region_t**  place_region_x;
static place_region_t**  place_region_y;

/* Used only with nonlinear congestion.  [0..num_regions]. */
static double*  place_region_bounds_x;
static double*  place_region_bounds_y;

/* The arrays below are used to precompute the inverse of the average   *
 * number of tracks per channel between [subhigh] and [sublow].  Access *
 * them as chan?_place_cost_fac[subhigh][sublow].  They are used to     *
 * speed up the computation of the cost function that takes the length  *
 * of the net bounding box in each dimension, divided by the average    *
 * number of tracks in that direction; for other cost functions they    *
 * will never be used.                                                  */
static double** chanx_place_cost_fac = NULL;
static double** chany_place_cost_fac = NULL;

/* Expected crossing counts for nets with different #'s of pins.  From *
 * ICCAD 94 pp. 690 - 695 (with linear interpolation applied by me).   */
const double cross_count[50] = { /* [0..49] */
    1.0,    1.0,    1.0,    1.0828, 1.1536, 1.2206, 1.2823, 1.3385, 1.3991, 1.4493,
    1.4974, 1.5455, 1.5937, 1.6418, 1.6899, 1.7304, 1.7709, 1.8114, 1.8519, 1.8924,
    1.9288, 1.9652, 2.0015, 2.0379, 2.0743, 2.1061, 2.1379, 2.1698, 2.2016, 2.2334,
    2.2646, 2.2958, 2.3271, 2.3583, 2.3895, 2.4187, 2.4479, 2.4772, 2.5064, 2.5356,
    2.5610, 2.5864, 2.6117, 2.6371, 2.6625, 2.6887, 2.7148, 2.7410, 2.7671, 2.7933
};

/* grid_tile_t** localvert_grid[num_grid_columns+2][num_grid_rows+2] */
grid_tile_t** localvert_grid = NULL;


/********************* Static subroutines local to place.c *******************/
static void initial_placement(pad_loc_t pad_loc_type,
                              char* pad_loc_file);

static void run_main_placement(const placer_opts_t  placer_opts,
                               const annealing_sched_t annealing_sched,
                               const int*        pins_on_block,
                               placer_paras_t*   placer_paras_ptr,
                               double**  old_region_occ_x,
                               double**  old_region_occ_y,
                               double**  net_slack,
                               double**  net_delay,
                               placer_costs_t*   placer_costs_ptr);

static void run_low_temperature_place(const placer_opts_t   placer_opts,
                                      const int*   pins_on_block,
                                      placer_paras_t*  placer_paras_ptr,
                                      double**  old_region_occ_x,
                                      double**  old_region_occ_y,
                                      double**  net_slack,
                                      double**  net_delay,
                                      placer_costs_t*  placer_cost_ptr);

static void perform_timing_analyze(const placer_opts_t* placer_opts_ptr,
                                   double**  net_delay,
                                   double**  net_slack,
                                   placer_paras_t*  placer_paras_ptr,
                                   placer_costs_t*  placer_costs_ptr);

static void recompute_td_cost_after_swap_certain_times(const placer_opts_t*  placer_opts_ptr,
                                                       const int inner_iter,
                                                       double**  net_delay,
                                                       double**  net_slack,
                                                       placer_paras_t*  placer_paras_ptr,
                                                       placer_costs_t*  placer_costs_ptr);

static void compute_timing_driven_cost(const placer_opts_t*  placer_opts_ptr,
                                       const placer_paras_t* placer_paras_ptr,
                                       double** net_slack,
                                       int**    block_pin_to_tnode,
                                       placer_costs_t*  placer_costs_ptr);

static void  update_place_cost_after_max_move_times(const placer_opts_t* placer_opts_ptr,
                                                    placer_costs_t*  placer_costs_ptr);

static void compute_timing_driven_cost_in_outer_loop(const placer_opts_t* placer_opts_ptr,
                                                     placer_costs_t*  placer_costs_ptr);

static void update_place_costs_by_success_sum(const placer_paras_t* placer_paras,
                                              placer_costs_t*  placer_opts_ptr);

/* New change the following functions to global functions due to place_parallel */
static int count_connections(void);

static void compute_net_pin_index_values(void);

static double get_std_dev(int n,
                          double sum_x_squared,
                          double av_x);

static double starting_temperature(const annealing_sched_t annealing_sched,
                                   const int* pins_on_block,
                                   const placer_opts_t   placer_opts,
                                   placer_paras_t* placer_paras_ptr,
                                   double**  old_region_occ_x,
                                   double**  old_region_occ_y,
                                   placer_costs_t* placer_costs_ptr);

static void update_temperature(double* t,
                               double rlim,
                               double success_ratio,
                               annealing_sched_t annealing_sched);

static void update_range_limit(double* rlim,
                               double success_ratio);

static double update_crit_exponent(const placer_opts_t*  placer_opts_ptr,
                                   const placer_paras_t* placer_paras_ptr);

static int exit_crit(double t,
                     double total_cost,
                     annealing_sched_t annealing_sched);

/* FIXME: Try to swap a pair of plbs or io_pads randomly*/
/* Picks some blocks and moves it to another spot. If this spot had occupied *
 * , switch the blocks. Assess the change in cost function, and accept or   *
 * reject the move. If rejected, return 0, else return 1. Pass back the new *
 * value of the cost function. rlim is the range_limit. pins_on_block gives *
 * the number of pins on each type of blocks(improves efficiency). Pins on  *
 * each type of blocks(improves efficiency).                                */
static int try_swap(const placer_paras_t* placer_paras_ptr,
                    const placer_opts_t   placer_opts,
                    const int*  pins_on_block,
                    double**  old_region_occ_x,
                    double**  old_region_occ_y,
                    placer_costs_t*  placer_costs_ptr);

/* I change this function from static to global function. Due to some functions
 * was need by both single-thread and multi-threads */
static void alloc_and_load_placement_structs(const placer_opts_t* placer_opts_ptr,
                                             double*** old_region_occ_x,
                                             double*** old_region_occ_y);

static void free_placement_structs(int place_cost_type,
                                   int num_regions,
                                   double** old_region_occ_x,
                                   double** old_region_occ_y,
                                   placer_opts_t placer_opts);


static void alloc_place_regions(int num_regions);

static void load_place_regions(int num_regions);

static void update_region_occ(int inet, bbox_t* coords,
                              int add_or_sub, int num_regions);

static void save_region_occ(double** old_region_occ_x,
                            double** old_region_occ_y, int num_regions);

static void restore_region_occ(double** old_region_occ_x,
                               double** old_region_occ_y, int num_regions);

static void free_place_regions(int num_regions);


static void alloc_and_load_unique_pin_list(void);

static void free_unique_pin_list(void);


static int find_affected_nets(int* nets_to_update,
                              int* net_block_moved,
                              int from_block,
                              int to_block,
                              int num_of_pins);

static void find_to(int x_from,
                    int y_from,
                    int type,
                    double rlim,
                    int* x_to,
                    int* y_to);

static int assess_swap(double delta_cost, double t);

static double recompute_bb_cost(int place_cost_type,
                                int num_regions);
/********************************************************************
 * The following functions were used for Timing-Driven-Placement  */
static double compute_point_to_point_delay(int inet,
                                           int ipin);

static void update_timing_driven_cost(int from_block,
                                      int to_block,
                                      int num_of_pins);

static void compute_delta_timing_driven_cost(place_algorithm_t place_algo,
                                             int  from_block,
                                             int  to_block,
                                             int  num_of_pins,
                                             double* delta_timing,
                                             double* delta_delay);

/* FIXME: compute timing-driven costs of all signal nets(its all subnets)  *
 * Attention: I found this function was called by NET_TIMING_DRIVEN_PLACE. */
/* Computes the cost(from scratch) due to the Tdels and criticalities on all  *
 * point-to-point connections, we define the timing cost of each connection as *
 * criticality * Tdel.                                                        */
static void compute_timing_driven_cost_by_orig_algo(double* timing_cost,
                                                    double* connection_delay_sum);

/*======    New added by Pengqiu Deng for Timing-Driven Placement   ======*/
extern int num_of_vertexs;
extern vertex_t*  vertexes;
extern int* driver_node_index_of_net;

/* FIXME: This function was used for calculate Timing-Driven_Placement by *
 * PATH algorithm, which noted at "A Novel Net Weighting Algorithm for    *
 * Timing-Driven Placement", Tim Kong, 2002.                              */
static void compute_timing_driven_costs_by_path_algo(double* total_timing_cost,
                                                     double* connection_delay_sum);
/********************     End!     ***********************************/
static double compute_bb_cost(int method,
                              int place_cost_type,
                              int num_regions);

static double nonlinear_cong_cost(int num_regions);

static double get_net_cost(int inet,
                           bbox_t* bb_ptr);

static void get_bb_from_scratch(int inet, bbox_t* coords,
                                bbox_t* num_on_edges);

static void get_non_updateable_bb(int inet,
                                  bbox_t* bb_coord_new);

static void update_bb(int inet,
                      bbox_t* bb_coord_new,
                      bbox_t* bb_edge_new,
                      int xold,
                      int yold,
                      int xnew,
                      int ynew);

static void alloc_and_load_for_fast_cost_update(double place_cost_exp);

static void free_fast_cost_update_structs(void);

static void check_place(const placer_opts_t*  placer_opts_ptr,
                        placer_costs_t* placer_costs_ptr);


/*****************************************************************************
 *       The following function were designed for Parallel Placememt         *
 ****************************************************************************/
static void barrier_polling(const int kthread_id);

static void barrier_polling_reset(void);

static void* try_place_parallel(pthread_data_t* args);

static void  try_place_a_subregion(const int  kthread_id,
                                   const int  krow,
                                   const int  kcol,
                                   const int  prob,
                                   int*  move_counter,
                                   thread_local_common_paras_t*  common_paras_ptr,
                                   thread_local_data_for_swap_t* swap_data_ptr);

/* try_place_parallel directed-called functions */
static void initial_common_paras(pthread_data_t*  input_args,
                                 int*  max_pins_per_fb,
                                 thread_local_common_paras_t* common_paras_ptr);

static void alloc_memory_for_swap_data(const int max_pins_per_fb,
                                       thread_local_common_paras_t*  common_paras_ptr,
                                       thread_local_data_for_swap_t* swap_data_ptr);

static void initial_swap_data(thread_local_common_paras_t*  common_paras_ptr,
                              thread_local_data_for_swap_t*  swap_data_ptr);

static void find_fanin_parallel(const int kthread_id);

static void initial_localvert_grid(void);

static void perform_timing_analyze_parallel(const int thread_id,
                                            double*** net_delay,
                                            pthread_data_t* input_args,
                                            double*  max_delay);

static void compute_net_slack_full(const int kthread_id,
                                   double*** net_slack,
                                   double*   timing_cost,
                                   double*   delay_cost,
                                   double    crit_exponent,
                                   double    max_delay,
                                   pthread_data_t* input_args);

static void calculate_timing_without_update_crit(const int thread_id,
                                                 double* timing_cost,
                                                 double* delay_cost,
                                                 pthread_data_t* input_args);

static void iter_data_update_from_global_to_local_grid(const int kiter,
                                                       pthread_data_t* input_args,
                                                       thread_local_common_paras_t*  common_paras_ptr,
                                                       thread_local_data_for_swap_t* swap_data_ptr);

static void balance_two_consecutive_threads_edge(const int kthread_id);

static void balance_two_consecutive_threads_sinks(const int kthread_id);

static double compute_bb_cost_parallel(const int kstart_net,
                                       const int kend_net);

/* try swap a pair of blocks in each Extend-SubRegion by each thread parallely */
static int  try_swap_parallel(const double kt,
                              const int kthread_id,
                              const int kx_from,
                              const int ky_from,
                              const int kz_from,
                              const int krow,
                              const int kcol,
                              thread_local_common_paras_t*  common_paras_ptr,
                              thread_local_data_for_swap_t* swap_data_ptr);

static int find_affected_nets_parallel(int* nets_to_update,
                                       int* net_block_moved,
                                       int from_block,
                                       int to_block,
                                       int num_of_pins,
                                       double* local_temp_net_cost);

static void get_bb_from_scratch_parallel(int inet,
                                         bbox_t* coords,
                                         bbox_t* num_on_edges,
                                         local_block_t* local_block);

static void get_non_updateable_bb_parallel(int inet,
                                           bbox_t* bb_coord_new,
                                           local_block_t* local_block);

static void compute_delta_td_cost_parallel(const int kfrom_block,
                                           const int kto_block,
                                           int num_of_pins,
                                           double*  delta_timing,
                                           double*  delta_delay,
                                           local_block_t* local_block,
                                           double** local_temp_point_to_point_timing_cost,
                                           double** local_temp_point_to_point_delay_cost);

static int assess_swap_parallel(double delta_c,
                                double t,
                                int local_seed);

static void update_bb_parallel(int inet,
                               bbox_t* local_bb_coord,
                               bbox_t* local_bb_edge,
                               bbox_t* bb_coord_new,
                               bbox_t* bb_edge_new,
                               int xold,
                               int yold,
                               int xnew,
                               int ynew,
                               local_block_t* local_block);

static boolean find_to_block_parallel(int x_from,
                                      int y_from,
                                      block_type_ptr type,
                                      int* x_to,
                                      int* y_to,
                                      int thread_id,
                                      int xmin, int xmax,
                                      int ymin, int ymax);

/* 4 functions for data broadcast*/
static void update_from_local_to_global(local_block_t* local_block,
                                        grid_tile_t** local_grid,
                                        int x_start, int x_end,
                                        int y_start, int y_end);

static void update_from_global_to_local_grid_only(grid_tile_t** local_grid,
                                                  const int x_start,
                                                  const int x_end,
                                                  const int y_start,
                                                  const int y_end);

static void update_local_data_from_global(const int* region_x_boundary,
                                          const int* region_y_boundary,
                                          grid_tile_t** local_grid,
                                          local_block_t* local_block,
                                          const int krow,
                                          const int kcol);

static void update_from_global_to_local_hori(local_block_t* local_block,
                                             grid_tile_t** local_grid,
                                             int x_start, int x_end,
                                             int y_start, int y_end);

static void update_from_global_to_local_vert(local_block_t* local_block,
                                             grid_tile_t** local_grid,
                                             int x_start, int x_end,
                                             int y_start, int y_end);

static void update_t_parallel(double* t,
                              int* inner_iter_num,
                              double range_limit,
                              double success_rat,
                              annealing_sched_t annealing_sched);

/* calculates the time difference*/
static double my_difftime2(struct timeval* start,
                           struct timeval* end);


/*===========================================================================*/
/* Very Important! */
/* Does almost all the work of placing a circuit. Width_fac gives the width of*
 * the widest channel. Place_cost_exp says what exponent the width should be  *
 * taken to when calculating costs. This allows a greater bias for anisotropic*
 * architectures. Place_cost_type determines which cost function is used.     *
 * num_regions is used only the place_cost_type is NONLINEAR_CONG.            */
void try_place(const char*         netlist_file,
               const placer_opts_t       placer_opts,
               const annealing_sched_t   annealing_sched,
               chan_width_distr_t  chan_width_dist,
               router_opts_t       router_opts,
               detail_routing_arch_t det_routing_arch,
               segment_info_t*   segment_inf,
               timing_info_t     timing_inf,
               subblock_data_t*  subblock_data_ptr)
{
    /* FIXME, according to "Timing-Driven Placement for FPGAs", before T-VPlace *
     * run, it allocate the Tdel lookup matrix for path-timing-driven placement */
    double** net_slack = NULL; /* FIXME */
    double** net_delay = NULL; /* FIXME */
    double** remember_net_delay_original_ptr = NULL; /*used to free net_delay if it is re-assigned*/
    if (placer_opts.place_algorithm == NET_TIMING_DRIVEN_PLACE
          || placer_opts.place_algorithm == PATH_TIMING_DRIVEN_PLACE
          /* new added for support PATH Timing-Driven Placement */
          || placer_opts.place_algorithm == NEW_TIMING_DRIVEN_PLACE
          || placer_opts.enable_timing_computations) {
        /*do this before the initial placement to avoid messing up the initial placement */
        alloc_and_load_timing_graph(placer_opts,
                                    timing_inf,
                                    *subblock_data_ptr);
        net_slack = alloc_net_slack();
        assert(net_slack != NULL);

        alloc_delay_lookup_matrixes_and_criticalities(placer_opts,
                                                      *subblock_data_ptr,
                                                      chan_width_dist,
                                                      timing_inf,
                                                      router_opts,
                                                      det_routing_arch,
                                                      segment_inf,
                                                      &net_delay);
        remember_net_delay_original_ptr = net_delay;
    } /* end of create data structures(timing_graph) needed by TDP */


    /***************************************************************************
     * Due to VPR4.3 had a lot of paramertes, it was so sophiscated that       *
     * disturbed for a long time. I new added 2 data structure: placer_costs_t *
     * and placer_paras_t. It will simplified my working.                      *
     **************************************************************************/
    placer_paras_t*  placer_paras_ptr = init_placer_paras();
    placer_paras_ptr->m_width_factor = placer_opts.place_chan_width;

    if (placer_opts.pad_loc_type == FREE) { /* the io pad can place freely */
        placer_paras_ptr->m_fixed_pins = FALSE;
    } else {
        placer_paras_ptr->m_fixed_pins = TRUE;
    }

    init_channel_t(placer_paras_ptr->m_width_factor,
                   chan_width_dist);

    double**  old_region_occ_x = NULL;
    double**  old_region_occ_y = NULL;
    alloc_and_load_placement_structs(&placer_opts,
                                     &old_region_occ_x,
                                     &old_region_occ_y);

    /* FIXME, run initial_placement, I understand it! */
    initial_placement(placer_opts.pad_loc_type,
                      placer_opts.pad_loc_file);

    init_draw_coords((double)placer_paras_ptr->m_width_factor);

    /* Storing the number of pins on each type of blocks makes the swap routine *
     * slightly more efficient.                                                */
    int pins_on_block[3];  /* 0: CLB_TYPE, 1: OUTPAD_TYPE, 2: INPAD_TYPE */
    pins_on_block[CLB_TYPE] = pins_per_clb;
    pins_on_block[OUTPAD_TYPE] = 1;
    pins_on_block[INPAD_TYPE] = 1;

    placer_costs_t*  placer_costs_ptr = init_placer_costs();
    int  inet = -1;
    int  ipin = -1;
    /* Gets initial cost and loads bounding boxes. */
    placer_costs_ptr->m_bb_cost = compute_bb_cost(NORMAL,
                                                  placer_opts.place_cost_type,
                                                  placer_opts.num_regions);
    /*=========   Now compute initial_cost after initial placement    ========*/
    placer_paras_ptr->m_outer_crit_iter_count = 0;
    if (placer_opts.place_algorithm == NET_TIMING_DRIVEN_PLACE
          || placer_opts.place_algorithm == PATH_TIMING_DRIVEN_PLACE
          || placer_opts.place_algorithm == NEW_TIMING_DRIVEN_PLACE) {
        placer_paras_ptr->m_crit_exponent = placer_opts.td_place_exp_first;

        compute_net_pin_index_values();

        placer_paras_ptr->m_num_connections = count_connections();

        printf("\nThere are %d point to point connections in this circuit\n\n",
                placer_paras_ptr->m_num_connections);

        if (placer_opts.place_algorithm == NET_TIMING_DRIVEN_PLACE) {
            for (inet = 0; inet < num_nets; ++inet) {
                for (ipin = 1; ipin < net[inet].num_pins; ++ipin) {
                    timing_place_crit[inet][ipin] = 0.0; /*dummy crit values*/
                } /* <source, sink> */
            }
            /* first pass gets delay_cost, which is used in criticality computations *
             * in the next call to compute_timing_driven_cost_by_orig_algo. */
            compute_timing_driven_cost_by_orig_algo(&placer_costs_ptr->m_timing_cost,
                                                    &placer_costs_ptr->m_delay_cost);

            /* Used for computing criticalities, but why did it subdivide *
             * num_connections? (FIXME)                                   */
            placer_paras_ptr->m_place_delay_value =
                placer_costs_ptr->m_delay_cost / placer_paras_ptr->m_num_connections;

            /* For NET_TIMING_DRIVEN_PLACE, all subnets of a net had same Tdel  *
             * value. so double net_delay[][] had same value for all connections. */
            load_constant_net_delay(net_delay,
                                    placer_paras_ptr->m_place_delay_value);
        } else {
            placer_paras_ptr->m_place_delay_value = 0;
        }

        /* this keeps net_delay up to date with the same values that *
         * the placer is using point_to_point_delay_cost is computed *
         * each time that compute_timing_driven_cost_by_orig_algo is *
         * called, and is also updated after any swap is accepted.   *
         * point_to_point_delay was pin_to_pin_delay.                */
        if (placer_opts.place_algorithm == PATH_TIMING_DRIVEN_PLACE
              || placer_opts.place_algorithm == NEW_TIMING_DRIVEN_PLACE) {
            net_delay = point_to_point_delay_cost;
        }

        /* Initialize all edges' Tdel(Tdel) in Timing_Analyze_Graph. */
        load_timing_graph_net_delays(net_delay);
        /* Compute each vertexes's req_time and arr_time time, and calculate the *
         * max_delay in critical path.                                           */
        placer_paras_ptr->m_max_delay =
            calc_all_vertexs_arr_req_time(placer_paras_ptr->m_place_delay_value);

        compute_net_slacks(net_slack);

        compute_timing_driven_cost(&placer_opts,
                                   placer_paras_ptr,
                                   net_slack,
                                   block_pin_to_tnode,
                                   placer_costs_ptr);

        placer_costs_ptr->m_inverse_prev_bb_cost = 1 / placer_costs_ptr->m_bb_cost;
        placer_costs_ptr->m_inverse_prev_timing_cost =
                                1 / placer_costs_ptr->m_timing_cost;

        placer_paras_ptr->m_outer_crit_iter_count = 1;
        /* our new cost function uses normalized values of bb_cost and timing_cost,*
         * the value of cost will be reset to 1 at each temperature when       *
         * TIMING_DRIVEN_PLACE is true.                                        */
        placer_costs_ptr->m_total_cost = 1.0;
    } else { /*BOUNDING_BOX_PLACE*/
        placer_costs_ptr->m_total_cost = placer_costs_ptr->m_bb_cost;
        placer_costs_ptr->m_timing_cost = placer_costs_ptr->m_delay_cost = 0.0;
        placer_costs_ptr->m_inverse_prev_bb_cost = placer_costs_ptr->m_inverse_prev_timing_cost = 0.0;

        placer_paras_ptr->m_place_delay_value = placer_paras_ptr->m_max_delay = 0.0;
        placer_paras_ptr->m_crit_exponent = 0.0;
        placer_paras_ptr->m_outer_crit_iter_count = placer_paras_ptr->m_num_connections = 0;
    }
    /*-------------    Compute initial placemnt cost end    ---------------- */

    /* move_limit_timis = 10*(num_of_clb+num_of_io)^(4/3), it used to inner loop of VPR */
    /* Sometimes I want to run the router with a random placement. Avoid using *
     * 0 moves to stop division by 0 and 0 length vector problems, by setting  *
     * move_limit to 1 (which is still too small to do any significant optimization).  */
    placer_paras_ptr->m_move_limit = (int)(annealing_sched.inner_num *
                                             pow(num_blocks, 1.3333));
    if (placer_paras_ptr->m_move_limit <= 0) {
        placer_paras_ptr->m_move_limit = 1;
    }

    if (placer_opts.inner_loop_recompute_divider != 0) { /* its default value was 0 */
        placer_paras_ptr->m_inner_recompute_limit =
                (int)(0.5 + (double)placer_paras_ptr->m_move_limit /
                              (double)placer_opts.inner_loop_recompute_divider);
    } else { /*don't do an inner recompute */
        placer_paras_ptr->m_inner_recompute_limit = placer_paras_ptr->m_move_limit + 1;
    }


    /* range_limit was used in timing-driven placement for exponent computation*/
    placer_paras_ptr->m_range_limit = (double)max(num_grid_columns,
                                                  num_grid_rows);
    const double first_rlim = placer_paras_ptr->m_range_limit;
    placer_paras_ptr->m_final_rlim = 1.0;
    placer_paras_ptr->m_inverse_delta_rlim =
                    1 / (first_rlim - placer_paras_ptr->m_final_rlim);

    /* FIXME: initial start_temperature */
    placer_paras_ptr->m_temper = starting_temperature(annealing_sched,
                                                      pins_on_block,
                                                      placer_opts,
                                                      placer_paras_ptr,
                                                      old_region_occ_x,
                                                      old_region_occ_y,
                                                      placer_costs_ptr);

    printf("Initial Placement Cost: %g bb_cost: %g td_cost: %g delay_cost: %g\n\n",
           placer_costs_ptr->m_total_cost,
           placer_costs_ptr->m_bb_cost,
           placer_costs_ptr->m_timing_cost,
           placer_costs_ptr->m_delay_cost);

#ifndef SPEC
    printf("%11s  %10s %11s  %11s  %11s %11s  %11s %9s %8s  %7s  %7s  %10s  %7s\n",
           "T", "Cost", "Av. BB Cost", "Av. TD Cost", "Av Tot Del", "P_to_P Del",
           "crit_delay", "Ac Rate", "Std Dev", "R limit", "Exp", "Tot. Moves",
           "Alpha");
    printf("%11s  %10s %11s  %11s  %11s %11s  %11s %9s %8s  %7s  %7s  %10s  %7s\n",
           "--------", "----------", "-----------", "-----------", "---------",
           "----------", "-----", "-------", "-------", "-------", "-------",
           "----------", "-----");
#endif
    char msg[BUFSIZE] = "";
    sprintf(msg, "Initial Placement.  Cost: %g  BB Cost: %g  TD Cost %g  Delay Cost: %g "
            "\t max_delay %g Channel Factor: %d",
            placer_costs_ptr->m_total_cost,
            placer_costs_ptr->m_bb_cost,
            placer_costs_ptr->m_timing_cost,
            placer_costs_ptr->m_delay_cost,
            placer_paras_ptr->m_max_delay,
            placer_paras_ptr->m_width_factor);
    update_screen(MAJOR, msg, PLACEMENT, FALSE);

    /*************  MAIN SIMULATED ANNEANLING PlACEMENT STAGE  ****************/
    run_main_placement(placer_opts,
                       annealing_sched,
                       pins_on_block,
                       placer_paras_ptr,
                       old_region_occ_x,
                       old_region_occ_y,
                       net_slack,
                       net_delay,
                       placer_costs_ptr);
    /*****************  END OF MAIN PLACEMENT STAGE   ******************/


    /* Now run low-temperature Simulated-Annealing placement! */
    printf("Attention, then will run low-temperature SA-Based Placement.......\n");
    run_low_temperature_place(placer_opts,
                              pins_on_block,
                              placer_paras_ptr,
                              old_region_occ_x,
                              old_region_occ_y,
                              net_slack,
                              net_delay,
                              placer_costs_ptr);
    /* Low temperature SA-based Placement end! */

#ifndef SPEC
    printf("After low-temperature SA-Based Placement, print the result......\n");
    printf("%11.5g  %10.6g %11.6g  %11.6g  %11.6g %11.6g %11.4g %9.4g %8.3g  %7.4g  %7.4g  %10d  \n\n",
           placer_paras_ptr->m_temper,
           placer_costs_ptr->m_av_cost,
           placer_costs_ptr->m_av_bb_cost,
           placer_costs_ptr->m_av_timing_cost,
           placer_costs_ptr->m_av_delay_cost,
           placer_paras_ptr->m_place_delay_value,
           placer_paras_ptr->m_max_delay,
           placer_paras_ptr->m_success_ratio,
           placer_paras_ptr->m_std_dev,
           placer_paras_ptr->m_range_limit,
           placer_paras_ptr->m_crit_exponent,
           placer_paras_ptr->m_total_iter);

    printf("For low-temperature SA-Based Placment, success_sum = %d,\
            total_iter = %10d, success_ratio = %9.4g\n",
            placer_paras_ptr->m_success_sum,
            placer_paras_ptr->m_total_iter,
            placer_paras_ptr->m_success_ratio);
#endif

#ifdef VERBOSE
    dump_clbs();
#endif

    check_place(&placer_opts,
                placer_costs_ptr);

    if (placer_opts.enable_timing_computations &&
            placer_opts.place_algorithm == BOUNDING_BOX_PLACE) {
        /*need this done since the timing data has not been kept up to date*
         *in bounding_box mode */
        for (inet = 0; inet < num_nets; ++inet)
            for (ipin = 1; ipin < net[inet].num_pins; ++ipin) {
                timing_place_crit[inet][ipin] = 0; /*dummy crit values*/
            }
        /*computes point_to_point_delay_cost*/
        compute_timing_driven_cost_by_orig_algo(&placer_costs_ptr->m_timing_cost,
                                                &placer_costs_ptr->m_delay_cost);
    }

    double place_est_crit = 0.0;
    if (placer_opts.place_algorithm == NET_TIMING_DRIVEN_PLACE
          || placer_opts.place_algorithm == PATH_TIMING_DRIVEN_PLACE
          /* new added for supporting PATH algorithm */
          || placer_opts.place_algorithm == NEW_TIMING_DRIVEN_PLACE
          || placer_opts.enable_timing_computations) {
        /* this makes net_delay up to date with    *
         * the same values that the placer is using*/
        net_delay = point_to_point_delay_cost;
        load_timing_graph_net_delays(net_delay);

        place_est_crit = calc_all_vertexs_arr_req_time(0);

        compute_net_slacks(net_slack);

#ifdef PRINT_SINK_DELAYS
        print_sink_delays("Placement_Sink_Delays.echo");
#endif

#ifdef PRINT_NET_SLACKS
        print_net_slack("Placement_Net_Slacks.echo", net_slack);
#endif

        char crit_path_file_name[48] = "";
        strcpy(crit_path_file_name, netlist_file);
        strcat(crit_path_file_name, "-Place_Crit_Path.echo");
        print_critical_path(crit_path_file_name);
        printf("Placement Estimated Crit Path Delay: %g\n\n", place_est_crit);
    } /* end of  TDP */

    sprintf(msg, "Placement. Cost: %g, bb_cost: %g, td_cost: %g, Channel Factor: %d, critical_delay: %g",
            placer_costs_ptr->m_total_cost,
            placer_costs_ptr->m_bb_cost,
            placer_costs_ptr->m_timing_cost,
            placer_paras_ptr->m_width_factor,
            placer_paras_ptr->m_max_delay);
    printf("Placement. Cost: %g, bb_cost: %g, timing_cost: %g, delay_cost: %g\n",
           placer_costs_ptr->m_total_cost,
           placer_costs_ptr->m_bb_cost,
           placer_costs_ptr->m_timing_cost,
           placer_costs_ptr->m_delay_cost);
    update_screen(MAJOR,
                  msg,
                  PLACEMENT,
                  FALSE);

    printf("Total moves attempted: %d.\n", placer_paras_ptr->m_total_iter);
    if (placer_opts.place_algorithm == NET_TIMING_DRIVEN_PLACE
          || placer_opts.place_algorithm == PATH_TIMING_DRIVEN_PLACE
          /* new added for supporting PATH timing-driven placement */
          || placer_opts.place_algorithm == NEW_TIMING_DRIVEN_PLACE
          || placer_opts.enable_timing_computations) {
        net_delay = remember_net_delay_original_ptr;
        free_placement_structs(placer_opts.place_cost_type,
                               placer_opts.num_regions,
                               old_region_occ_x,
                               old_region_occ_y,
                               placer_opts);
        free_lookups_and_criticalities(&placer_opts,
                                       &net_delay,
                                       &net_slack);
    }

    /* Do not forget these 2 type */
    free_placer_paras(placer_paras_ptr);
    free_placer_costs(placer_costs_ptr);
} /* end of try_place() */


static void initial_placement(pad_loc_t pad_loc_type,
                              char* pad_loc_file) /* FIXME */
{
    /* Randomly places the blocks to create an initial placement. */
    int max_value = max(num_grid_columns * num_grid_rows,
                          2 * (num_grid_columns + num_grid_rows));
    pos = (struct s_pos*)my_malloc(max_value * sizeof(struct s_pos));

    /* Initialize all occupancy to zero. */
    int i = -1;
    int j = -1;
    for (i = 0; i <= num_grid_columns + 1; ++i) {
        for (j = 0; j <= num_grid_rows + 1; ++j) {
            clb_grids[i][j].m_usage = 0;
        } /* clb[rows][columns] described FPGA Architecture */
    }  /* clb[0~(1-num_grid_columns)~num_grid_columns+1][0~(1-num_grid_rows)~num_grid_rows+1] */

    /* initialize all clb location from left to right, from bottom to top */
    int count = 0;
    for (i = 1; i <= num_grid_columns; ++i) {  /* from left to right */
        for (j = 1; j <= num_grid_rows; ++j) { /* from bottom to top */
            pos[count].x = i;
            pos[count].y = j;
            ++count; /* total num_grid_columns * num_grid_rows clbs clb locations */
        }
    }

    int iblk = -1;
    for (iblk = 0; iblk < num_blocks; ++iblk) { /* num_blocks = plbs + io_blocks */
        if (blocks[iblk].type == CLB_TYPE) { /* only place CLBs in center */
            int choice = my_irand(count - 1); /* choice >= 1 && choice <= count-1*/
            clb_grids[pos[choice].x][pos[choice].y].u.blocks = iblk;
            clb_grids[pos[choice].x][pos[choice].y].m_usage = 1;

            /* Ensure randomizer doesn't pick this blocks again */
            pos[choice] = pos[count - 1]; /* overwrite used blocks position */
            /* I think it should be pos[count-1] = pos[choice] */
            --count;
        }
    }

    /* Now do the io blocks around the periphery */
    if (pad_loc_type == USER) {
        read_user_pad_loc(pad_loc_file);
    } else {
        /*========   Now place the I/O pads randomly. ========*/
        count = 0;
        for (i = 1; i <= num_grid_columns; ++i) {
            /* initial I/O pad locations at BOTTEOM side*/
            pos[count].x = i;
            pos[count].y = 0;
            /* initial I/O pad locations at TOP side */
            pos[count + 1].x = i;
            pos[count + 1].y = num_grid_rows + 1;
            count += 2;
        }

        for (j = 1; j <= num_grid_rows; ++j) {
            /* initial I/O pad locations at LEFT side */
            pos[count].x = 0;
            pos[count].y = j;
            /* initial I/O pad locations at RIGHT side */
            pos[count + 1].x = num_grid_columns + 1;
            pos[count + 1].y = j;
            count += 2;
        }
        /* current time, the count == num_of_io_pads */
        for (iblk = 0; iblk < num_blocks; ++iblk) {
            if (blocks[iblk].type == INPAD_TYPE || blocks[iblk].type == OUTPAD_TYPE) {
                int choice = my_irand(count - 1);

                int isubblk = clb_grids[pos[choice].x][pos[choice].y].m_usage;

                clb_grids[pos[choice].x][pos[choice].y].u.io_blocks[isubblk] = iblk;
                ++(clb_grids[pos[choice].x][pos[choice].y].m_usage);
                /* In an I/O pad location of FPGA chip, it may accommodate more
                 * than 1 I/O pad. */
                if (clb_grids[pos[choice].x][pos[choice].y].m_usage == io_ratio) {
                    /* Ensure randomizer doesn't pick this blocks again */
                    pos[choice] = pos[count - 1]; /* overwrite used blocks position */
                    /* the "choice" location had used, so don't choose it again */
                    --count;
                }
            }
        }
    }    /* End randomly place IO_TYPE blocks branch of if */

    /* All the blocks are placed now. Make the blocks array agree with the *
     * clb array.                                                         */
    int k = 0;
    for (i = 0; i <= num_grid_columns + 1; ++i) {
        for (j = 0; j <= num_grid_rows + 1; ++j) {
            if (clb_grids[i][j].type == CLB_TYPE && clb_grids[i][j].m_usage == 1) {
                blocks[clb_grids[i][j].u.blocks].x = i;
                blocks[clb_grids[i][j].u.blocks].y = j;
            } else if (clb_grids[i][j].type == IO_TYPE) {
                for (k = 0; k < clb_grids[i][j].m_usage; ++k) {
                    blocks[clb_grids[i][j].u.io_blocks[k]].x = i;
                    blocks[clb_grids[i][j].u.io_blocks[k]].y = j;
                }
            } else {
                /* No operation */
            }
        }
    }

#ifdef VERBOSE
    printf("At end of initial_placement.\n");
    dump_clbs();
#endif
    free(pos);
} /* end of void initial_placement() */

static void run_main_placement(const placer_opts_t  placer_opts,
                               const annealing_sched_t annealing_sched,
                               const int*        pins_on_block,
                               placer_paras_t*   placer_paras_ptr,
                               double**  old_region_occ_x,
                               double**  old_region_occ_y,
                               double**  net_slack,
                               double**  net_delay,
                               placer_costs_t*   placer_costs_ptr)
{
    int  inner_iter = 0;
    int moves_since_cost_recompute = 0;
    placer_paras_ptr->m_total_iter = 0;
    char msg[BUFSIZE] = "";
    while (exit_crit(placer_paras_ptr->m_temper,
                     placer_costs_ptr->m_total_cost,
                     annealing_sched) == 0) { /* FIXME: outer loop of VPR */
        if (placer_opts.place_algorithm == NET_TIMING_DRIVEN_PLACE
              || placer_opts.place_algorithm == PATH_TIMING_DRIVEN_PLACE
              /* New added for support PATH Timing-Driven Placement */
              || placer_opts.place_algorithm == NEW_TIMING_DRIVEN_PLACE) {
            placer_costs_ptr->m_total_cost = 1.0;
        }

        placer_costs_ptr->m_av_cost = placer_costs_ptr->m_av_bb_cost = 0.0;
        placer_costs_ptr->m_av_timing_cost = placer_costs_ptr->m_av_delay_cost = 0.0;
        placer_paras_ptr->m_sum_of_squares = placer_paras_ptr->m_success_sum = 0;

        /*------   First running timing_analyze before try_swap() in outer loop ------*/
        /* for timing_driven_placement, outer_crit_iter_count initial as 1 */
        const int kouter_crit_iter_count = placer_paras_ptr->m_outer_crit_iter_count;
        const int krecompute_crit_iter = placer_opts.recompute_crit_iter;
        if ((placer_opts.place_algorithm == NET_TIMING_DRIVEN_PLACE
              || placer_opts.place_algorithm == PATH_TIMING_DRIVEN_PLACE
              /* new added for supporting PATH Timing-Driven Placement */
              || placer_opts.place_algorithm == NEW_TIMING_DRIVEN_PLACE)
              && (kouter_crit_iter_count >= krecompute_crit_iter
                    || placer_opts.inner_loop_recompute_divider != 0)) {
            perform_timing_analyze(&placer_opts,
                                   net_slack,
                                   net_delay,
                                   placer_paras_ptr,
                                   placer_costs_ptr);
        } /* end of placement_algorithm == TIMING_DRIVEN_PLACE */
        /*----   Run Timing Analyze before try_swap() in Outer Loop OK! -----*/

        ++(placer_paras_ptr->m_outer_crit_iter_count);
        /* at each temperature change, we update these values to be used  *
         * for normalizing the tradeoff between timing and wirelength(bb) */
        placer_costs_ptr->m_inverse_prev_bb_cost = 1 / placer_costs_ptr->m_bb_cost;
        placer_costs_ptr->m_inverse_prev_timing_cost = 1 / placer_costs_ptr->m_timing_cost;

        placer_paras_ptr->m_inner_crit_iter_count = 1;
        const int move_limit = placer_paras_ptr->m_move_limit;
        for (inner_iter = 0; inner_iter < move_limit; ++inner_iter) {
            /* FIXME: try to swap a pair of clbs or io pads randomly */
            if (try_swap(placer_paras_ptr,
                         placer_opts,
                         pins_on_block,
                         old_region_occ_x,
                         old_region_occ_y,
                         placer_costs_ptr) == 1) {
                ++(placer_paras_ptr->m_success_sum);
                placer_costs_ptr->m_av_cost += placer_costs_ptr->m_total_cost;
                placer_costs_ptr->m_av_bb_cost += placer_costs_ptr->m_bb_cost;
                placer_costs_ptr->m_av_delay_cost += placer_costs_ptr->m_delay_cost;
                placer_costs_ptr->m_av_timing_cost += placer_costs_ptr->m_timing_cost;
                placer_paras_ptr->m_sum_of_squares +=
                    placer_costs_ptr->m_total_cost * placer_costs_ptr->m_total_cost;
            } /* -------------     end of try_swap() success   -------------*/

            /*--------------   Update Timing-Driven Placement Cost  After swap ------------*/
            if (placer_opts.place_algorithm == NET_TIMING_DRIVEN_PLACE
                  || placer_opts.place_algorithm == PATH_TIMING_DRIVEN_PLACE
                  /* New added for supporting PATH Timing-Driven Placement */
                  || placer_opts.place_algorithm == NEW_TIMING_DRIVEN_PLACE) {
                recompute_td_cost_after_swap_certain_times(&placer_opts,
                                                           inner_iter,
                                                           net_delay,
                                                           net_slack,
                                                           placer_paras_ptr,
                                                           placer_costs_ptr);
            } /* end of update timing-cost after try_swap() */

#ifdef VERBOSE
            /* Attention, when I use the following codes, VPR will call  *
             * compute_bb_cost() as frequency as try_swap(), it will result *
             * in VPR costing 4 or 5 times long than origin algorithm. So*
             * Do not use the following codes!!!                         */
            if (fabs(placer_costs_ptr->m_bb_cost - compute_bb_cost(CHECK,
                                                                   placer_opts.place_cost_type,
                                                                   placer_opts.num_regions))
                    > placer_costs_ptr->m_bb_cost * ERROR_TOL) {
                exit(1);
            }
#endif
        } /* end of INNER LOOP OF VPR */

        /* Lines below prevent too much round-off error from accumulating *
         * in the cost over many iterations. This round-off can lead to   *
         * error checks failing because the cost is different from what   *
         * you get when you recompute from scratch.Update both wirelength *
         * cost and timing_cost.                                          */
        moves_since_cost_recompute += move_limit;
        if (moves_since_cost_recompute > MAX_MOVES_BEFORE_RECOMPUTE) { /* > 10^6 */
            update_place_cost_after_max_move_times(&placer_opts,
                                                   placer_costs_ptr);
            moves_since_cost_recompute = 0;
        } /* update placement_cost after MAX_MOVES_BEFORE_RECOMPUTE in inner_loop */

        placer_paras_ptr->m_total_iter += move_limit;
        placer_paras_ptr->m_success_ratio =
                    ((double)placer_paras_ptr->m_success_sum) / move_limit;

        update_place_costs_by_success_sum(placer_paras_ptr,
                                          placer_costs_ptr);
        /* std_dev was standard deviation. */
        placer_paras_ptr->m_std_dev = get_std_dev(placer_paras_ptr->m_success_sum,
                                                  placer_paras_ptr->m_sum_of_squares,
                                                  placer_costs_ptr->m_av_cost);

        /* FIXME: update_temperature */
        update_temperature(&placer_paras_ptr->m_temper,
                           placer_paras_ptr->m_range_limit,
                           placer_paras_ptr->m_success_ratio,
                           annealing_sched);

        sprintf(msg, "Cost: %g  BB Cost %g  TD Cost %g  Temperature: %g  Max_Delay: %g",
                placer_costs_ptr->m_total_cost,
                placer_costs_ptr->m_bb_cost,
                placer_costs_ptr->m_timing_cost,
                placer_paras_ptr->m_temper,
                placer_paras_ptr->m_max_delay);
        update_screen(MINOR,
                      msg,
                      PLACEMENT,
                      FALSE);

        /* FIXME: update range_limit */
        update_range_limit(&placer_paras_ptr->m_range_limit,
                           placer_paras_ptr->m_success_ratio);

        if (placer_opts.place_algorithm == NET_TIMING_DRIVEN_PLACE
              || placer_opts.place_algorithm == PATH_TIMING_DRIVEN_PLACE
              /* new added for supporting PATH timing-driven placement */
              || placer_opts.place_algorithm == NEW_TIMING_DRIVEN_PLACE) {
            /* FIXME: update timing_critical_exponent */
            placer_paras_ptr->m_crit_exponent = update_crit_exponent(&placer_opts,
                                                                     placer_paras_ptr);
        }

#ifdef VERBOSE
        dump_clbs();
#endif
    } /* FIXME:  end of VPR outer loop */
}  /* end of void run_main_placement(const placer_opts_t  placer_opts,..) */

static void  run_low_temperature_place(const placer_opts_t  placer_opts,
                                       const int*  pins_on_block,
                                       placer_paras_t*  placer_paras_ptr,
                                       double**  old_region_occ_x,
                                       double**  old_region_occ_y,
                                       double**  net_slack,
                                       double**  net_delay,
                                       placer_costs_t*  placer_costs_ptr)
{
    placer_paras_ptr->m_temper = 0.0; /* freeze out */
    placer_costs_ptr->m_av_cost = placer_costs_ptr->m_av_bb_cost = 0.0;
    placer_costs_ptr->m_av_timing_cost = placer_costs_ptr->m_av_delay_cost = 0.0;
    placer_paras_ptr->m_success_sum = placer_paras_ptr->m_sum_of_squares = 0.0;

    /* before run low-temperature placement, it should do timing_analyze! */
    const int kouter_crit_iter_count = placer_paras_ptr->m_outer_crit_iter_count;
    const int krecompute_crit_iter = placer_opts.recompute_crit_iter; /* 1 */
    if ((placer_opts.place_algorithm == NET_TIMING_DRIVEN_PLACE
          || placer_opts.place_algorithm == PATH_TIMING_DRIVEN_PLACE
          /* new added for supporting PATH timing-driven placement */
          || placer_opts.place_algorithm == NEW_TIMING_DRIVEN_PLACE)
          && (kouter_crit_iter_count >= krecompute_crit_iter
                || placer_opts.inner_loop_recompute_divider != 0)) {
            /*at each temperature change we update these values to be used   */
            /*for normalizing the tradeoff between timing and wirelength(bb) */
        perform_timing_analyze(&placer_opts,
                               net_slack,
                               net_delay,
                               placer_paras_ptr,
                               placer_costs_ptr);
    } /* end of running timing-analyze before low-temperature SA-based placement */

    ++(placer_paras_ptr->m_outer_crit_iter_count);

    placer_costs_ptr->m_inverse_prev_bb_cost = 1 / placer_costs_ptr->m_bb_cost;
    placer_costs_ptr->m_inverse_prev_timing_cost = 1 / placer_costs_ptr->m_timing_cost;

    printf("Now low-temperature SA-Based Placement......\n");
    placer_paras_ptr->m_inner_crit_iter_count = 1; /* count running inner-loop times */
    /* TODO Q: After SA-based loop finished, why did the loop occurred here? */
    const int move_limit = placer_paras_ptr->m_move_limit;
    int  inner_iter = -1;
    for (inner_iter = 0; inner_iter < move_limit; ++inner_iter) {
        /* FIXME, after main loop in try_place(), current t set to 0. */
        if (try_swap(placer_paras_ptr,
                     placer_opts,
                     pins_on_block,
                     old_region_occ_x,
                     old_region_occ_y,
                     placer_costs_ptr) == 1) {
            ++(placer_paras_ptr->m_success_sum);
            placer_costs_ptr->m_av_cost += placer_costs_ptr->m_total_cost;
            placer_costs_ptr->m_av_bb_cost += placer_costs_ptr->m_bb_cost;
            placer_costs_ptr->m_av_delay_cost += placer_costs_ptr->m_delay_cost;
            placer_costs_ptr->m_av_timing_cost += placer_costs_ptr->m_timing_cost;
            placer_paras_ptr->m_sum_of_squares +=
                placer_costs_ptr->m_total_cost * placer_costs_ptr->m_total_cost;

            if (placer_opts.place_algorithm == NET_TIMING_DRIVEN_PLACE
                  || placer_opts.place_algorithm == PATH_TIMING_DRIVEN_PLACE
                  /* new added for supporting PATH timing-driven placement */
                  || placer_opts.place_algorithm == NEW_TIMING_DRIVEN_PLACE) {
                recompute_td_cost_after_swap_certain_times(&placer_opts,
                                                           inner_iter,
                                                           net_delay,
                                                           net_slack,
                                                           placer_paras_ptr,
                                                           placer_costs_ptr);
            } /* update timing-cost after try_swap() successful in low-temperature placement! */
        } /* end of try_swap() successful in low-temperature placement */
    } /* end of for(inner_iter = 0; inner_iter < move_limit; ++inner_iter) */

    placer_paras_ptr->m_total_iter += move_limit;
    placer_paras_ptr->m_success_ratio =
            ((double)placer_paras_ptr->m_success_sum) / move_limit;

    update_place_costs_by_success_sum(placer_paras_ptr,
                                      placer_costs_ptr);

    placer_paras_ptr->m_std_dev = get_std_dev(placer_paras_ptr->m_success_sum,
                                              placer_paras_ptr->m_sum_of_squares,
                                              placer_costs_ptr->m_av_cost);
}  /* end of void  run_low_temperature_place(const placer_opts_t  placer_opts,) */


static void perform_timing_analyze(const placer_opts_t* placer_opts_ptr,
                                   double**  net_slack,
                                   double**  net_delay,
                                   placer_paras_t*  placer_paras_ptr,
                                   placer_costs_t*  placer_costs_ptr)
{
#ifdef VERBOSE
    printf("Outer Loop Recompute Criticalities\n");
#endif
    placer_paras_ptr->m_place_delay_value =
        placer_costs_ptr->m_delay_cost / placer_paras_ptr->m_num_connections;
    /* For NET_TIMING_DRIVEN_PLACE, all subnets(or connections) will *
     * set the same and constant Tdel value.                        */
    if (placer_opts_ptr->place_algorithm == NET_TIMING_DRIVEN_PLACE) {
        /* delay_cost was total_delay_cost after Timing_Analyze_Graph *
         * analysis the circuit.                                      */
        load_constant_net_delay(net_delay,
                                placer_paras_ptr->m_place_delay_value);
    }

    /*note, for path_based, the net Tdel is not updated since it is current,
     *because it accesses point_to_point_delay array. */
    load_timing_graph_net_delays(net_delay);
    /* Compute each vertexes's req_time and arr_time time, and calculate *
     * the max_delay in critical path.                          */
    placer_paras_ptr->m_max_delay = calc_all_vertexs_arr_req_time(0);
    compute_net_slacks(net_slack);

    compute_timing_driven_cost(placer_opts_ptr,
                               placer_paras_ptr,
                               net_slack,
                               block_pin_to_tnode,
                               placer_costs_ptr);
    placer_paras_ptr->m_outer_crit_iter_count = 0;
}  /* end of void perform_timing_analyze() */

static void  recompute_td_cost_after_swap_certain_times(const placer_opts_t* placer_opts_ptr,
                                                        const int inner_iter,
                                                        double**  net_delay,
                                                        double**  net_slack,
                                                        placer_paras_t*  placer_paras_ptr,
                                                        placer_costs_t*  placer_costs_ptr)
{
    const int kinner_recompute_limit = placer_paras_ptr->m_inner_recompute_limit;
    const int kinner_crit_iter_count = placer_paras_ptr->m_inner_crit_iter_count;
    const int kmove_limit = placer_paras_ptr->m_move_limit;
    if (kinner_crit_iter_count >= kinner_recompute_limit
          && inner_iter != kmove_limit - 1) {
        placer_paras_ptr->m_inner_crit_iter_count = 0;
#ifdef VERBOSE
        printf("Inner Loop Recompute Criticalities\n");
#endif
        if (placer_opts_ptr->place_algorithm == NET_TIMING_DRIVEN_PLACE) {
            placer_paras_ptr->m_place_delay_value =
              placer_costs_ptr->m_delay_cost / placer_paras_ptr->m_num_connections;
            load_constant_net_delay(net_delay,
                                    placer_paras_ptr->m_place_delay_value);
        }
        /* First, load all nets' delay, then calculate all timing-node arr_time,
         * req_time and max_delay. Last compute timing_cost */
        load_timing_graph_net_delays(net_delay);
        placer_paras_ptr->m_max_delay = calc_all_vertexs_arr_req_time(0);
        compute_net_slacks(net_slack);

        compute_timing_driven_cost(placer_opts_ptr,
                                   placer_paras_ptr,
                                   net_slack,
                                   block_pin_to_tnode,
                                   placer_costs_ptr);
    } /* update timing_cost_ptr in try_swap() at low-temperature placement */

    ++(placer_paras_ptr->m_inner_crit_iter_count);
}  /* end of recompute_td_cost_after_swap_certain_times() */

static void compute_timing_driven_cost(const placer_opts_t*  placer_opts_ptr,
                                       const placer_paras_t* placer_paras_ptr,
                                       double**  net_slack,
                                       int** block_pin_to_tnode,
                                       placer_costs_t*  placer_costs_ptr)
{
    const double kmax_delay = placer_paras_ptr->m_max_delay;
    const double kcrit_exponent = placer_paras_ptr->m_crit_exponent;
    double* timing_cost_ptr = &(placer_costs_ptr->m_timing_cost);
    double* delay_cost_ptr = &(placer_costs_ptr->m_delay_cost);
    if (placer_opts_ptr->place_algorithm == NEW_TIMING_DRIVEN_PLACE) {
        compute_all_nets_local_crit_weight(net_slack,
                                           block_pin_to_tnode,
                                           kmax_delay);
        compute_timing_driven_costs_by_path_algo(timing_cost_ptr,
                                                 delay_cost_ptr);
    } else { /* PATH_TIMING_DRIVEN_PLACE or NET_TIMING_DRIVEN_PLACE */
        load_criticalities(net_slack,
                           kmax_delay,
                           kcrit_exponent);
        compute_timing_driven_cost_by_orig_algo(timing_cost_ptr,
                                                delay_cost_ptr);
    }
}  /* end of void compute_timing_driven_cost() */

static void update_place_cost_after_max_move_times(const placer_opts_t* placer_opts_ptr,
                                                   placer_costs_t*  placer_costs_ptr)
{
    /* first recompute bbox_cost..... */
    enum place_c_types place_cost_type = placer_opts_ptr->place_cost_type;
    const int knum_regions = placer_opts_ptr->num_regions;
    placer_costs_ptr->m_new_bb_cost = recompute_bb_cost(place_cost_type,
                                                        knum_regions);

    const double new_bb_cost = placer_costs_ptr->m_new_bb_cost;
    const double bb_cost = placer_costs_ptr->m_bb_cost;
    if (fabs(new_bb_cost - bb_cost) > bb_cost * ERROR_TOL) {
        printf("Error in try_place:  new_bb_cost = %g, old bb_cost = %g.\n",
               new_bb_cost, bb_cost);
        exit(1);
    }
    placer_costs_ptr->m_bb_cost = placer_costs_ptr->m_new_bb_cost;

    /* then recompute timing-driven placement cost after innner-loop...*/
    const place_algorithm_t place_algo = placer_opts_ptr->place_algorithm;
    if (place_algo == NET_TIMING_DRIVEN_PLACE || place_algo == PATH_TIMING_DRIVEN_PLACE
          /* new added for support PATH timing-driven placement */
          || place_algo == NEW_TIMING_DRIVEN_PLACE) {
        /* Q: current time, why it needn't loading Timing_Analyze_Graph, *
         * update edge slack value to compute timing-driven cost?        *
         * */
        compute_timing_driven_cost_in_outer_loop(placer_opts_ptr,
                                                 placer_costs_ptr);

        placer_costs_ptr->m_timing_cost = placer_costs_ptr->m_new_timing_cost;
        /* I new added */
        placer_costs_ptr->m_delay_cost = placer_costs_ptr->m_new_delay_cost;
    }
} /* end of void update_place_cost_after_max_move_times() */

static void compute_timing_driven_cost_in_outer_loop(const placer_opts_t* placer_opts_ptr,
                                                     placer_costs_t*  placer_costs_ptr)
{
    const place_algorithm_t place_algo = placer_opts_ptr->place_algorithm;
    double* new_timing_cost_ptr = &(placer_costs_ptr->m_new_timing_cost);
    double* new_delay_cost_ptr = &(placer_costs_ptr->m_new_delay_cost);
    if (place_algo == NET_TIMING_DRIVEN_PLACE || place_algo == PATH_TIMING_DRIVEN_PLACE) {
        compute_timing_driven_cost_by_orig_algo(new_timing_cost_ptr,
                                                new_delay_cost_ptr);
    } else { /* NEW_TIMING_DRIVEN_PLACE */
        compute_timing_driven_costs_by_path_algo(new_timing_cost_ptr,
                                                 new_delay_cost_ptr);
    }

    const double ktiming_cost = placer_costs_ptr->m_timing_cost;
    if (fabs(*new_timing_cost_ptr - ktiming_cost) > ktiming_cost * ERROR_TOL) {
        printf("Error in try_place:  new_timing_cost = %g, old timing_cost = %g.\n",
               *new_timing_cost_ptr,
               ktiming_cost);
        exit(1);
    }

    const double kdelay_cost = placer_costs_ptr->m_delay_cost;
    if (fabs(*new_delay_cost_ptr - kdelay_cost) > kdelay_cost * ERROR_TOL) {
        printf("Error in try_place:  new_delay_cost = %g, old delay_cost = %g.\n",
               *new_delay_cost_ptr,
               kdelay_cost);
        exit(1);
    }
}  /* end of void compute_timing_driven_cost_in_outer_loop() */

static void update_place_costs_by_success_sum(const placer_paras_t* placer_paras_ptr,
                                              placer_costs_t*  placer_costs_ptr)
{
    const int success_sum = placer_paras_ptr->m_success_sum;
    if (0 == success_sum) {
        placer_costs_ptr->m_av_bb_cost = placer_costs_ptr->m_bb_cost;
        placer_costs_ptr->m_av_timing_cost = placer_costs_ptr->m_timing_cost;
        placer_costs_ptr->m_av_delay_cost = placer_costs_ptr->m_delay_cost;
        placer_costs_ptr->m_av_cost = placer_costs_ptr->m_total_cost;
    } else {
        placer_costs_ptr->m_av_bb_cost /= success_sum;
        placer_costs_ptr->m_av_timing_cost /= success_sum;
        placer_costs_ptr->m_av_delay_cost /= success_sum;
        placer_costs_ptr->m_av_cost /= success_sum;
    }
}  /* end of static void update_place_costs_by_success_sum(const placer_paras..) */


/* It only count the inter-CLB_TYPE net connections */
static int count_connections(void)
{
    /*only count non-global connections*/
    int count = 0;
    int inet = -1;
    for (inet = 0; inet < num_nets; ++inet) {
        if (is_global[inet]) {
            continue;
        }
        /* for a n-pin net, it had n-1 connections. */
        count += (net[inet].num_pins - 1);
    }

    return count;
} /* end of static int count_connections(void) */

/* Computes net_pin_index[0..num_blocks-1][0..num_pins-1] array, this array  *
 * allows us to quickly find which net connected on the pin(a blocks pin).    */
static void compute_net_pin_index_values(void)  /* FIXME */
{
    /*initialize values to OPEN */
    int iblk = -1;
    int ipin = -1;
    for (iblk = 0; iblk < num_blocks; ++iblk) {
        for (ipin = 0; ipin < pins_per_clb; ++ipin) {
            net_pin_index[iblk][ipin] = OPEN;
        }
    }

    int inet = -1;
    for (inet = 0; inet < num_nets; ++inet) {
        if (is_global[inet]) {
            continue;
        }

        int netpin = -1;
        for (netpin = 0; netpin < net[inet].num_pins; ++netpin) {
            const int blk_index = net[inet].blocks[netpin];
            /* there is only one blocks pin, so it is 0, and it is driving the *
             * net since this is an INPAD_TYPE.                                    */
            if (blocks[blk_index].type == INPAD_TYPE) {
                net_pin_index[blk_index][0] = 0;
            } else if (blocks[blk_index].type == OUTPAD_TYPE) {
              /*there is only one blocks pin, it is 0 */
                net_pin_index[blk_index][0] = netpin;
            } else { /* CLB_TYPE, [block_index][cur_block_pin_index] = netpin */
                net_pin_index[blk_index][net[inet].blk_pin[netpin]] = netpin;
            }
        }
    }
}  /* end of static void compute_net_pin_index_values(void) */

/* Returns the standard deviation of data set x.  There are n sample points, *
 * sum_x_squared is the summation over n of x^2 and av_x is the average x.   *
 * All operations are done in double precision, since round off error can be *
 * a problem in the initial temp. std_dev calculation for big circuits.      */
static double get_std_dev(int success_sum,
                          double sum_of_square,
                          double av_cost)
{
    double std_dev = 0.0;
    if (success_sum <= 1) {
        std_dev = 0.0;
    } else {
        std_dev = (sum_of_square - success_sum * av_cost * av_cost) /
                    (double)(success_sum - 1);
    }

    /* Very small variances sometimes round negative */
    if (std_dev > 0.0) {
        std_dev = sqrt(std_dev);
    } else {
        std_dev = 0.0;
    }

    return std_dev;
}  /* end of static double get_std_dev(int success_sum, */

static double starting_temperature(const annealing_sched_t annealing_sched,
                                   const int*  pins_on_block,
                                   const placer_opts_t   placer_opts,
                                   placer_paras_t* placer_paras_ptr,
                                   double**  old_region_occ_x,
                                   double**  old_region_occ_y,
                                   placer_costs_t* placer_costs_ptr)
{
    /* Finds the starting temperature (hot condition). */
    if (annealing_sched.type == USER_SCHED) {
        return (annealing_sched.init_t);
    }

    int num_accepted = 0;
    double av_cost = 0.0;
    double sum_of_squares = 0.;
    /* Try one move per blocks.  Set t high so essentially all accepted. */
    const int move_limit = min(placer_paras_ptr->m_move_limit, num_blocks);
    placer_paras_ptr->m_temper = 1.e30;
    int i = -1;
    for (i = 0; i < move_limit; ++i) {
        if (try_swap(placer_paras_ptr,
                     placer_opts,
                     pins_on_block,
                     old_region_occ_x,
                     old_region_occ_y,
                     placer_costs_ptr) == 1) {
            ++num_accepted;
            av_cost += placer_costs_ptr->m_total_cost;
            sum_of_squares += placer_costs_ptr->m_total_cost * placer_costs_ptr->m_total_cost;
        }
    }  /* end of for(i = 0; i < move_limit; ++i) */

    /* Initial Temp = 20 * std_dev. */
    if (num_accepted != 0) {
        av_cost /= num_accepted;
    } else {
        av_cost = 0.0;
    }

    /* Double important to avoid round off */
    double std_dev = get_std_dev(num_accepted,
                                 sum_of_squares,
                                 av_cost);
#ifdef DEBUG
    if (num_accepted != move_limit) {
        printf("Warning:  Starting t: %d of %d configurations accepted.\n",
               num_accepted, move_limit);
    }
#endif

    return (20. * std_dev);
} /* end of static double starting_temperature() */

/* Update the temperature according to the annealing-schedule, success_ratio
 * and range_limit. */
static void update_temperature(double* t,
                               double rlim,
                               double success_ratio,
                               annealing_sched_t annealing_sched)
{
    /*  double fac; */
    if (annealing_sched.type == USER_SCHED) {
        *t = annealing_sched.alpha_t * (*t);
    } else {  /* AUTO_SCHED */
        if (success_ratio > 0.96) {
            *t = (*t) * 0.5;
        } else if (success_ratio > 0.8) {
            *t = (*t) * 0.9;
        } else if (success_ratio > 0.15 || rlim > 1.) {
            *t = (*t) * 0.95;
        } else {
            *t = (*t) * 0.8;
        }
    }
}  /* end of static void update_temperature(double* t, ) */


static void update_range_limit(double* rlim,
                               double success_ratio)
{
    /* Update the range limited to keep acceptance prob. near 0.44.  Use *
     * a doubleing point rlim to allow gradual transitions at low temps.  */
    *rlim = (*rlim) * (1.0 - 0.44 + success_ratio);
    double upper_lim = max(num_grid_columns,
                           num_grid_rows);
    *rlim = min(*rlim, upper_lim);
    *rlim = max(*rlim, 1.0);
    /* *rlim = (double) num_grid_columns; */
}  /* end of static void update_range_limit(double* rlim, */

static double update_crit_exponent(const placer_opts_t*  placer_opts_ptr,
                                   const placer_paras_t* placer_paras_ptr)
{
    const double krange_limit = placer_paras_ptr->m_range_limit;
    const double kfinal_rlim  = placer_paras_ptr->m_final_rlim;
    const double kinverse_delta_rlim = placer_paras_ptr->m_inverse_delta_rlim;
    const double rlim_ratio = (krange_limit - kfinal_rlim) * kinverse_delta_rlim;

    const double place_exp_range =
        placer_opts_ptr->td_place_exp_last - placer_opts_ptr->td_place_exp_first;
    double new_crit_exponent = (1 - rlim_ratio) * place_exp_range +
                    placer_opts_ptr->td_place_exp_first;

    return new_crit_exponent;
}  /* end of static double update_crit_exponent(const placer_opts_t*)*/

static int exit_crit(double t,
                     double total_cost,
                     annealing_sched_t annealing_sched)
{
    /* Return 1 when the exit criterion is meet. */
    if (annealing_sched.type == USER_SCHED) {
        if (t < annealing_sched.exit_t) {
            return(1);
        } else {
            return(0);
        }
    }

    /* Automatic annealing schedule */
    if (t < 0.005 * total_cost / num_nets) {
        return 1;
    } else {
        return 0;
    }
} /* end of static int exit_crit() */


/* FIXME: Try to swap a pair of plbs or io_pads randomly*/
/* Picks some blocks and moves it to another spot. If this spot had occupied *
 * , switch the blocks. Assess the change in cost function, and accept or   *
 * reject the move. If rejected, return 0, else return 1. Pass back the new *
 * value of the cost function. rlim is the range_limit. pins_on_block gives *
 * the number of pins on each type of blocks(improves efficiency). Pins on   *
 * each type of blocks(improves efficiency).                                 */
static int try_swap(const placer_paras_t*  placer_paras_ptr,
                    const placer_opts_t   placer_opts,
                    const int*  pins_on_block,
                    double**    old_region_occ_x,
                    double**    old_region_occ_y,
                    placer_costs_t*  placer_costs_ptr)
{
    static bbox_t* bb_coord_new = NULL;
    static bbox_t* bb_edge_new = NULL;
    static int*  nets_to_update = NULL;
    static int*  net_block_moved = NULL;

    /* Allocate the local bb_coordinate storage, etc. only once. */
    /* Q: Why did the array allocate double memory space? TODO*/
    if (bb_coord_new == NULL) {
        bb_coord_new = (bbox_t*)my_malloc(2 * pins_per_clb *
                                                sizeof(bbox_t));
        bb_edge_new = (bbox_t*)my_malloc(2 * pins_per_clb *
                                               sizeof(bbox_t));
        nets_to_update = (int*)my_malloc(2 * pins_per_clb * sizeof(int));
        net_block_moved = (int*)my_malloc(2 * pins_per_clb * sizeof(int));
    }

    /* If the pins are fixed, we never move them from their initial random *
     * locations. The code below could be made more efficient by using the *
     * fact that pins appear first in the blocks list, but this shouldn't   *
     * cause any significant slowdown and won't be broken if I ever change *
     * the parser so that the pins aren't necessarily at the start of the  *
     * blocks list.                                                         */
    int from_block = my_irand(num_blocks - 1); /* choose from blocks randomly */
    if (placer_paras_ptr->m_fixed_pins == TRUE) {
        while (blocks[from_block].type != CLB_TYPE) {
            from_block = my_irand(num_blocks - 1);
        }
    }

    int x_from = blocks[from_block].x;
    int y_from = blocks[from_block].y;
    int x_to = 0;
    int y_to = 0;
    /* find the plb or io_pad (x_to, y_to) randomly */
    find_to(x_from,
            y_from,
            blocks[from_block].type,
            placer_paras_ptr->m_range_limit,
            &x_to,
            &y_to);

    /* Make the switch in order to let compute the new bounding-box simpler. If *
     * increase is too high, switch them back.(blocks data structures switched,  *
     * clb not switched until success of move is determinded.)                 */
    int io_num = 0;
    int to_block = 0;
    if (blocks[from_block].type == CLB_TYPE) {
        if (clb_grids[x_to][y_to].m_usage == 1) { /* target clb location had occupied -- do a switch */
            /* then swap (x_from, y_from) and (x_to, y_to) coordinate. */
            to_block = clb_grids[x_to][y_to].u.blocks;
            blocks[from_block].x = x_to;
            blocks[from_block].y = y_to;
            blocks[to_block].x = x_from;
            blocks[to_block].y = y_from;
        } else { /* if clb[x_to][y_to] was empty, move blocks[from_block] to clb[x_to][y_to] location. */
            to_block = EMPTY;
            blocks[from_block].x = x_to;
            blocks[from_block].y = y_to;
        }
    } else { /* io pads was selected for moving */
        io_num = my_irand(io_ratio - 1);
        if (io_num >= clb_grids[x_to][y_to].m_usage) { /* Moving to an empty location, why? TODO:*/
            to_block = EMPTY;
            blocks[from_block].x = x_to;
            blocks[from_block].y = y_to;
        } else { /* Swapping two blocks */
            to_block = *(clb_grids[x_to][y_to].u.io_blocks + io_num);
            blocks[to_block].x = x_from;
            blocks[to_block].y = y_from;
            blocks[from_block].x = x_to;
            blocks[from_block].y = y_to;
        }
    }

    /*===========  Now Compute the cost ==============*/
    /* Now update the cost function. May have to do major optimizations here    *
     * later. I'm using negative values of temp_net_cost as a flag, so DO NOT   *
     * use cost-functions that can go negative.                                 */
    const int num_of_pins = pins_on_block[blocks[from_block].type];
    /* When blocks[to_block] was a EMPTY, it only deal with blocks[from_block] */
    const int num_nets_affected = find_affected_nets(nets_to_update,
                                                     net_block_moved,
                                                     from_block,
                                                     to_block,
                                                     num_of_pins);
    enum place_c_types place_cost_type = placer_opts.place_cost_type;
    const int knum_regions = placer_opts.num_regions;
    if (place_cost_type == NONLINEAR_CONG) {
        save_region_occ(old_region_occ_x,
                        old_region_occ_y,
                        knum_regions);
    }

    /* Then calculate the wirelength_cost and timing_cost_ptr */
    double bb_delta_c = 0.0;
    int bb_index = 0; /* Index of new bounding box. */
    int k = -1;
    int inet = -1;
    int off_from = -1;
    for (k = 0; k < num_nets_affected; ++k) {
        inet = nets_to_update[k];
        /* If we swapped two blocks connected to the same net, its bounding box *
         * doesn't change. Due to it only swap a pair of blocks, it didn't change*
         * the bounding-box coordinate.                                         */
        if (net_block_moved[k] == FROM_AND_TO) {
            continue;
        }
        /* Then update the net bounding-box. */
        if (net[inet].num_pins <= SMALL_NET) { /* 4 */
            get_non_updateable_bb(inet,
                                  &bb_coord_new[bb_index]);
        } else {
            if (net_block_moved[k] == FROM) {
                update_bb(inet,
                          &bb_coord_new[bb_index],
                          &bb_edge_new[bb_index],
                          x_from,
                          y_from,
                          x_to,
                          y_to);
            } else {
                update_bb(inet,
                          &bb_coord_new[bb_index],
                          &bb_edge_new[bb_index],
                          x_to,
                          y_to,
                          x_from,
                          y_from);
            }
        }

        /* then calculate bounding_box_cost */
        if (place_cost_type != NONLINEAR_CONG) {
            temp_net_cost[inet] = get_net_cost(inet,
                                               &bb_coord_new[bb_index]);
            bb_delta_c += temp_net_cost[inet] - net_cost[inet];
        } else {
            /* Rip up, then replace with new bb. */
            update_region_occ(inet,
                              &bb_coords[inet],
                              -1,
                              knum_regions);
            update_region_occ(inet,
                              &bb_coord_new[bb_index],
                              1,
                              knum_regions);
        }

        ++bb_index;
    } /* end of for (k = 0; k < num_nets_affected; k++)*/

    double  newcost = 0.0;
    if (place_cost_type == NONLINEAR_CONG) {
        newcost = nonlinear_cong_cost(placer_opts.num_regions);
        /* bb_delta_c = newcost - *bb_cost_ptr; */
        bb_delta_c = newcost - placer_costs_ptr->m_bb_cost;
    }

    /* then calculate timing-total_cost_ptr */
    double delta_cost = 0.0; /* Change in total_cost_ptr due to this swap. */
    double timing_delta_c = 0.0;
    double delay_delta_c = 0.0;
    const place_algorithm_t place_algo = placer_opts.place_algorithm;
    if (place_algo == NET_TIMING_DRIVEN_PLACE
          || place_algo == PATH_TIMING_DRIVEN_PLACE
    /* new added for supporting PATH timing-driven placement */
          || place_algo == NEW_TIMING_DRIVEN_PLACE) {
        /*in this case we redefine delta_cost as a combination of timing and bb.*
         *additionally, we normalize all values, therefore delta_cost is in     *
         *relation to 1 */
        compute_delta_timing_driven_cost(place_algo,
                                         from_block,
                                         to_block,
                                         num_of_pins,
                                         &timing_delta_c,
                                         &delay_delta_c);

        const double ktiming_tradeoff = placer_opts.timing_tradeoff;
        const double normal_bb_cost =
                bb_delta_c * placer_costs_ptr->m_inverse_prev_bb_cost;
        const double normal_timing_cost =
                timing_delta_c * placer_costs_ptr->m_inverse_prev_timing_cost;
        delta_cost = (1 - ktiming_tradeoff) * normal_bb_cost
                       + ktiming_tradeoff * normal_timing_cost;
    } else {
        delta_cost = bb_delta_c;
    }

    /* 1: move accepted, 0: rejected. */
    int keep_switch = assess_swap(delta_cost,
                                  placer_paras_ptr->m_temper);
    if (keep_switch) {
        placer_costs_ptr->m_total_cost += delta_cost;
        placer_costs_ptr->m_bb_cost += bb_delta_c;

        if (place_algo == NET_TIMING_DRIVEN_PLACE || place_algo == PATH_TIMING_DRIVEN_PLACE
              || place_algo == NEW_TIMING_DRIVEN_PLACE) {
            placer_costs_ptr->m_timing_cost += timing_delta_c;
            placer_costs_ptr->m_delay_cost += delay_delta_c;

            /* update the point_to_point_timing_cost and point_to_point_delay_cost,*
             * when VPR had accept current move.                                   */
            update_timing_driven_cost(from_block,
                                      to_block,
                                      num_of_pins);
        }

        /* update net total_cost_ptr functions and reset flags. */
        bb_index = 0;
        for (k = 0; k < num_nets_affected; ++k) {
            inet = nets_to_update[k];

            /* If we swapped two blocks connected to the same net, its bounding *
             * box doesn't change. So ignore it                                 */
            if (net_block_moved[k] == FROM_AND_TO) {
                temp_net_cost[inet] = -1;
                continue;
            }

            bb_coords[inet] = bb_coord_new[bb_index];
            if (net[inet].num_pins > SMALL_NET) {
                bb_num_on_edges[inet] = bb_edge_new[bb_index];
            }

            ++bb_index;
            net_cost[inet] = temp_net_cost[inet];
            temp_net_cost[inet] = -1;
        }

        /* Update Clb data structures since we kept the move. */
        if (blocks[from_block].type == CLB_TYPE) {
            if (to_block != EMPTY) {
                clb_grids[x_from][y_from].u.blocks = to_block;
                clb_grids[x_to][y_to].u.blocks = from_block;
            } else {
                clb_grids[x_to][y_to].u.blocks = from_block;
                clb_grids[x_to][y_to].m_usage = 1;
                clb_grids[x_from][y_from].m_usage = 0;
            }
        } else {   /* io blocks was selected for moving */
            /* Get the "sub_block" number of the from_block blocks. */
            for (off_from = 0;; ++off_from) {
                if (clb_grids[x_from][y_from].u.io_blocks[off_from] == from_block) {
                    break;
                }
            }

            if (to_block != EMPTY) { /* Swapped two blocks. */
                clb_grids[x_to][y_to].u.io_blocks[io_num] = from_block;
                clb_grids[x_from][y_from].u.io_blocks[off_from] = to_block;
            } else {             /* Moved to an empty location */
                clb_grids[x_to][y_to].u.io_blocks[clb_grids[x_to][y_to].m_usage] = from_block;
                ++(clb_grids[x_to][y_to].m_usage);

                for (k = off_from; k < clb_grids[x_from][y_from].m_usage-1; ++k) { /* prevent gap  */
                    clb_grids[x_from][y_from].u.io_blocks[k] =   /* in io_blocks */
                        clb_grids[x_from][y_from].u.io_blocks[k + 1];
                }

                --(clb_grids[x_from][y_from].m_usage);
            }
        } /* end of selecting move io_block */
    } else {  /* Move was rejected.  */
        /* Reset the net total_cost_ptr function flags first. */
        for (k = 0; k < num_nets_affected; k++) {
            inet = nets_to_update[k];
            temp_net_cost[inet] = -1;
        }

        /* Restore the blocks data structures to their state before the move. */
        blocks[from_block].x = x_from;
        blocks[from_block].y = y_from;

        if (to_block != EMPTY) {
            blocks[to_block].x = x_to;
            blocks[to_block].y = y_to;
        }

        /* Restore the region occupancies to their state before the move. */
        if (place_cost_type == NONLINEAR_CONG) {
            restore_region_occ(old_region_occ_x,
                               old_region_occ_y,
                               knum_regions);
        }
    }

    return keep_switch;
} /* end of static int try_swap() */


static void alloc_and_load_placement_structs(const placer_opts_t* placer_opts_ptr,
                                             double*** old_region_occ_x,
                                             double*** old_region_occ_y)
{
    /* If I didn't add NEW_TIMING_DRIVEN_PLACE, point_to_point_delay_cost
     * point_to_point_timing_cost and net_pin_index must be NULL pointer.
     * It must result in segment fault error! */
    const place_algorithm_t kplace_algo = placer_opts_ptr->place_algorithm;
    if (kplace_algo == NET_TIMING_DRIVEN_PLACE
          || kplace_algo == PATH_TIMING_DRIVEN_PLACE
          || kplace_algo == NEW_TIMING_DRIVEN_PLACE
          || placer_opts_ptr->enable_timing_computations) {
        point_to_point_delay_cost = (double**)my_malloc(num_nets * sizeof(double*));
        temp_point_to_point_delay_cost = (double**)my_malloc(num_nets * sizeof(double*));

        point_to_point_timing_cost = (double**)my_malloc(num_nets * sizeof(double*));
        temp_point_to_point_timing_cost = (double**)my_malloc(num_nets * sizeof(double*));

        net_pin_index = (int**)alloc_matrix(0, num_blocks - 1,
                                            0, pins_per_clb - 1,
                                            sizeof(int));

        /* TODO: When I comment the following 4 statements like:                *
         * --point_to_point_dealy_cost[inet], the placeent result keep the same *
         * like previous vpr4.3, but when free the placement structs, it generated
         * segment fault. Now I should discover the this phenomenon.            */
        int inet = -1;
        for (inet = 0; inet < num_nets; ++inet) {
        /* in the following, subract one so index starts at 1 instead of 0 */
            point_to_point_delay_cost[inet] =
                (double*)my_malloc((net[inet].num_pins - 1) * sizeof(double));
            --(point_to_point_delay_cost[inet]);

            temp_point_to_point_delay_cost[inet] =
                (double*)my_malloc((net[inet].num_pins - 1) * sizeof(double));
            --(temp_point_to_point_delay_cost[inet]);

            point_to_point_timing_cost[inet] =
                (double*)my_malloc((net[inet].num_pins - 1) * sizeof(double));
            --(point_to_point_timing_cost[inet]);

            temp_point_to_point_timing_cost[inet] =
                (double*)my_malloc((net[inet].num_pins - 1) * sizeof(double));
            --(temp_point_to_point_timing_cost[inet]);
        } /* end of for() */

        int ipin = 0;
        for (inet = 0; inet < num_nets; ++inet) {
            for (ipin = 1; ipin < net[inet].num_pins; ++ipin) {
                point_to_point_delay_cost[inet][ipin] = 0;
                temp_point_to_point_delay_cost[inet][ipin] = 0;
            }
        }
    }

    /* double new_cost[0..num_nets-1] */
    net_cost = (double*)my_malloc(num_nets * sizeof(double));
    temp_net_cost = (double*)my_malloc(num_nets * sizeof(double));

    /* Used to store costs for moves not yet made and to indicate when a net's   *
     * cost has been recomputed. temp_net_cost[inet] < 0 means net's cost hasn't *
     * been recomputed.                                                          */
    int inet = -1;
    for (inet = 0; inet < num_nets; ++inet) {
        temp_net_cost[inet] = -1.0;
    }

    bb_coords = (bbox_t*)my_malloc(num_nets * sizeof(bbox_t));
    bb_num_on_edges = (bbox_t*)my_malloc(num_nets * sizeof(bbox_t));

    /* Get a list of pins with no duplicates. */
    alloc_and_load_unique_pin_list();

    /* Allocate storage for subregion data, if needed. */
    enum place_c_types place_cost_type = placer_opts_ptr->place_cost_type;
    const int knum_regions = placer_opts_ptr->num_regions;
    if (place_cost_type == NONLINEAR_CONG) {
        alloc_place_regions(knum_regions);
        load_place_regions(knum_regions);
        *old_region_occ_x = (double**)alloc_matrix(0, knum_regions - 1,
                                                   0, knum_regions - 1,
                                                   sizeof(double));
        *old_region_occ_y = (double**)alloc_matrix(0, knum_regions - 1,
                                                   0, knum_regions - 1,
                                                   sizeof(double));
    } else { /* Shouldn't use them; crash hard if I do!   */
        *old_region_occ_x = NULL;
        *old_region_occ_y = NULL;

        alloc_and_load_for_fast_cost_update(placer_opts_ptr->place_cost_exp);
    }
} /* end of static void alloc_and_load_placement_structs() */


/* Frees the major structures needed by the placer(and not needed *
 * elsewhere).   */
/* Allocates the major structures needed only by the placer, primarily for *
 * computing costs quickly and such. FIXME                                 */
static void free_placement_structs(int place_cost_type,
                                   int num_regions,
                                   double** old_region_occ_x,
                                   double** old_region_occ_y,
                                   placer_opts_t placer_opts)
{
    if (placer_opts.place_algorithm == NET_TIMING_DRIVEN_PLACE
          || placer_opts.place_algorithm == PATH_TIMING_DRIVEN_PLACE
          || placer_opts.place_algorithm == NEW_TIMING_DRIVEN_PLACE
          || placer_opts.enable_timing_computations) {
        int inet = -1;
        for (inet = 0; inet < num_nets; ++inet) {
            /*add one to the address since it is indexed from 1 not 0 */
            ++point_to_point_delay_cost[inet];
            free(point_to_point_delay_cost[inet]);

            ++point_to_point_timing_cost[inet];
            free(point_to_point_timing_cost[inet]);

            ++temp_point_to_point_delay_cost[inet];
            free(temp_point_to_point_delay_cost[inet]);

            ++temp_point_to_point_timing_cost[inet];
            free(temp_point_to_point_timing_cost[inet]);
        }

        free(point_to_point_delay_cost);
        free(temp_point_to_point_delay_cost);
        free(point_to_point_timing_cost);
        free(temp_point_to_point_timing_cost);
        free_matrix(net_pin_index,
                    0,
                    num_blocks - 1,
                    0,
                    sizeof(int));
    } /* end of if () */

    free(net_cost);
    free(temp_net_cost);
    free(bb_num_on_edges);
    free(bb_coords);
    net_cost = NULL;  /* Defensive coding. */
    temp_net_cost = NULL;
    bb_num_on_edges = NULL;
    bb_coords = NULL;
    free_unique_pin_list();

    if (place_cost_type == NONLINEAR_CONG) {
        free_place_regions(num_regions);
        free_matrix(old_region_occ_x, 0, num_regions - 1, 0, sizeof(double));
        free_matrix(old_region_occ_y, 0, num_regions - 1, 0, sizeof(double));
    } else if (place_cost_type == LINEAR_CONG) {
        free_fast_cost_update_structs();
    }
} /* static void free_placement_structs() */

/* Allocates memory for the regional occupancy, cost, etc. counts *
 * kept when we're using the NONLINEAR_CONG placement cost        *
 * function.                                                      */
static void alloc_place_regions(int num_regions)
{
    place_region_x = (place_region_t**)alloc_matrix(0,
                                                    num_regions - 1,
                                                    0,
                                                    num_regions - 1,
                                                    sizeof(place_region_t));
    place_region_y = (place_region_t**)alloc_matrix(0,
                                                    num_regions - 1,
                                                    0,
                                                    num_regions - 1,
                                                    sizeof(place_region_t));
    place_region_bounds_x = (double*)my_malloc((num_regions + 1) *
                                               sizeof(double));
    place_region_bounds_y = (double*)my_malloc((num_regions + 1) *
                                               sizeof(double));
}  /* end of static void alloc_place_regions(int num_regions) */

static void load_place_regions(int num_regions)
{
    /* Loads the capacity values in each direction for each of the placement *
     * regions.  The chip is divided into a num_regions x num_regions array. */
    int i, j, low_block, high_block, rnum;
    double low_lim, high_lim, capacity, fac, block_capacity;
    double len_fac, height_fac;

    /* First load up horizontal channel capacities.  */
    for (j = 0; j < num_regions; j++) {
        capacity = 0.;
        low_lim = (double) j / (double) num_regions * num_grid_rows + 1.;
        high_lim = (double)(j + 1) / (double) num_regions * num_grid_rows;
        low_block = floor(low_lim);
        low_block = max(1, low_block); /* Watch for weird roundoff effects. */
        high_block = ceil(high_lim);
        high_block = min(high_block, num_grid_rows);
        block_capacity = (chan_width_x[low_block - 1] +
                          chan_width_x[low_block]) / 2.;

        if (low_block == 1) {
            block_capacity += chan_width_x[0] / 2.;
        }

        fac = 1. - (low_lim - low_block);
        capacity += fac * block_capacity;

        for (rnum = low_block + 1; rnum < high_block; rnum++) {
            block_capacity = (chan_width_x[rnum - 1] + chan_width_x[rnum]) / 2.;
            capacity += block_capacity;
        }

        block_capacity = (chan_width_x[high_block - 1] +
                          chan_width_x[high_block]) / 2.;

        if (high_block == num_grid_rows) {
            block_capacity += chan_width_x[num_grid_rows] / 2.;
        }

        fac = 1. - (high_block - high_lim);
        capacity += fac * block_capacity;

        for (i = 0; i < num_regions; i++) {
            place_region_x[i][j].capacity = capacity;
            place_region_x[i][j].inv_capacity = 1. / capacity;
            place_region_x[i][j].occupancy = 0.;
            place_region_x[i][j].cost = 0.;
        }
    }

    /* Now load vertical channel capacities.  */
    for (i = 0; i < num_regions; i++) {
        capacity = 0.;
        low_lim = (double) i / (double) num_regions * num_grid_columns + 1.;
        high_lim = (double)(i + 1) / (double) num_regions * num_grid_columns;
        low_block = floor(low_lim);
        low_block = max(1, low_block); /* Watch for weird roundoff effects. */
        high_block = ceil(high_lim);
        high_block = min(high_block, num_grid_columns);
        block_capacity = (chan_width_y[low_block - 1] +
                          chan_width_y[low_block]) / 2.;

        if (low_block == 1) {
            block_capacity += chan_width_y[0] / 2.;
        }

        fac = 1. - (low_lim - low_block);
        capacity += fac * block_capacity;

        for (rnum = low_block + 1; rnum < high_block; rnum++) {
            block_capacity = (chan_width_y[rnum - 1] + chan_width_y[rnum]) / 2.;
            capacity += block_capacity;
        }

        block_capacity = (chan_width_y[high_block - 1] +
                          chan_width_y[high_block]) / 2.;

        if (high_block == num_grid_columns) {
            block_capacity += chan_width_y[num_grid_columns] / 2.;
        }

        fac = 1. - (high_block - high_lim);
        capacity += fac * block_capacity;

        for (j = 0; j < num_regions; j++) {
            place_region_y[i][j].capacity = capacity;
            place_region_y[i][j].inv_capacity = 1. / capacity;
            place_region_y[i][j].occupancy = 0.;
            place_region_y[i][j].cost = 0.;
        }
    }

    /* Finally set up the arrays indicating the limits of each of the *
     * placement subregions.                                          */
    len_fac = (double) num_grid_columns / (double) num_regions;
    height_fac = (double) num_grid_rows / (double) num_regions;
    place_region_bounds_x[0] = 0.5;
    place_region_bounds_y[0] = 0.5;

    for (i = 1; i <= num_regions; i++) {
        place_region_bounds_x[i] = place_region_bounds_x[i - 1] + len_fac;
        place_region_bounds_y[i] = place_region_bounds_y[i - 1] + height_fac;
    }
} /* end of static void load_place_regions(int num_regions) */

static void update_region_occ(int inet, bbox_t* coords,
                              int add_or_sub, int num_regions)
{
    /* Called only when the place_cost_type is NONLINEAR_CONG.  If add_or_sub *
     * is 1, this uses the new net bounding box to increase the occupancy     *
     * of some regions.  If add_or_sub = - 1, it decreases the occupancy      *
     * by that due to this bounding box.                                      */
    double net_xmin, net_xmax, net_ymin, net_ymax, crossing;
    double inv_region_len, inv_region_height;
    double inv_bb_len, inv_bb_height;
    double overlap_xlow, overlap_xhigh, overlap_ylow, overlap_yhigh;
    double y_overlap, x_overlap, x_occupancy, y_occupancy;
    int imin, imax, jmin, jmax, i, j;

    if (net[inet].num_pins > 50) {
        crossing = 2.7933 + 0.02616 * (net[inet].num_pins - 50);
    } else {
        crossing = cross_count[net[inet].num_pins - 1];
    }

    net_xmin = coords->xmin - 0.5;
    net_xmax = coords->xmax + 0.5;
    net_ymin = coords->ymin - 0.5;
    net_ymax = coords->ymax + 0.5;
    /* I could precompute the two values below.  Should consider this. */
    inv_region_len = (double) num_regions / (double) num_grid_columns;
    inv_region_height = (double) num_regions / (double) num_grid_rows;
    /* Get integer coordinates defining the rectangular area in which the *
     * subregions have to be updated.  Formula is as follows:  subtract   *
     * 0.5 from net_xmin, etc. to get numbers from 0 to num_grid_columns or num_grid_rows;         *
     * divide by num_grid_columns or num_grid_rows to scale between 0 and 1; multiply by           *
     * num_regions to scale between 0 and num_regions; and truncate to    *
     * get the final answer.                                              */
    imin = (int)(net_xmin - 0.5) * inv_region_len;
    imax = (int)(net_xmax - 0.5) * inv_region_len;
    imax = min(imax, num_regions - 1);        /* Watch for weird roundoff */
    jmin = (int)(net_ymin - 0.5) * inv_region_height;
    jmax = (int)(net_ymax - 0.5) * inv_region_height;
    jmax = min(jmax, num_regions - 1);        /* Watch for weird roundoff */
    inv_bb_len = 1. / (net_xmax - net_xmin);
    inv_bb_height = 1. / (net_ymax - net_ymin);

    /* See RISA paper (ICCAD '94, pp. 690 - 695) for a description of why *
     * I use exactly this cost function.                                  */

    for (i = imin; i <= imax; i++) {
        for (j = jmin; j <= jmax; j++) {
            overlap_xlow = max(place_region_bounds_x[i], net_xmin);
            overlap_xhigh = min(place_region_bounds_x[i + 1], net_xmax);
            overlap_ylow = max(place_region_bounds_y[j], net_ymin);
            overlap_yhigh = min(place_region_bounds_y[j + 1], net_ymax);
            x_overlap = overlap_xhigh - overlap_xlow;
            y_overlap = overlap_yhigh - overlap_ylow;
#ifdef DEBUG

            if (x_overlap < -0.001) {
                printf("Error in update_region_occ:  x_overlap < 0"
                       "\n inet = %d, overlap = %g\n", inet, x_overlap);
            }

            if (y_overlap < -0.001) {
                printf("Error in update_region_occ:  y_overlap < 0"
                       "\n inet = %d, overlap = %g\n", inet, y_overlap);
            }

#endif
            x_occupancy = crossing * y_overlap * x_overlap * inv_bb_height *
                          inv_region_len;
            y_occupancy = crossing * x_overlap * y_overlap * inv_bb_len *
                          inv_region_height;
            place_region_x[i][j].occupancy += add_or_sub * x_occupancy;
            place_region_y[i][j].occupancy += add_or_sub * y_occupancy;
        }
    }
} /* end of static void update_region_occ(int inet, bbox_t* coords, */


static void save_region_occ(double** old_region_occ_x,
                            double** old_region_occ_y,
                            int num_regions)
{
    /* Saves the old occupancies of the placement subregions in case the  *
     * current move is not accepted.  Used only for NONLINEAR_CONG.       */
    int i, j;

    for (i = 0; i < num_regions; i++) {
        for (j = 0; j < num_regions; j++) {
            old_region_occ_x[i][j] = place_region_x[i][j].occupancy;
            old_region_occ_y[i][j] = place_region_y[i][j].occupancy;
        }
    }
}

static void restore_region_occ(double** old_region_occ_x,
                               double** old_region_occ_y, int num_regions)
{
    /* Restores the old occupancies of the placement subregions when the  *
     * current move is not accepted.  Used only for NONLINEAR_CONG.       */
    int i, j;

    for (i = 0; i < num_regions; i++) {
        for (j = 0; j < num_regions; j++) {
            place_region_x[i][j].occupancy = old_region_occ_x[i][j];
            place_region_y[i][j].occupancy = old_region_occ_y[i][j];
        }
    }
}

static void free_place_regions(int num_regions)
{
    /* Frees the place_regions data structures needed by the NONLINEAR_CONG *
     * cost function.                                                       */
    free_matrix(place_region_x,
                0,
                num_regions - 1,
                0,
                sizeof(place_region_t));
    free_matrix(place_region_y,
                0,
                num_regions - 1,
                0,
                sizeof(place_region_t));
    free(place_region_bounds_x);
    free(place_region_bounds_y);
}  /* end of static void free_place_regions(int num_regions) */


/* This routine looks for multiple pins going to the same blocks in the *
 * pinlist of each net.  If it finds any, it marks that net as having  *
 * duplicate pins, and creates a new pinlist with no duplicates.  This *
 * is then used by the updatable bounding box calculation routine for  *
 * efficiency.                                                         */
static void alloc_and_load_unique_pin_list(void)
{
    duplicate_pins = (int*)my_calloc(num_nets, sizeof(int));

    /* [0..num_blocks-1]: number of times a blocks is   *
     * listed in the pinlist of a net.  Temp. storage. */
    int* times_listed = (int*)my_calloc(num_blocks, sizeof(int));
    int  any_dups = 0;
    int inet = -1;
    for (inet = 0; inet < num_nets; ++inet) {
        int num_dup = 0;
        int ipin = -1;
        int block_num = -1;
        for (ipin = 0; ipin < net[inet].num_pins; ++ipin) {
            block_num = net[inet].blocks[ipin];
            ++times_listed[block_num];
            if (times_listed[block_num] > 1) {
                ++num_dup;
            }
        }

        if (num_dup > 0) { /* Duplicates found. Make unique pin list. */
            duplicate_pins[inet] = num_dup;
            if (any_dups == 0) { /* This is the first duplicate found */
                /* int unique_pin_list[0..num_nets-1][0..num_unique_blocks-1] */
                unique_pin_list = (int**)my_calloc(num_nets, sizeof(int*));
                any_dups = 1;
            }

            /* nets[inet].num_pins - num_dup */
            unique_pin_list[inet] = (int*)my_malloc((net[inet].num_pins - num_dup) *
                                              sizeof(int));

            int offset = 0;
            for (ipin = 0; ipin < net[inet].num_pins; ++ipin) {
                block_num = net[inet].blocks[ipin];
                if (times_listed[block_num] != 0) {
                    times_listed[block_num] = 0;
                    unique_pin_list[inet][offset] = block_num;
                    ++offset;
                }
            }
        } else { /* No duplicates found. Reset times_listed. */
            for (ipin = 0; ipin < net[inet].num_pins; ++ipin) {
                block_num = net[inet].blocks[ipin];
                times_listed[block_num] = 0;
            }
        }
    }

    free(times_listed);
    times_listed = NULL;
}  /* end of static void alloc_and_load_unique_pin_list(void) */

/* Frees the unique pin list structures. */
static void free_unique_pin_list(void)
{
    int any_dup, inet;
    any_dup = 0;

    for (inet = 0; inet < num_nets; inet++) {
        if (duplicate_pins[inet] != 0) {
            free(unique_pin_list[inet]);
            any_dup = 1;
        }
    }

    if (any_dup != 0) {
        free(unique_pin_list);
    }

    free(duplicate_pins);
}  /* end of static void free_unique_pin_list(void) */


/* When blocks[to_block] was a EMPTY, it only deal with blocks[from_block] */
static int find_affected_nets(int* nets_to_update, int* net_block_moved,
                              int from_block, int to_block, int num_of_pins)
{
    /* Puts a list of all the nets connected to from_block and to_block into nets_to_update. *
     * Returns the number of affected nets. Net_block_moved is either FROM, TO or    *
     * FROM_AND_TO -- the blocks connected to this net that has moved.                */
    int count = 0;
    int affected_index = 0;
    int inet = 0;
    int k = -1;
    for (k = 0; k < num_of_pins; ++k) {
        inet = blocks[from_block].nets[k];
        if (inet == OPEN || is_global[inet] == TRUE) {
            continue;
        }

        /* This is here in case the same blocks connects to a net twice. */
        if (temp_net_cost[inet] > 0.0) {
            continue;
        }

        nets_to_update[affected_index] = inet;
        net_block_moved[affected_index] = FROM;
        ++affected_index;
        temp_net_cost[inet] = 1.0; /* Flag to say we've marked this net. */
    }

    if (to_block != EMPTY) {
        for (k = 0; k < num_of_pins; ++k) {
            inet = blocks[to_block].nets[k];
            if (inet == OPEN || is_global[inet] == TRUE) {
                continue;
            }

            if (temp_net_cost[inet] > 0.0) { /* Net already marked. */
                for (count = 0; count < affected_index; ++count) {
                    /* this net connect both FROM and TO blocks. */
                    if (nets_to_update[count] == inet) {
                        if (net_block_moved[count] == FROM) {
                            net_block_moved[count] = FROM_AND_TO;
                        }

                        break;
                    }
                }

#ifdef DEBUG
                if (count > affected_index) {
                    printf("Error in find_affected_nets -- count = %d,"
                           " affected index = %d.\n", count, affected_index);
                    exit(1);
                }

#endif
            } else { /* Net not marked yet. */
                nets_to_update[affected_index] = inet;
                net_block_moved[affected_index] = TO;
                ++affected_index;
                temp_net_cost[inet] = 1.;    /* Flag means we've  marked net. */
            }
        } /* end of for(k = 0; k < num_of_pins; ++k) */
    } /* end of if(to_block != EMPTY) */

    return (affected_index);
} /* end of static int find_affected_nets() */

/* Returns the point to which I want to swap, properly range limited.  *
 * rlim must always be between 1 and num_grid_columns(inclusive) for   *
 * this routine to  work.                                           */
static void find_to(int x_from,
                    int y_from,
                    int type,
                    double rlim,
                    int* x_to,
                    int* y_to)
{ 
    const int rlx = min(num_grid_columns, rlim); /* x_range_limit, Only needed when num_grid_columns < num_grid_rows. */
    const int rly = min(num_grid_rows, rlim);  /* y_range_limit, Added rly for aspect_ratio != 1 case. */

#ifdef DEBUG
    if (rlx < 1 || rlx > num_grid_columns) {
        printf("Error in find_to: rlx = %d\n", rlx);
        exit(1);
    }
#endif

    do {   /* Until (x_to, y_to) different from (x_from, y_from) */
        int x_rel = 0;
        int y_rel = 0;
        if (type == CLB_TYPE) {
            x_rel = my_irand(2 * rlx); /* x_rel >= 0 && x_rel <= 2 * rlx */
            y_rel = my_irand(2 * rly);
            /* why? Now I see. */
            /* When x_rel >= 0 && x_rel < rlx, (*x_to) = x_from - (rlx - x_rel), *
             * when x_rel >= rlx && x_rel <= 2 * rlx, (*x_to) = x_from + (x_rel - rlx)*/
            *x_to = x_from + (x_rel - rlx);
            *y_to = y_from + (y_rel - rly);

            if (*x_to > num_grid_columns) {
                *x_to = *x_to - num_grid_columns;    /* better spectral props. */
            }
            if (*x_to < 1) {
                *x_to = *x_to + num_grid_columns;    /* than simple min, max   */
            }

            if (*y_to > num_grid_rows) {
                *y_to = *y_to - num_grid_rows;    /* clipping. */
            }
            if (*y_to < 1) {
                *y_to = *y_to + num_grid_rows;
            }
        } else { /* IO_TYPE blocks will be moved. */
            int iside = 0;
            int iplace = 0;
            if (rlx >= num_grid_columns) { /* x_range_limit >= num_columns */
                iside = my_irand(3); /* the random number was: 0,1,2,3 */
                /********************************
                 *       +-----1----+           *
                 *       |          |           *
                 *       |          |           *
                 *       0          2           *
                 *       |          |           *
                 *       |          |           *
                 *       +-----3----+           *
                 ********************************/
                switch (iside) {
                    case 0:
                        iplace = my_irand(num_grid_rows - 1) + 1;
                        *x_to = 0;
                        *y_to = iplace;
                        break;

                    case 1:
                        iplace = my_irand(num_grid_columns - 1) + 1;
                        *x_to = iplace;
                        *y_to = num_grid_rows + 1;
                        break;

                    case 2:
                        iplace = my_irand(num_grid_rows - 1) + 1;
                        *x_to = num_grid_columns + 1;
                        *y_to = iplace;
                        break;

                    case 3:
                        iplace = my_irand(num_grid_columns - 1) + 1;
                        *x_to = iplace;
                        *y_to = 0;
                        break;

                    default:
                        printf("Error in find_to.  Unexpected io swap location.\n");
                        exit(1);
                } /* end of switch(iside) */
            } else { /* rlx is less than whole chip */
                if (x_from == 0) {
                    iplace = my_irand(2 * rly);
                    *y_to = y_from + (iplace - rly);
                    *x_to = x_from;

                    if (*y_to > num_grid_rows) {
                        *y_to = num_grid_rows + 1; /* Attention, num_grid_rows+1 was top iopads */
                        *x_to = my_irand(rlx - 1) + 1;
                    } else if (*y_to < 1) {
                        *y_to = 0;
                        *x_to = my_irand(rlx - 1) + 1;
                    }
                } else if (x_from == num_grid_columns + 1) {
                    iplace = my_irand(2 * rly);
                    *y_to = y_from + (iplace - rly);
                    *x_to = x_from;

                    if (*y_to > num_grid_rows) {
                        *y_to = num_grid_rows + 1;
                        *x_to = num_grid_columns - my_irand(rlx - 1);
                    } else if (*y_to < 1) {
                        *y_to = 0;
                        *x_to = num_grid_columns - my_irand(rlx - 1);
                    }
                } else if (y_from == 0) {
                    iplace = my_irand(2 * rlx);
                    *x_to = x_from + (iplace - rlx);
                    *y_to = y_from;

                    if (*x_to > num_grid_columns) {
                        *x_to = num_grid_columns + 1;
                        *y_to = my_irand(rly - 1) + 1;
                    } else if (*x_to < 1) {
                        *x_to = 0;
                        *y_to = my_irand(rly - 1) + 1;
                    }
                } else { /* *y_from == num_grid_rows + 1 */
                    iplace = my_irand(2 * rlx);
                    *x_to = x_from + (iplace - rlx);
                    *y_to = y_from;

                    if (*x_to > num_grid_columns) {
                        *x_to = num_grid_columns + 1;
                        *y_to = num_grid_rows - my_irand(rly - 1);
                    } else if (*x_to < 1) {
                        *x_to = 0;
                        *y_to = num_grid_rows - my_irand(rly - 1);
                    }
                }
            }  /* End of else(rlx is less than whole chip) */
        } /* end of else(io_block to be moved) */
    } while ((x_from == *x_to) && (y_from == *y_to));

#ifdef DEBUG
    if (*x_to < 0 || *x_to > num_grid_columns + 1 || *y_to < 0 || *y_to > num_grid_rows + 1) {
        printf("Error in routine find_to:  (x_to,y_to) = (%d,%d)\n",
               *x_to, *y_to);
        exit(1);
    }

    if (type == CLB_TYPE) {
        if (clb_grids[*x_to][*y_to].type != CLB_TYPE) {
            printf("Error: Moving CLB_TYPE to illegal type blocks at (%d,%d)\n",
                   *x_to, *y_to);
            exit(1);
        }
    } else {
        if (clb_grids[*x_to][*y_to].type != IO_TYPE) {
            printf("Error: Moving IO_TYPE blocks to illegal type location at "
                   "(%d,%d)\n", *x_to, *y_to);
            exit(1);
        }
    }

#endif
    /* printf("(%d,%d) moved to (%d,%d)\n",x_from,y_from,*x_to,*y_to); */
} /* end of static void find_to() */

static int assess_swap(double delta_cost,
                       double t)
{
    /* Returns: 1 -> move accepted, 0 -> rejected. */
    int accept = -1;
    double fnum;

    if (delta_cost <= 0) {
#ifdef SPEC          /* Reduce variation in final solution due to round off */
        fnum = my_frand();
#endif
        accept = 1;
        return(accept);
    }

    if (t == 0.) {
        return(0);
    }

    fnum = my_frand();
    double prob_fac = exp(-delta_cost / t);
    if (prob_fac > fnum) {
        accept = 1;
    } else {
        accept = 0;
    }

    return accept;
} /* end of static int assess_swap(double delta_cost, ) */


/* Recomputes the cost to eliminate roundoff that may have accrued.  *
 * This routine does as little work as possible to compute this new  *
 * cost.                                                             */
static double recompute_bb_cost(int place_cost_type,
                                int num_regions)
{
    double cost = 0;
    /* Initialize occupancies to zero if regions are being used. */
    int i, j, inet;
    if (place_cost_type == NONLINEAR_CONG) {
        for (i = 0; i < num_regions; ++i) {
            for (j = 0; j < num_regions; ++j) {
                place_region_x[i][j].occupancy = 0.;
                place_region_y[i][j].occupancy = 0.;
            }
        }
    }

    for (inet = 0; inet < num_nets; ++inet) { /* for each net ... */
        if (is_global[inet] == FALSE) {    /* Do only if not global. */
            /* Bounding boxes don't have to be recomputed; they're correct. */
            if (place_cost_type != NONLINEAR_CONG) {
                cost += net_cost[inet];
            } else {    /* Must be nonlinear_cong case. */
                update_region_occ(inet,
                                  &bb_coords[inet],
                                  1,
                                  num_regions);
            }
        }
    }

    if (place_cost_type == NONLINEAR_CONG) {
        cost = nonlinear_cong_cost(num_regions);
    }

    return cost;
} /* end of static double recompute_bb_cost(int place_cost_type, ) */

/* FIXME: compute a subnet Tdel from source pin to sink pin according to *
 * find (delta_x, delta_y) in Tdel lookup matrix                         *
 * TODO: In vpr4.3, due to FPGA arch was homogeneous arch, when the arch  *
 * change to heterogeneous one, How did I compute bb_cost and td_cost?    */
static double compute_point_to_point_delay(int inet,
                                           int sink_pin)
{
    int source_block_index = net[inet].blocks[0];
    block_types_t source_type = blocks[source_block_index].type; /* it maybe plb, or io pads*/

    int sink_block_index = net[inet].blocks[sink_pin];
    block_types_t sink_type = blocks[sink_block_index].type;

    int delta_x = abs(blocks[sink_block_index].x - blocks[source_block_index].x);
    int delta_y = abs(blocks[sink_block_index].y - blocks[source_block_index].y);

    /* from clb_outpin_to_clb_inpin, delta_clb_to_clb[delta_x][delta_y]. */
    double delay_source_to_sink = 0.0;
    if (source_type == CLB_TYPE) {
        if (sink_type == CLB_TYPE) { /* from clb_outpin to clb input_pin */
            delay_source_to_sink = delta_clb_to_clb[delta_x][delta_y];
        } else if (sink_type == OUTPAD_TYPE) { /* from clb_outpin to output pad */
        /* from clb_outpin_to_outpad, delta_clb_to_outpad[delta_x][delta_y] */
            delay_source_to_sink = delta_clb_to_outpad[delta_x][delta_y];
        } else {  /* NO clb_to_inpad */
            printf("Error in compute_point_to_point_delay in place.c, \
                    cannot from clb output pin to input pad\n");
            exit(1);
        }
    } else if (source_type == INPAD_TYPE) {
        if (sink_type == CLB_TYPE) { /* from input pad to clb input pin */
            delay_source_to_sink = delta_inpad_to_clb[delta_x][delta_y];
        } else if (sink_type == OUTPAD_TYPE) { /* from input pad to output pad */
            delay_source_to_sink = delta_inpad_to_outpad[delta_x][delta_y];
        } else { /* NO inpad_to_inpad */
            printf("Error in compute_point_to_point_delay in place.c, \
                    cannot from input pad to input pad\n");
            exit(1);
        }
    } else {  /* NO outpad to others */
        printf("Error in compute_point_to_point_delay in place.c, bad source_type\n");
        exit(1);
    }

    if (delay_source_to_sink < 0.0) {
        printf("Error in compute_point_to_point_delay in place.c, Tdel is less than 0\n");
        exit(1);
    }

    return delay_source_to_sink;
} /* end of static double compute_point_to_point_delay() */

/* Update point_to_point_delay_cost and point_to_point_timing_cost, *
 * when from_block and to_block had moved.                                  */
static void update_timing_driven_cost(int from_block,
                                      int to_block,
                                      int num_of_pins)
{
    /* Update the point_to_point_timing_cost values from the temporary *
     * values for all connections that have changed */
    int blkpin, net_pin, inet, ipin;
    for (blkpin = 0; blkpin < num_of_pins; ++blkpin) {
        inet = blocks[from_block].nets[blkpin];
        if (inet == OPEN || is_global[inet] == TRUE) {
            continue;
        }

        net_pin = net_pin_index[from_block][blkpin];
        if (net_pin != 0) {
            /*the following "if" prevents the value from being updated twice*/
            if (net[inet].blocks[0] != to_block
                  && net[inet].blocks[0] != from_block) {
                point_to_point_delay_cost[inet][net_pin] =
                        temp_point_to_point_delay_cost[inet][net_pin];
                temp_point_to_point_delay_cost[inet][net_pin] = -1;

                point_to_point_timing_cost[inet][net_pin] =
                        temp_point_to_point_timing_cost[inet][net_pin];
                temp_point_to_point_timing_cost[inet][net_pin] = -1;
            }
        } else { /*this net is being driven by a moved blocks, recompute */
            /*all point to point connections on this net.*/
            for (ipin = 1; ipin < net[inet].num_pins; ++ipin) {
                point_to_point_delay_cost[inet][ipin] =
                        temp_point_to_point_delay_cost[inet][ipin];
                temp_point_to_point_delay_cost[inet][ipin] = -1;

                point_to_point_timing_cost[inet][ipin] =
                        temp_point_to_point_timing_cost[inet][ipin];
                temp_point_to_point_timing_cost[inet][ipin] = -1;
            }
        }
    } /* end of for(blkpin = 0; blkpin < num_of_pins; blkpin++) */

    if (to_block != EMPTY) {
        for (blkpin = 0; blkpin < num_of_pins; ++blkpin) {
            inet = blocks[to_block].nets[blkpin];
            if (inet == OPEN || is_global[inet] == TRUE) {
                continue;
            }

            net_pin = net_pin_index[to_block][blkpin];
            if (net_pin != 0) {
                /*the following "if" prevents the value from being updated 2x*/
                if (net[inet].blocks[0] != to_block
                      && net[inet].blocks[0] != from_block) {
                    point_to_point_delay_cost[inet][net_pin] =
                        temp_point_to_point_delay_cost[inet][net_pin];
                    temp_point_to_point_delay_cost[inet][net_pin] = -1;

                    point_to_point_timing_cost[inet][net_pin] =
                        temp_point_to_point_timing_cost[inet][net_pin];
                    temp_point_to_point_timing_cost[inet][net_pin] = -1;
                }
            } else { /*this net is being driven by a moved blocks, recompute */
                /*all point to point connections on this net.*/
                for (ipin = 1; ipin < net[inet].num_pins; ++ipin) {
                    point_to_point_delay_cost[inet][ipin] =
                        temp_point_to_point_delay_cost[inet][ipin];
                    temp_point_to_point_delay_cost[inet][ipin] = -1;

                    point_to_point_timing_cost[inet][ipin] =
                        temp_point_to_point_timing_cost[inet][ipin];
                    temp_point_to_point_timing_cost[inet][ipin] = -1;
                }
            }
        }
    } /* end of if(to_block != EMPTY) */
}  /* end of static void update_timing_driven_cost() */

/* TODO: compute the timing-cost incrmentally, this function was very important! */
/* A net that is being driven by a moved blocks must have all of its sinks timing *
 * costs recomputed. A net that is driving a moved blocks must only have the      *
 * timing cost on the connection driving the input pin computed.                 */
static void compute_delta_timing_driven_cost(place_algorithm_t place_algorithm,
                                             int  from_block,
                                             int  to_block,
                                             int  num_of_pins,
                                             double* delta_timing,
                                             double* delta_delay)
{
    int inet, net_pin, ipin;
    double temp_delay = 0.0;
    double delta_timing_cost = 0.0;
    double delta_delay_cost = 0.0;
    int k = -1;
    for (k = 0; k < num_of_pins; ++k) { /* num_of_pins was pins of from blocks */
        inet = blocks[from_block].nets[k];
        if (inet == OPEN || is_global[inet]) {
            continue;
        }

        net_pin = net_pin_index[from_block][k]; /* it was pin index */
        if (net_pin != 0) { /*this net is driving a moved blocks */
        /*if this net is being driven by a blocks that has moved, we do not  */
        /*need to compute the change in the timing cost (here) since it will*/
        /*be computed in the fanout of the net on  the driving blocks, also  */
        /*computing it here would double count the change, and mess up the  */
        /*delta_timing_cost value */
            if (net[inet].blocks[0] != to_block
                  && net[inet].blocks[0] != from_block) {
                temp_delay = compute_point_to_point_delay(inet,
                                                          net_pin);
                temp_point_to_point_delay_cost[inet][net_pin] = temp_delay;

                if (place_algorithm == NEW_TIMING_DRIVEN_PLACE) {
                    temp_point_to_point_timing_cost[inet][net_pin] =
                      subnet_local_crit_weight[inet][net_pin] * temp_delay;
                } else {
                    temp_point_to_point_timing_cost[inet][net_pin] =
                      timing_place_crit[inet][net_pin] * temp_delay;
                }

                delta_delay_cost += temp_point_to_point_delay_cost[inet][net_pin]
                                      - point_to_point_delay_cost[inet][net_pin];
                delta_timing_cost += temp_point_to_point_timing_cost[inet][net_pin]
                                       - point_to_point_timing_cost[inet][net_pin];
            }
        } else { /*this net is being driven by a moved blocks, recompute */
            /*Why?  all point to point connections on this net.*/
            for (ipin = 1; ipin < net[inet].num_pins; ++ipin) {
                temp_delay = compute_point_to_point_delay(inet,
                                                          ipin);
                temp_point_to_point_delay_cost[inet][ipin] = temp_delay;

                if (place_algorithm == NEW_TIMING_DRIVEN_PLACE) {
                    temp_point_to_point_timing_cost[inet][ipin] =
                      subnet_local_crit_weight[inet][ipin] * temp_delay;
                } else {
                    temp_point_to_point_timing_cost[inet][ipin] =
                      timing_place_crit[inet][ipin] * temp_delay;
                }

                delta_delay_cost += temp_point_to_point_delay_cost[inet][ipin]
                                      - point_to_point_delay_cost[inet][ipin];
                delta_timing_cost += temp_point_to_point_timing_cost[inet][ipin]
                                       - point_to_point_timing_cost[inet][ipin];
            }
        }
    } /* end of for (k = 0; k < num_of_pins; k++) */

    if (to_block != EMPTY) {
        for (k = 0; k < num_of_pins; ++k) {
            inet = blocks[to_block].nets[k];
            if (inet == OPEN || is_global[inet]) {
                continue;
            }

            net_pin = net_pin_index[to_block][k];
            if (net_pin != 0) { /*this net is driving a moved blocks*/
            /*if this net is being driven by a blocks that has moved, we do not */
            /*need to compute the change in the timing cost (here) since it was*/
            /*computed in the fanout of the net on  the driving blocks, also    */
            /*computing it here would double count the change, and mess up the */
            /*delta_timing_cost value */
                if (net[inet].blocks[0] != to_block
                      && net[inet].blocks[0] != from_block) {
                    temp_delay = compute_point_to_point_delay(inet,
                                                              net_pin);
                    temp_point_to_point_delay_cost[inet][net_pin] = temp_delay;

                    if (place_algorithm == NEW_TIMING_DRIVEN_PLACE) {
                        temp_point_to_point_timing_cost[inet][net_pin] =
                          subnet_local_crit_weight[inet][net_pin] * temp_delay;
                    } else {
                        temp_point_to_point_timing_cost[inet][net_pin] =
                          timing_place_crit[inet][net_pin] * temp_delay;
                    }

                    delta_delay_cost += temp_point_to_point_delay_cost[inet][net_pin]
                                          - point_to_point_delay_cost[inet][net_pin];
                    delta_timing_cost += temp_point_to_point_timing_cost[inet][net_pin]
                                           - point_to_point_timing_cost[inet][net_pin];
                }
            } else { /*this net is being driven by a moved blocks, recompute */
                /*Why?  all point to point connections on this net.*/
                for (ipin = 1; ipin < net[inet].num_pins; ++ipin) {
                    temp_delay = compute_point_to_point_delay(inet,
                                                              ipin);
                    temp_point_to_point_delay_cost[inet][ipin] = temp_delay;

                    if (place_algorithm == NEW_TIMING_DRIVEN_PLACE) {
                        temp_point_to_point_timing_cost[inet][ipin] =
                          subnet_local_crit_weight[inet][ipin] * temp_delay;
                    } else {
                        temp_point_to_point_timing_cost[inet][ipin] =
                          timing_place_crit[inet][ipin] * temp_delay;
                    }

                    delta_delay_cost += temp_point_to_point_delay_cost[inet][ipin]
                                          - point_to_point_delay_cost[inet][ipin];

                    delta_timing_cost += temp_point_to_point_timing_cost[inet][ipin]
                                           - point_to_point_timing_cost[inet][ipin];
                }
            } /* end of else */
        }  /* end of for(k = 0; k < num_of_pins; k++) */
    } /* end of if (to_block != EMPTY) */

    *delta_timing = delta_timing_cost;
    *delta_delay = delta_delay_cost;
}  /* end of static void compute_delta_td_cost() */

/* FIXME: compute timing-driven costs of all signal nets(its all subnets)  *
 * Attention: I found this function was called by NET_TIMING_DRIVEN_PLACE. */
/* Computes the cost(from scratch) due to the Tdels and criticalities on all  *
 * point-to-point connections, we define the timing cost of each connection as *
 * criticality * Tdel.                                                        */
static void compute_timing_driven_cost_by_orig_algo(double* timing_cost,
                                                    double* connection_delay_sum)
{
    double local_timing_cost = 0.0;
    double local_connect_delay_sum = 0.0;

    int inet = -1;
    for (inet = 0; inet < num_nets; ++inet) { /* for each net ... */
        if (is_global[inet] == FALSE) { /* Do only for signal nets. */
            int ipin = 0;
            for (ipin = 1; ipin < net[inet].num_pins; ++ipin) {
                /* FIXME: subnet_delay_cost was a subnet(in a net) delay_cost */
                double subnet_delay_cost = compute_point_to_point_delay(inet,
                                                                        ipin);
                point_to_point_delay_cost[inet][ipin] = subnet_delay_cost;
                local_connect_delay_sum += subnet_delay_cost;

                temp_point_to_point_delay_cost[inet][ipin] = -1; /*undefined*/

                /* subnet_timing_cost was a subnet(in a net) timing_cost */
                double subnet_timing_cost =
                           subnet_delay_cost * timing_place_crit[inet][ipin];
                point_to_point_timing_cost[inet][ipin] = subnet_timing_cost;
                local_timing_cost += subnet_timing_cost;

                /* TODO: Why? */
                temp_point_to_point_timing_cost[inet][ipin] = -1; /*undefined*/
            }
        } /* end of if(is_global[inet] == FALSE) */
    } /* end of for (inet = 0; inet < num_nets; ++inet) */

    *timing_cost = local_timing_cost;
    *connection_delay_sum = local_connect_delay_sum;
} /* end of static void compute_timing_driven_cost_by_orig_algo() */


/* FIXME: This function was used for calculate Timing-Driven_Placement by *
 * PATH algorithm, which noted at "A Novel Net Weighting Algorithm for    *
 * Timing-Driven Placement", Tim Kong, 2002.                              */
static void compute_timing_driven_costs_by_path_algo(double* timing_cost,
                                                     double* connection_delay_sum)
{
    double local_timing_cost = 0.0;
    double local_connect_delay_sum = 0.0;

    int inet = -1;
    for (inet = 0; inet < num_nets; ++inet) {
        if (is_global[inet] == FALSE) {
            int ipin = 0;
            for (ipin = 1; ipin < net[inet].num_pins; ++ipin) {
                double subnet_delay_cost = compute_point_to_point_delay(inet,
                                                                        ipin);
                point_to_point_delay_cost[inet][ipin] = subnet_delay_cost;
                local_connect_delay_sum += subnet_delay_cost;

                temp_point_to_point_delay_cost[inet][ipin] = -1;

                double subnet_timing_cost =
                    subnet_delay_cost * subnet_local_crit_weight[inet][ipin];

                point_to_point_timing_cost[inet][ipin] = subnet_timing_cost;
                local_timing_cost += subnet_timing_cost;

                temp_point_to_point_timing_cost[inet][ipin] = -1;
            }  /* end of for(ipin = 1; ipin < net[inet].num_pins; ++ipin) */
        } /* end of if(is_global[inet] == FALSE) */
    } /* end of for(inet = 0; inet < num_nets; ++inet) */

    *timing_cost = local_timing_cost;
    *connection_delay_sum = local_connect_delay_sum;
}  /* end of static void compute_timing_driven_costs_by_path_algo() */

/* Finds the cost from scratch. Done only when the placement has been     *
 * radically changed(i.e. after initial placement). Otherwise find the    *
 * cost change Incrementally. If method check is NORMAL, we find bounding *
 * -boxes that are updateable for the larger nets. If the method is CHECK,*
 * all bounding-boxes are found via the non_updateable_bb routine, to     *
 * provide a cost which can be used to chekc the correctness of the other *
 * routine.                                                               */
double compute_bb_cost(int method,
                       int place_cost_type,
                       int num_regions)
{
    /* Initialize occupancies to zero if regions are being used. */
    if (place_cost_type == NONLINEAR_CONG) {
        int i = -1;
        int j = -1;
        for (i = 0; i < num_regions; ++i) {
            for (j = 0; j < num_regions; ++j) {
                place_region_x[i][j].occupancy = 0.;
                place_region_y[i][j].occupancy = 0.;
            }
        }
    }

    int n = -1;
    double cost = 0.0;
    for (n = 0; n < num_nets; ++n) { /* for each net ... */
        if (is_global[n] == FALSE) { /* Do only if not global. */
            /* Small nets don't use incremental updating on their bounding boxes, *
             * so they can use a fast bounding box calculator.                    */
            if (net[n].num_pins > SMALL_NET && method == NORMAL) { /* 4 */
                get_bb_from_scratch(n,
                                    &bb_coords[n],
                                    &bb_num_on_edges[n]);
            } else {
                get_non_updateable_bb(n,
                                      &bb_coords[n]);
            }

            if (place_cost_type != NONLINEAR_CONG) {
                net_cost[n] = get_net_cost(n,
                                           &bb_coords[n]);
                cost += net_cost[n];
            } else {    /* Must be nonlinear_cong case. */
                update_region_occ(n,
                                  &bb_coords[n],
                                  1,
                                  num_regions);
            }
        }
    }

    if (place_cost_type == NONLINEAR_CONG) {
        cost = nonlinear_cong_cost(num_regions);
    }

    return cost;
} /* end of static double compute_bb_cost(int method,) */


/* This routine computes the cost of a placement when the NONLINEAR_CONG *
 * option is selected.  It assumes that the occupancies of all the       *
 * placement subregions have been properly updated, and simply           *
 * computes the cost due to these occupancies by summing over all        *
 * subregions.  This will be inefficient for moves that don't affect     *
 * many subregions (i.e. small moves late in placement), esp. when there *
 * are a lot of subregions.  May recode later to update only affected    *
 * subregions.                                                           */
static double nonlinear_cong_cost(int num_regions)
{
    int i, j;
    double cost = 0.;

    for (i = 0; i < num_regions; i++) {
        for (j = 0; j < num_regions; j++) {
            /* Many different cost metrics possible.  1st try:  */
            if (place_region_x[i][j].occupancy < place_region_x[i][j].capacity) {
                cost += place_region_x[i][j].occupancy *
                        place_region_x[i][j].inv_capacity;
            } else { /* Overused region -- penalize. */
                double tmp = place_region_x[i][j].occupancy *
                              place_region_x[i][j].inv_capacity;
                cost += tmp * tmp;
            }

            if (place_region_y[i][j].occupancy < place_region_y[i][j].capacity) {
                cost += place_region_y[i][j].occupancy *
                        place_region_y[i][j].inv_capacity;
            } else { /* Overused region -- penalize. */
                double tmp = place_region_y[i][j].occupancy *
                              place_region_y[i][j].inv_capacity;
                cost += tmp * tmp;
            }
        }
    }

    return cost;
}  /* end of static double nonlinear_cong_cost(int num_regions) */


/* This routine finds the bounding-box of each net from scratch(i.e. from   *
 * only the blocks location information). It updates both the coordinate and *
 * number of blocks on each edge information. It should only be called when *
 * the bounding-box information is not valid.                               */
static void get_bb_from_scratch(int inet, bbox_t* coords,
                                bbox_t* num_on_edges)
{
    /* I need a list of blocks to which this net connects, with no blocks listed *
     * more than once, in order to get a proper count of the number on the edge *
     * of the bounding box.                                                     */
    int  n_pins;
    int* plist;
    if (duplicate_pins[inet] == 0) {
        plist = net[inet].blocks;
        n_pins = net[inet].num_pins;
    } else {
        plist = unique_pin_list[inet];
        n_pins = net[inet].num_pins - duplicate_pins[inet];
    }

    int x = blocks[plist[0]].x; /* plist[0] was source pin */
    int y = blocks[plist[0]].y;
    x = max(min(x, num_grid_columns), 1);
    y = max(min(y, num_grid_rows), 1);

    int xmin = x;
    int ymin = y;
    int xmax = x;
    int ymax = y;

    int xmin_edge = 1;
    int ymin_edge = 1;
    int xmax_edge = 1;
    int ymax_edge = 1;

    int ipin = 0;
    for (ipin = 1; ipin < n_pins; ++ipin) {
        int block_num = plist[ipin];
        x = blocks[block_num].x;
        y = blocks[block_num].y;
        /* Code below counts IO_TYPE blocks as being within the (1..num_grid_columns, 1..num_grid_rows clb) array. *
         * This is because channels do not go out of the 0..num_grid_columns, 0..num_grid_rows range, and I   *
         * always take all channels impinging on the bounding box to be within that  *
         * bounding box. Hence, this "movement" of IO_TYPE blocks does not affect the     *
         * which channels are included within the bounding box, and it simplifies the*
         * the code a lot.                                                           */
        x = max(min(x, num_grid_columns), 1);
        y = max(min(y, num_grid_rows), 1);

        if (x == xmin) {
            ++xmin_edge;
        } else if (x == xmax) { /* Recall that xmin could equal xmax -- don't use else */
            ++xmax_edge;
        } else if (x < xmin) {
            xmin = x;
            xmin_edge = 1;
        } else if (x > xmax) {
            xmax = x;
            xmax_edge = 1;
        }

        if (y == ymin) {
            ++ymin_edge;
        } else if (y == ymax) {
            ++ymax_edge;
        } else if (y < ymin) {
            ymin = y;
            ymin_edge = 1;
        } else if (y > ymax) {
            ymax = y;
            ymax_edge = 1;
        }
    } /* end of for (ipin = 1; ipin < n_pins; ++ipin) */

    /* Copy the coordinates and number on edges information into the proper   *
     * structures.                                                            */
    coords->xmin = xmin;
    coords->xmax = xmax;
    coords->ymin = ymin;
    coords->ymax = ymax;
    num_on_edges->xmin = xmin_edge;
    num_on_edges->xmax = xmax_edge;
    num_on_edges->ymin = ymin_edge;
    num_on_edges->ymax = ymax_edge;
} /* end of static void get_bb_from_scratch() */


static double get_net_cost(int inet, bbox_t* bbptr)
{
    /* Finds the cost due to one net by looking at its coordinate bounding  *
     * box.                                                                 */
    double ncost, crossing;

    /* Get the expected "crossing count" of a net, based on its number *
     * of pins.  Extrapolate for very large nets.                      */
    if (net[inet].num_pins > 50) {
        crossing = 2.7933 + 0.02616 * (net[inet].num_pins - 50);
        /*    crossing = 3.0;    Old value  */
    } else {
        crossing = cross_count[net[inet].num_pins - 1];
    }

    /* Could insert a check for xmin == xmax.  In that case, assume  *
     * connection will be made with no bends and hence no x-cost.    *
     * Same thing for y-cost.                                        */
    /* Cost = wire length along channel * cross_count / average      *
     * channel capacity.   Do this for x, then y direction and add.  */
    ncost = (bbptr->xmax - bbptr->xmin + 1) * crossing *
            chanx_place_cost_fac[bbptr->ymax][bbptr->ymin - 1];
    ncost += (bbptr->ymax - bbptr->ymin + 1) * crossing *
             chany_place_cost_fac[bbptr->xmax][bbptr->xmin - 1];
    return(ncost);
}  /* end of static double get_net_cost(int inet, bbox_t* bbptr) */


/* Finds the bounding box of a net and stores its coordinates in the  *
 * bb_coord_new data structure.  This routine should only be called   *
 * for small nets, since it does not determine enough information for *
 * the bounding box to be updated incrementally later.                *
 * Currently assumes channels on both sides of the CLBs forming the   *
 * edges of the bounding box can be used.  Essentially, I am assuming *
 * the pins always lie on the outside of the bounding box.            */
static void get_non_updateable_bb(int inet, bbox_t* bb_coord_new)
{
    int x = blocks[net[inet].blocks[0]].x;
    int y = blocks[net[inet].blocks[0]].y;
    int xmin = x;
    int ymin = y;
    int xmax = x;
    int ymax = y;

    int k = 0;
    for (k = 1; k < net[inet].num_pins; ++k) {
        x = blocks[net[inet].blocks[k]].x;
        y = blocks[net[inet].blocks[k]].y;

        if (x < xmin) {
            xmin = x;
        } else if (x > xmax) {
            xmax = x;
        }

        if (y < ymin) {
            ymin = y;
        } else if (y > ymax) {
            ymax = y;
        }
    }

    /* Now I've found the coordinates of the bounding box.  There are no *
     * channels beyond num_grid_columns and num_grid_rows, so I want to clip to that.  As well,   *
     * since I'll always include the channel immediately below and the   *
     * channel immediately to the left of the bounding box, I want to    *
     * clip to 1 in both directions as well (since minimum channel index *
     * is 0).  See route.c for a channel diagram.                        */
    bb_coord_new->xmin = max(min(xmin, num_grid_columns), 1);
    bb_coord_new->ymin = max(min(ymin, num_grid_rows), 1);
    bb_coord_new->xmax = max(min(xmax, num_grid_columns), 1);
    bb_coord_new->ymax = max(min(ymax, num_grid_rows), 1);
}  /* end of static void get_non_updateable_bb(int inet, bbox_t* bb_coord_new) */

/* Updates the bounding box of a net by storing its coordinates in    *
 * the bb_coord_new data structure and the number of blocks on each   *
 * edge in the bb_edge_new data structure.  This routine should only  *
 * be called for large nets, since it has some overhead relative to   *
 * just doing a brute force bounding box calculation.  The bounding   *
 * box coordinate and edge information for inet must be valid before  *
 * this routine is called.                                            *
 * Currently assumes channels on both sides of the CLBs forming the   *
 * edges of the bounding box can be used.  Essentially, I am assuming *
 * the pins always lie on the outside of the bounding box.            */
/* IO_TYPE blocks are considered to be one cell in for simplicity. */
static void update_bb(int inet,
                      bbox_t* bb_coord_new,
                      bbox_t* bb_edge_new,
                      int xold, int yold, int xnew, int ynew)
{
    xnew = max(min(xnew, num_grid_columns), 1);
    ynew = max(min(ynew, num_grid_rows), 1);
    xold = max(min(xold, num_grid_columns), 1);
    yold = max(min(yold, num_grid_rows), 1);

    /* Check if I can update the bounding box incrementally. */
    if (xnew < xold) {  /* Move to left. */
        /* Update the xmax fields for coordinates and number of edges first. */
        if (xold == bb_coords[inet].xmax) {  /* Old position at xmax. */
            if (bb_num_on_edges[inet].xmax == 1) {
                get_bb_from_scratch(inet, bb_coord_new, bb_edge_new);
                return;
            } else {
                bb_edge_new->xmax = bb_num_on_edges[inet].xmax - 1;
                bb_coord_new->xmax = bb_coords[inet].xmax;
            }
        } else {
            /* Move to left, old postion was not at xmax. */
            bb_coord_new->xmax = bb_coords[inet].xmax;
            bb_edge_new->xmax = bb_num_on_edges[inet].xmax;
        }

        /* Now do the xmin fields for coordinates and number of edges. */
        if (xnew < bb_coords[inet].xmin) {    /* Moved past xmin */
            bb_coord_new->xmin = xnew;
            bb_edge_new->xmin = 1;
        } else if (xnew == bb_coords[inet].xmin) { /* Moved to xmin */
            bb_coord_new->xmin = xnew;
            bb_edge_new->xmin = bb_num_on_edges[inet].xmin + 1;
        } else {                                /* Xmin unchanged. */
            bb_coord_new->xmin = bb_coords[inet].xmin;
            bb_edge_new->xmin = bb_num_on_edges[inet].xmin;
        }
    } else if (xnew > xold) {  /* Move to right. */
        /* Update the xmin fields for coordinates and number of edges first. */
        if (xold == bb_coords[inet].xmin) {   /* Old position at xmin. */
            if (bb_num_on_edges[inet].xmin == 1) {
                get_bb_from_scratch(inet, bb_coord_new, bb_edge_new);
                return;
            } else {
                bb_edge_new->xmin = bb_num_on_edges[inet].xmin - 1;
                bb_coord_new->xmin = bb_coords[inet].xmin;
            }
        } else {              /* Move to right, old position was not at xmin. */
            bb_coord_new->xmin = bb_coords[inet].xmin;
            bb_edge_new->xmin = bb_num_on_edges[inet].xmin;
        }

        /* Now do the xmax fields for coordinates and number of edges. */
        if (xnew > bb_coords[inet].xmax) {    /* Moved past xmax. */
            bb_coord_new->xmax = xnew;
            bb_edge_new->xmax = 1;
        } else if (xnew == bb_coords[inet].xmax) { /* Moved to xmax */
            bb_coord_new->xmax = xnew;
            bb_edge_new->xmax = bb_num_on_edges[inet].xmax + 1;
        } else {                                /* Xmax unchanged. */
            bb_coord_new->xmax = bb_coords[inet].xmax;
            bb_edge_new->xmax = bb_num_on_edges[inet].xmax;
        }
        /* End of move to right case. */
    } else {  /* xnew == xold -- no x motion. */
        bb_coord_new->xmin = bb_coords[inet].xmin;
        bb_coord_new->xmax = bb_coords[inet].xmax;
        bb_edge_new->xmin = bb_num_on_edges[inet].xmin;
        bb_edge_new->xmax = bb_num_on_edges[inet].xmax;
    }

    /* Now account for the y-direction motion. */
    if (ynew < yold) { /* Move down. */
        /* Update the ymax fields for coordinates and number of edges first. */
        if (yold == bb_coords[inet].ymax) {       /* Old position at ymax. */
            if (bb_num_on_edges[inet].ymax == 1) {
                get_bb_from_scratch(inet, bb_coord_new, bb_edge_new);
                return;
            } else {
                bb_edge_new->ymax = bb_num_on_edges[inet].ymax - 1;
                bb_coord_new->ymax = bb_coords[inet].ymax;
            }
        } else {            /* Move down, old postion was not at ymax. */
            bb_coord_new->ymax = bb_coords[inet].ymax;
            bb_edge_new->ymax = bb_num_on_edges[inet].ymax;
        }

        /* Now do the ymin fields for coordinates and number of edges. */
        if (ynew < bb_coords[inet].ymin) {    /* Moved past ymin */
            bb_coord_new->ymin = ynew;
            bb_edge_new->ymin = 1;
        } else if (ynew == bb_coords[inet].ymin) { /* Moved to ymin */
            bb_coord_new->ymin = ynew;
            bb_edge_new->ymin = bb_num_on_edges[inet].ymin + 1;
        } else {                                /* ymin unchanged. */
            bb_coord_new->ymin = bb_coords[inet].ymin;
            bb_edge_new->ymin = bb_num_on_edges[inet].ymin;
        }
        /* End of move down case. */
    } else if (ynew > yold) { /* Moved up. */
        /* Update the ymin fields for coordinates and number of edges first. */
        if (yold == bb_coords[inet].ymin) {   /* Old position at ymin. */
            if (bb_num_on_edges[inet].ymin == 1) {
                get_bb_from_scratch(inet, bb_coord_new, bb_edge_new);
                return;
            } else {
                bb_edge_new->ymin = bb_num_on_edges[inet].ymin - 1;
                bb_coord_new->ymin = bb_coords[inet].ymin;
            }
        } else {  /* Moved up, old position was not at ymin. */
            bb_coord_new->ymin = bb_coords[inet].ymin;
            bb_edge_new->ymin = bb_num_on_edges[inet].ymin;
        }

        /* Now do the ymax fields for coordinates and number of edges. */
        if (ynew > bb_coords[inet].ymax) {    /* Moved past ymax. */
            bb_coord_new->ymax = ynew;
            bb_edge_new->ymax = 1;
        } else if (ynew == bb_coords[inet].ymax) { /* Moved to ymax */
            bb_coord_new->ymax = ynew;
            bb_edge_new->ymax = bb_num_on_edges[inet].ymax + 1;
        } else {                                /* ymax unchanged. */
            bb_coord_new->ymax = bb_coords[inet].ymax;
            bb_edge_new->ymax = bb_num_on_edges[inet].ymax;
        }
        /* End of move up case. */
    } else { /* ynew == yold -- no y motion. */
        bb_coord_new->ymin = bb_coords[inet].ymin;
        bb_coord_new->ymax = bb_coords[inet].ymax;
        bb_edge_new->ymin = bb_num_on_edges[inet].ymin;
        bb_edge_new->ymax = bb_num_on_edges[inet].ymax;
    }
}  /* end of static void update_bb(int inet,) */


/* Allocates and loads the chanx_place_cost_fac and chany_place_cost_fac *
 * arrays with the inverse of the average_number_of_tracks_per_channel   *
 * between [subhigh] and [sublow].  This is only useful for the cost     *
 * function that takes the length of the net bounding box in each        *
 * dimension divided by the average number of tracks in that direction.  *
 * For other cost functions, you don't have to bother calling this       *
 * routine; when using the cost function described above, however, you   *
 * must always call this routine after you call init_channel_t and before*
 * you do any placement cost determination.  The place_cost_exp factor   *
 * specifies to what power the width of the channel should be taken --   *
 * larger numbers make narrower channels more expensive.                 */
static void alloc_and_load_for_fast_cost_update(double place_cost_exp)
{
    int low, high, i;

    /* Access arrays below as chan?_place_cost_fac[subhigh][sublow].  Since   *
     * subhigh must be greater than or equal to sublow, we only need to       *
     * allocate storage for the lower half of a matrix.                       */
    chanx_place_cost_fac = (double**)my_malloc((num_grid_rows + 1) * sizeof(double*));
    for (i = 0; i <= num_grid_rows; ++i) { /* why did it set 0~num_grid_rows for channel_x? */
        chanx_place_cost_fac[i] = (double*)my_malloc((i + 1) * sizeof(double));
    }

    /* Why did it set 0~num_grid_columns in channel_y? */
    chany_place_cost_fac = (double**)my_malloc((num_grid_columns + 1) * sizeof(double*));
    for (i = 0; i <= num_grid_columns; ++i) {
        chany_place_cost_fac[i] = (double*)my_malloc((i + 1) * sizeof(double));
    }

    /* First compute the number of tracks between channel high and channel *
     * low, inclusive, in an efficient manner.                             */
    chanx_place_cost_fac[0][0] = chan_width_x[0];

    for (high = 1; high <= num_grid_rows; ++high) {
        chanx_place_cost_fac[high][high] = chan_width_x[high]; /* ? */

        for (low = 0; low < high; ++low) {
            chanx_place_cost_fac[high][low] =
                chanx_place_cost_fac[high-1][low] + chan_width_x[high]; /* ? */
        }
    }

    /* Now compute the inverse of the average_number_of_tracks_per_channel *
     * between high and low.  The cost function divides by the average     *
     * number of tracks per channel, so by storing the inverse I convert   *
     * this to a faster multiplication.  Take this final number to the     *
     * place_cost_exp power -- numbers other than one mean this is no      *
     * longer a simple "average number of tracks"; it is some power of     *
     * that, allowing greater penalization of narrow channels.             */
    for (high = 0; high <= num_grid_rows; ++high)
        for (low = 0; low <= high; ++low) {
            chanx_place_cost_fac[high][low] =
                (high - low + 1.0) / chanx_place_cost_fac[high][low];
            chanx_place_cost_fac[high][low] =
                pow((double)chanx_place_cost_fac[high][low],
                    (double)place_cost_exp);
        }

    /* Now do the same thing for the y-directed channels.  First get the  *
     * number of tracks between channel high and channel low, inclusive.  */
    chany_place_cost_fac[0][0] = chan_width_y[0];
    for (high = 1; high <= num_grid_columns; ++high) {
        chany_place_cost_fac[high][high] = chan_width_y[high];
        for (low = 0; low < high; ++low) {
            chany_place_cost_fac[high][low] =
                chany_place_cost_fac[high - 1][low] + chan_width_y[high];
        }
    }

    /* Now compute the inverse of the average number of tracks per channel *
     * between high and low.  Take to specified power.                     */
    for (high = 0; high <= num_grid_columns; ++high) {
        for (low = 0; low <= high; ++low) {
            chany_place_cost_fac[high][low] =
                (high - low + 1.) / chany_place_cost_fac[high][low];
            chany_place_cost_fac[high][low] =
                pow((double)chany_place_cost_fac[high][low],
                    (double)place_cost_exp);
        }
    }
}  /* end of static void alloc_and_load_for_fast_cost_update(double place_cost_exp) */

static void free_fast_cost_update_structs(void)
{
    /* Frees the structures used to speed up evaluation of the nonlinear   *
     * congestion cost function.                                           */
    int i;
    for (i = 0; i <= num_grid_rows; i++) {
        free(chanx_place_cost_fac[i]);
    }
    free(chanx_place_cost_fac);

    for (i = 0; i <= num_grid_columns; i++) {
        free(chany_place_cost_fac[i]);
    }
    free(chany_place_cost_fac);
}  /* end of static void free_fast_cost_update_structs(void) */


/* Checks that the placement has not confused our data structures. *
 * i.e. the clb and blocks structures agree about the locations of *
 * every blocks, blocks are in legal spots, etc.  Also recomputes  *
 * the final placement cost from scratch and makes sure it is      *
 * within roundoff of what we think the cost is.                   */
static void check_place(const placer_opts_t*  placer_opts_ptr,
                        placer_costs_t* placer_costs_ptr)
{
    double bb_cost_check = compute_bb_cost(CHECK,
                                           placer_opts_ptr->place_cost_type,
                                           placer_opts_ptr->num_regions);
    printf("bb_cost recomputed from scratch is %g.\n",
           bb_cost_check);

    const double kbb_cost = placer_costs_ptr->m_bb_cost;
    int error = 0;
    if (fabs(bb_cost_check - kbb_cost) > kbb_cost * ERROR_TOL) {
        printf("Error:  bb_cost_check: %g and bb_cost: %g differ in check_place.\n",
               bb_cost_check, kbb_cost);
        ++error;
    }

    double timing_cost_check = 0.0;
    double delay_cost_check = 0.0;
    const place_algorithm_t place_algo = placer_opts_ptr->place_algorithm;
    if (place_algo == NET_TIMING_DRIVEN_PLACE || place_algo == PATH_TIMING_DRIVEN_PLACE
          || place_algo == NEW_TIMING_DRIVEN_PLACE) {
        if (place_algo == NEW_TIMING_DRIVEN_PLACE) {
            compute_timing_driven_costs_by_path_algo(&timing_cost_check,
                                                     &delay_cost_check);
        } else {
            compute_timing_driven_cost_by_orig_algo(&timing_cost_check,
                                        &delay_cost_check);
        }
        printf("timing_cost recomputed from scratch is %g. \n",
               timing_cost_check);

        const double ktiming_cost = placer_costs_ptr->m_timing_cost;
        if (fabs(timing_cost_check - ktiming_cost) > ktiming_cost * ERROR_TOL) {
            printf("Error:  timing_cost_check: %g and timing_cost: "
                   "%g differ in check_place.\n",
                   timing_cost_check,
                   ktiming_cost);
            ++error;
        }

        printf("delay_cost recomputed from scratch is %g. \n", delay_cost_check);
        const double kdelay_cost = placer_costs_ptr->m_delay_cost;
        if (fabs(delay_cost_check - kdelay_cost) > kdelay_cost * ERROR_TOL) {
            printf("Error:  delay_cost_check: %g and delay_cost: "
                   "%g differ in check_place.\n",
                   delay_cost_check,
                   kdelay_cost);
            ++error;
        }
    } /* end of check timing_driven_placement cost */

    static int* block_done;
    block_done = (int*)my_malloc(num_blocks * sizeof(int));

    int i, j, k;
    for (i = 0; i < num_blocks; ++i) {
        block_done[i] = 0;
    }

    /* Step through clb array. Check it against blocks array. */
    for (i = 0; i <= num_grid_columns + 1; ++i) {
        for (j = 0; j <= num_grid_rows + 1; ++j) {
            if (clb_grids[i][j].m_usage == 0) {
                continue;
            }

            if (clb_grids[i][j].type == CLB_TYPE) {
                int block_num = clb_grids[i][j].u.blocks;
                if (blocks[block_num].type != CLB_TYPE) {
                    printf("Error:  blocks %d type does not match clb(%d,%d) type.\n",
                           block_num, i, j);
                    ++error;
                }

                if ((blocks[block_num].x != i) || (blocks[block_num].y != j)) {
                    printf("Error:  blocks %d location conflicts with clb(%d,%d)"
                           "data.\n", block_num, i, j);
                    ++error;
                }

                if (clb_grids[i][j].m_usage > 1) {
                    printf("Error: clb(%d,%d) has occupancy of %d\n",
                           i, j, clb_grids[i][j].m_usage);
                    ++error;
                }

                block_done[block_num]++;
            } else { /* IO_TYPE blocks */
                if (clb_grids[i][j].m_usage > io_ratio) {
                    printf("Error:  clb(%d,%d) has occupancy of %d\n", i, j,
                           clb_grids[i][j].m_usage);
                    ++error;
                }

                for (k = 0; k < clb_grids[i][j].m_usage; ++k) {
                    int block_num = clb_grids[i][j].u.io_blocks[k];

                    if ((blocks[block_num].type != INPAD_TYPE) && blocks[block_num].type != OUTPAD_TYPE) {
                        printf("Error:  blocks %d type does not match clb(%d,%d) type.\n",
                               block_num, i, j);
                        ++error;
                    }

                    if ((blocks[block_num].x != i) || (blocks[block_num].y != j)) {
                        printf("Error:  blocks %d location conflicts with clb(%d,%d)"
                               "data.\n", block_num, i, j);
                        ++error;
                    }

                    block_done[block_num]++;
                }
            }  /* end of else IO_TYPE */
        } /* end of for(j = 0; j <= num_grid_rows + 1; ++j) */
    }  /* end of for(i = 0; i <= num_grid_columns+1; ++i) */

    /* Check that every blocks exists in the clb and blocks arrays somewhere. */
    for (i = 0; i < num_blocks; ++i)
        if (block_done[i] != 1) {
            printf("Error:  blocks %d listed %d times in data structures.\n",
                   i, block_done[i]);
            ++error;
        }

    free(block_done);

    if (error == 0) {
        printf("\nCompleted placement consistency check successfully.\n\n");
    } else {
        printf("\nCompleted placement consistency check, %d Errors found.\n\n",
               error);
        printf("Aborting program.\n");
        exit(1);
    }
}  /* end of static void check_place(double bb_cost, ) */

void read_place(char* place_file,
                char* net_file,
                char* arch_file,
                placer_opts_t placer_opts,
                router_opts_t router_opts,
                chan_width_distr_t chan_width_dist,
                detail_routing_arch_t det_routing_arch,
                segment_info_t* segment_inf,
                timing_info_t timing_inf,
                subblock_data_t* subblock_data_ptr)
{
    /* Reads in a previously computed placement of the circuit.  It      *
     * checks that the placement corresponds to the current architecture *
     * and netlist file.                                                 */
    int inet, ipin;
    double** dummy_x, **dummy_y;

    double** net_slack = NULL;
    double** net_delay = NULL;
    /*used to free net_delay if it is re-assigned*/
    double** remember_net_delay_original_ptr = NULL;
    if (placer_opts.place_algorithm == NET_TIMING_DRIVEN_PLACE
          || placer_opts.place_algorithm == PATH_TIMING_DRIVEN_PLACE
          || placer_opts.place_algorithm == NEW_TIMING_DRIVEN_PLACE
          || placer_opts.enable_timing_computations) {
        /*this must be called before alloc_and_load_placement_structs *
         *and parse_placement_file since it modifies the structures*/
        alloc_and_load_timing_graph(placer_opts,
                                    timing_inf,
                                    *subblock_data_ptr);
        net_slack = alloc_net_slack();

        alloc_delay_lookup_matrixes_and_criticalities(placer_opts,
                                                      *subblock_data_ptr,
                                                      chan_width_dist,
                                                      timing_inf,
                                                      router_opts,
                                                      det_routing_arch,
                                                      segment_inf,
                                                      &net_delay);
        remember_net_delay_original_ptr = net_delay;
    }

    /* First read in the placement.   */
    parse_placement_file(place_file,
                         net_file,
                         arch_file);
    /* Load the channel occupancies and cost factors so that:   *
     * (1) the cost check will be OK, and                       *
     * (2) the geometry will draw correctly.                    */
    int chan_width_factor = placer_opts.place_chan_width;
    init_channel_t(chan_width_factor, chan_width_dist);
    /* NB:  dummy_x and dummy_y used because I'll never use the old_place_occ *
     * arrays in this routine.  I need the placement structures loaded for    *
     * comp_cost and check_place to work.                                     */
    alloc_and_load_placement_structs(&placer_opts,
                                     &dummy_x,
                                     &dummy_y);
    /* Need cost in order to call check_place. */
    double bb_cost = compute_bb_cost(NORMAL,
                                     placer_opts.place_cost_type,
                                     placer_opts.num_regions);

    double delay_cost, timing_cost, est_crit;
    if (placer_opts.place_algorithm == NET_TIMING_DRIVEN_PLACE
          || placer_opts.place_algorithm == PATH_TIMING_DRIVEN_PLACE
          || placer_opts.place_algorithm == NEW_TIMING_DRIVEN_PLACE
          || placer_opts.enable_timing_computations) {
        for (inet = 0; inet < num_nets; ++inet)
            for (ipin = 1; ipin < net[inet].num_pins; ++ipin) {
                timing_place_crit[inet][ipin] = 0;    /*dummy crit values*/
            }

        net_delay = point_to_point_delay_cost;  /*this keeps net_delay up to date with the *
                         *same values that the placer is using     */
        load_timing_graph_net_delays(net_delay);
        est_crit = calc_all_vertexs_arr_req_time(0);
        compute_net_slacks(net_slack);

        compute_timing_driven_cost_by_orig_algo(&timing_cost,
                                                &delay_cost);  /*set up point_to_point_delay_cost*/

        printf("Placement. bb_cost: %g  delay_cost: %g.\n\n",
               bb_cost, delay_cost);
#ifdef PRINT_SINK_DELAYS
        print_sink_delays("Placement_Sink_Delays.echo");
#endif
#ifdef PRINT_NET_SLACKS
        print_net_slack("Placement_Net_Slacks.echo", net_slack);
#endif
#ifdef PRINT_PLACE_CRIT_PATH
        print_critical_path("Placement_Crit_Path.echo");
#endif
        printf("Placement Estimated Crit Path Delay: %g\n\n", est_crit);
    } else {
        timing_cost = 0;
        delay_cost = 0;
        printf("Placement bb_cost is %g.\n", bb_cost);
    }

    /* Attention, due to syntastic error, I comment check_place(), *
     * but it still important in check_place(), so I need fix this
     * bug later */
    /* check_place(&placer_opt,
                placer_costs_ptr); */

    free_placement_structs(placer_opts.place_cost_type,
                           placer_opts.num_regions,
                           dummy_x,
                           dummy_y,
                           placer_opts);

    if (placer_opts.place_algorithm == NET_TIMING_DRIVEN_PLACE ||
            placer_opts.place_algorithm == PATH_TIMING_DRIVEN_PLACE ||
            placer_opts.enable_timing_computations) {
        net_delay = remember_net_delay_original_ptr;
        free_lookups_and_criticalities(&placer_opts,
                                       &net_delay,
                                       &net_slack);
    }

    init_draw_coords((double) chan_width_factor);
    char msg[BUFSIZE] = "";
    sprintf(msg, "Placement from file %s.  bb_cost %g.", place_file,
            bb_cost);
    update_screen(MAJOR, msg, PLACEMENT, FALSE);
}

/****************************************************************************
 * The following functions were used for parallel placement                 *
 ***************************************************************************/

/***********************************************************************/
/* RESEARCH TODO: Bounding Box and range_limit need to be redone for    *
 * heterogeneous to prevent a QoR penalty */
/* Does almost all the work of placing a circuit. Width_fac gives the   *
 * width of the widest channel. Place_cost_exp says what exponent the   *
 * width should be taken to when calculating costs. This allows a       *
 * greater bias for anisotropic architectures. Place_cost_type          *
 * determines which cost function is used. num_regions is used only     *
 * the place_cost_type is NONLINEAR_CONG.                               */
void try_place_use_multi_threads(placer_opts_t      placer_opts,
                                 annealing_sched_t  annealing_sched,
                                 chan_width_distr_t chan_width_dist,
                                 router_opts_t      router_opts,
                                 detail_routing_arch_t det_routing_arch,
                                 segment_info_t*    segment_inf,
                                 timing_info_t      timing_inf,
                                 subblock_data_t*   subblock_data_ptr,
                                 operation_types_t  operation)
{
    /* Allocated here because it goes into timing critical code where each memory allocation is expensive */
    /* Used to quickly determine valid swap columns */
    printf("Now let's beginning parallel placement...\n");

    int* x_lookup = (int*)my_malloc(num_grid_columns * sizeof(int));

    /*used to free net_delay if it is re-assigned */
    double**  remember_net_delay_original_ptr = NULL;
    double**  net_slack = NULL;
    double**  net_delay = NULL;
    if (placer_opts.place_algorithm == NET_TIMING_DRIVEN_PLACE ||
        placer_opts.place_algorithm == PATH_TIMING_DRIVEN_PLACE ||
        placer_opts.enable_timing_computations) {
        /*do this before the initial placement to avoid messing up the initial placement */
        alloc_and_load_timing_graph(placer_opts,
                                    timing_inf,
                                    *subblock_data_ptr);

        net_slack = alloc_net_slack();
        assert(net_slack != NULL);

        alloc_delay_lookup_matrixes_and_criticalities(placer_opts,
                                                      *subblock_data_ptr,
                                                      chan_width_dist,
                                                      timing_inf,
                                                      router_opts,
                                                      det_routing_arch,
                                                      segment_inf,
                                                      &net_delay);
        remember_net_delay_original_ptr = net_delay;
    }

    int width_fac = placer_opts.place_chan_width;
    boolean fixed_pins; /* Can pads move or not? */
    if (placer_opts.pad_loc_type == FREE) {
        fixed_pins = FALSE;
    } else {
        fixed_pins = TRUE;
    }

    init_channel_t(width_fac,
                   chan_width_dist);

    double**  old_region_occ_x = NULL;
    double**  old_region_occ_y = NULL;
    alloc_and_load_placement_structs(&placer_opts,
                                     &old_region_occ_x,
                                     &old_region_occ_y);

    initial_placement(placer_opts.pad_loc_type,
                      placer_opts.pad_loc_file);
    init_draw_coords((double)width_fac);

    /* Storing the number of pins on each type of block makes the swap routine *
     * slightly more efficient.                                                */

    /* Gets initial cost and loads bounding boxes. */
    double bb_cost = comp_bb_cost(NORMAL,
                                 placer_opts.place_cost_type,
                                 placer_opts.num_regions);
    int    num_connections = 0;
    double crit_exponent = 0.0;
    double max_delay = 0.0;
    double place_delay_value = 0.0;
    double timing_cost, delay_cost;
    double  inverse_prev_bb_cost = 0.0;
    double inverse_prev_timing_cost = 0.0;
    double cost = 0.0;
    int inet = 0;
    int ipin = 0;
    if (placer_opts.place_algorithm == NET_TIMING_DRIVEN_PLACE ||
            placer_opts.place_algorithm == PATH_TIMING_DRIVEN_PLACE) {
        crit_exponent = placer_opts.td_place_exp_first; /*this will be modified when range_limit starts to change */

        compute_net_pin_index_values();

        num_connections = count_connections();
        printf("\nThere are %d point to point connections in this circuit\n\n",
                 num_connections);

        if (placer_opts.place_algorithm == NET_TIMING_DRIVEN_PLACE) {
            for (inet = 0; inet < num_nets; ++inet)
                for (ipin = 1; ipin <= net[inet].num_sinks; ++ipin) {
                    timing_place_crit[inet][ipin] = 0;    /*dummy crit values */
                }
           /* first pass gets delay_cost, which is used in criticality *
            * computations in the next call to comp_td_costs. */
            comp_td_costs(&timing_cost,
                          &delay_cost);
            place_delay_value = delay_cost / num_connections; /*used for computing criticalities */
            load_constant_net_delay(net_delay,
                                    place_delay_value);
        } else {
            place_delay_value = 0;
        }

        if (placer_opts.place_algorithm == PATH_TIMING_DRIVEN_PLACE) {
        /* This keeps net_delay up to date with the same values that *
         * the placer is using point_to_point_delay_cost is computed *
         * each that comp_td_costs is called, and is also updated    *
         * after any swap is accepted   */
            net_delay = point_to_point_delay_cost;
        }

        load_timing_graph_net_delays(net_delay);
        max_delay = load_net_slack(net_slack, 0);
        load_criticalities(placer_opts,
                           net_slack,
                           max_delay,
                           crit_exponent);

        /*now we can properly compute costs  */
        comp_td_costs(&timing_cost,
                      &delay_cost);

        inverse_prev_timing_cost = 1 / timing_cost;
        inverse_prev_bb_cost = 1 / bb_cost;
        cost = 1; /* our new cost function uses normalized values of  */
        /*bb_cost and timing_cost, the value of cost will be reset  */
        /*to 1 at each temperature when *_TIMING_DRIVEN_PLACE is true */
    } else {
        /*BOUNDING_BOX_PLACE */
        cost = bb_cost;
        timing_cost = delay_cost = num_connections = max_delay = 0;
        place_delay_value = crit_exponent = 0;

        inverse_prev_timing_cost = 0;   /*inverses not used */
        inverse_prev_bb_cost = 0;
    }

    /* Sometimes I want to run the router with a random placement.  Avoid *
     * using 0 moves to stop division by 0 and 0 length vector_t problems,  *
     * by setting move_lim to 1 (which is still too small to do any       *
     * significant optimization).                                         */
    int move_lim = (int)(10 * pow(num_blocks, 1.3333));
    if (move_lim <= 0) {
        move_lim = 1;
    }

    double range_limit = (double)max(num_grid_columns, num_grid_rows);
    double t = starting_t(&cost, &bb_cost, &timing_cost,
                          placer_opts.place_cost_type,
                          old_region_occ_x, old_region_occ_y,
                          placer_opts.num_regions, fixed_pins, annealing_sched,
                          move_lim, range_limit, placer_opts.place_algorithm,
                          placer_opts.timing_tradeoff, inverse_prev_bb_cost,
                          inverse_prev_timing_cost, &delay_cost);
    int tot_iter = 0;
    printf("Initial Placement Cost: %g bb_cost: %g td_cost: %g delay_cost: %g\n\n",
            cost, bb_cost, timing_cost, delay_cost);

#ifndef SPEC
    printf
    ("%11s  %10s %11s  %11s  %11s %11s  %11s %9s %8s  %7s  %7s  %10s  %7s\n",
     "T", "Cost", "Av. BB Cost", "Av. TD Cost", "Av Tot Del",
     "P to P Del", "max_delay", "Ac Rate", "Std Dev", "R limit", "Exp",
     "Tot. Moves", "Alpha");
    printf
    ("%11s  %10s %11s  %11s  %11s %11s  %11s %9s %8s  %7s  %7s  %10s  %7s\n",
     "--------", "----------", "-----------", "-----------",
     "---------", "----------", "-----", "-------", "-------",
     "-------", "-------", "----------", "-----");
#endif

    char msg[BUFSIZE];
    sprintf(msg,
            "Initial Placement.  Cost: %g  BB Cost: %g  TD Cost %g  Delay Cost: %g "
            "\t max_delay %g Channel Factor: %d", cost, bb_cost, timing_cost,
            delay_cost, max_delay, width_fac);
    update_screen(MAJOR, msg, PLACEMENT, FALSE);

    clock_t start_cpu = clock();
    struct  timeval start;
    gettimeofday(&start, NULL);
    my_srandom(0);

    int inner_iter_num = annealing_sched.inner_num;

    pthread_mutex_init(&global_data_access.mutex, NULL);
    barrier_polling_reset();

    /* horizon_regions is regions in horizontal direction, verti_regions is  *
     * regions in vertical direction. Horizon_regions and verti_regions also *
     * means that threads used in horizontal or vertical direction.          */
    const int horizon_regions = x_partition[NUM_OF_THREADS - 1];
    const int verti_regions = y_partition[NUM_OF_THREADS - 1];
    if (horizon_regions < 1 || verti_regions < 1 || NUM_OF_THREADS > 64) {
        printf("This program cannot be ran with %d threads, try another configuration.\n", NUM_OF_THREADS);
        assert (0);
    }
    assert(horizon_regions * verti_regions == NUM_OF_THREADS);

    /* iSize{X,Y} is the number of cols/rows assigned to each threads.
     * extra_cols is used to distribute the extra rows/cols when {x,y}_part is
     * not evenly divisible by {x,y}_part */
    const int cols_assign_to_thread = (num_grid_columns + 2) / horizon_regions;
    const int rows_assign_to_thread = (num_grid_rows + 2) / verti_regions;
    const int extra_cols =
                (num_grid_columns + 2) - cols_assign_to_thread * horizon_regions;
    const int extra_rows =
                (num_grid_rows + 2) - rows_assign_to_thread * verti_regions;

    int success_sum, exit;
    double success_rat;
    double av_cost, av_bb_cost, av_timing_cost, av_delay_cost, sum_of_squares;
    double std_dev;

    pthread_data_t thread_data_array[NUM_OF_THREADS];
    pthread_t place_threads[NUM_OF_THREADS];

    int thread_id = -1;
    for (thread_id = NUM_OF_THREADS - 1; thread_id >= 0 ; --thread_id) {
        /* for x_start */
        if (thread_id % horizon_regions == 0 || horizon_regions == 1) {
            thread_data_array[thread_id].x_start = 0;
        } else if (thread_id % horizon_regions == 1) {
            thread_data_array[thread_id].x_start = cols_assign_to_thread;
        } else if (thread_id % horizon_regions < extra_cols + 1) {
            thread_data_array[thread_id].x_start =
                (thread_id % horizon_regions - 1) * (cols_assign_to_thread + 1)
                    + cols_assign_to_thread;
        } else {
            thread_data_array[thread_id].x_start =
                (thread_id % horizon_regions) * cols_assign_to_thread + extra_cols;
        }
        /* for x_end */
        if (thread_id % horizon_regions == horizon_regions - 1 || horizon_regions == 1) {
            thread_data_array[thread_id].x_end = num_grid_columns + 2;
        } else if (thread_id % horizon_regions == 0) {
            thread_data_array[thread_id].x_end = cols_assign_to_thread;
        } else if (thread_id % horizon_regions < extra_cols + 1) {
            thread_data_array[thread_id].x_end =
                (thread_id % horizon_regions) * (cols_assign_to_thread + 1)
                    + cols_assign_to_thread;
        } else {
            thread_data_array[thread_id].x_end =
             (thread_id % horizon_regions + 1) * cols_assign_to_thread + extra_cols;
        }
        /* for y_start */
        if (thread_id / horizon_regions == 0 || verti_regions == 1) {
            thread_data_array[thread_id].y_start = 0;
        } else if (thread_id / horizon_regions == 1) {
            thread_data_array[thread_id].y_start = rows_assign_to_thread;
        } else if (thread_id / horizon_regions < extra_rows + 1) {
            thread_data_array[thread_id].y_start =
                (thread_id / horizon_regions - 1) * (rows_assign_to_thread + 1)
                    + rows_assign_to_thread;
        } else {
            thread_data_array[thread_id].y_start =
                (thread_id / horizon_regions) * rows_assign_to_thread + extra_rows;
        }
        /* for y_end */
        if (thread_id / horizon_regions == verti_regions - 1 || verti_regions == 1) {
            thread_data_array[thread_id].y_end = num_grid_rows + 2;
        } else if (thread_id / horizon_regions == 0) {
            thread_data_array[thread_id].y_end = rows_assign_to_thread;
        } else if (thread_id / horizon_regions < extra_rows + 1) {
            thread_data_array[thread_id].y_end =
                (thread_id / horizon_regions) * (rows_assign_to_thread + 1)
                     + rows_assign_to_thread;
        } else {
            thread_data_array[thread_id].y_end =
                ((thread_id / horizon_regions) + 1) * rows_assign_to_thread + extra_rows;
        }
        /* Initial region boundary OK */

        /* each region must be at least 8 X 8 */
        assert(thread_data_array[thread_id].x_end - thread_data_array[thread_id].x_start > 8);
        assert(thread_data_array[thread_id].y_end - thread_data_array[thread_id].y_start > 8);

        thread_data_array[thread_id].thread_id = thread_id;
        /* For each thread, it had these following global varibbles local copy, *
         * so it must synchronous with pthread mutex */
        thread_data_array[thread_id].placer_opts = placer_opts;
        thread_data_array[thread_id].annealing_sched = annealing_sched;
        thread_data_array[thread_id].fixed_pins = fixed_pins;
        thread_data_array[thread_id].net_slack = net_slack;
        thread_data_array[thread_id].net_delay = net_delay;

        thread_data_array[thread_id].t = &t;

        thread_data_array[thread_id].bb_cost = &bb_cost;
        thread_data_array[thread_id].timing_cost = &timing_cost;
        thread_data_array[thread_id].delay_cost = &delay_cost;
        thread_data_array[thread_id].inverse_prev_bb_cost = &inverse_prev_bb_cost;
        thread_data_array[thread_id].inverse_prev_timing_cost = &inverse_prev_timing_cost;
        thread_data_array[thread_id].av_cost = &av_cost;
        thread_data_array[thread_id].av_bb_cost = &av_bb_cost;
        thread_data_array[thread_id].av_timing_cost = &av_timing_cost;
        thread_data_array[thread_id].av_delay_cost = &av_delay_cost;
        thread_data_array[thread_id].cost = &cost;

        thread_data_array[thread_id].move_lim = &move_lim;
        thread_data_array[thread_id].inner_iter_num = &inner_iter_num;
        thread_data_array[thread_id].tot_iter = &tot_iter;
        thread_data_array[thread_id].success_sum = &success_sum;
        thread_data_array[thread_id].success_rat = &success_rat;
        thread_data_array[thread_id].sum_of_squares = &sum_of_squares;
        thread_data_array[thread_id].std_dev = &std_dev;

        thread_data_array[thread_id].place_delay_value = &place_delay_value;
        thread_data_array[thread_id].max_delay = &max_delay;
        thread_data_array[thread_id].num_connections = &num_connections;

        thread_data_array[thread_id].crit_exponent = &crit_exponent;
        thread_data_array[thread_id].exit = &exit;
        thread_data_array[thread_id].range_limit = &range_limit;

        assert(thread_data_array[thread_id].y_end > thread_data_array[thread_id].y_start);

        if (thread_id != 0) {
             pthread_create(&place_threads[thread_id],
                            NULL,
                            try_place_parallel,
                            (void*)&thread_data_array[thread_id]);
        } else {
            /* Why thread_id == 0, it needn't create pthread? */
            try_place_parallel(&thread_data_array[thread_id]);
        }
    } /* end of for(thread_id = NUM_OF_THREADS-1; thread_id >= 0; --thread_id) */

    for (thread_id = NUM_OF_THREADS - 1; thread_id > 0; --thread_id) {
        pthread_join(place_threads[thread_id], NULL);
    }

    pthread_mutex_destroy(&global_data_access.mutex);

    clock_t finish_cpu = clock();
    struct timeval  finish;
    gettimeofday(&finish, NULL);

    bb_cost = check_place(bb_cost,
                          timing_cost,
                          placer_opts.place_cost_type,
                          placer_opts.num_regions,
                          placer_opts.place_algorithm,
                          delay_cost);

    if (placer_opts.enable_timing_computations &&
            placer_opts.place_algorithm == BOUNDING_BOX_PLACE) {
        /*need this done since the timing data has not been kept up to date*
         *in bounding_box mode */
        for (inet = 0; inet < num_nets; ++inet)
            for (ipin = 1; ipin <= net[inet].num_sinks; ++ipin) {
                timing_place_crit[inet][ipin] = 0;    /*dummy crit values */
            }

        comp_td_costs(&timing_cost,
                      &delay_cost);  /*computes point_to_point_delay_cost */
    }

    double place_est_crit_delay = 0.0;
    if (placer_opts.place_algorithm == NET_TIMING_DRIVEN_PLACE ||
            placer_opts.place_algorithm == PATH_TIMING_DRIVEN_PLACE ||
            placer_opts.enable_timing_computations) {
        /*this makes net_delay up to date with    *
         *the same values that the placer is using*/
        net_delay = point_to_point_delay_cost;  
        load_timing_graph_net_delays(net_delay);
        place_est_crit_delay = load_net_slack(net_slack, 0);
#ifdef CREATE_ECHO_FILES
        /*      print_sink_delays("placement_sink_delays.echo"); */
        print_net_slack("placement_net_slacks.echo", net_slack);
        print_critical_path("placement_crit_path.echo",
                            *subblock_data_ptr);
#endif /* CREATE_ECHO_FILES */
        printf("Placement Estimated Crit Path Delay: %g\n\n",
               place_est_crit_delay);
    }

    sprintf(msg,
            "Placement. Cost: %g  bb_cost: %g td_cost: %g Channel Factor: %d max_delay: %g",
            cost, bb_cost, timing_cost, width_fac, max_delay);
    printf("Placement. Cost: %g  bb_cost: %g  td_cost: %g  delay_cost: %g.\n",
           cost, bb_cost, timing_cost, delay_cost);

    printf("inner loop wall: %f sec, cpu total: %f\n",
            my_difftime2(&start, &finish),
            (double)(finish_cpu - start_cpu) / CLOCKS_PER_SEC);
    update_screen(MAJOR, msg, PLACEMENT, FALSE);

#ifdef SPEC
    printf("Total moves attempted: %d.0\n", tot_iter);
#endif

    if (placer_opts.place_algorithm == NET_TIMING_DRIVEN_PLACE ||
            placer_opts.place_algorithm == PATH_TIMING_DRIVEN_PLACE ||
            placer_opts.enable_timing_computations) {
        net_delay = remember_net_delay_original_ptr;

        free_placement_structs(placer_opts.place_cost_type,
                               placer_opts.num_regions, old_region_occ_x,
                               old_region_occ_y, placer_opts);

        free_lookups_and_criticalities(&placer_opts,
                                       &net_delay,
                                       &net_slack);
    } 

    free(x_lookup);
}  /* end of try_place_use_multi_threads() */

/* ====== parallel placement functions ======*/
/* polling barriers */
/* Binary tree based barrier except node 0 and 1.
 * Barrier first waits for its children to arrive, then
 * wait for its parent to release them.
 * Sample structure for a tree with 8 nodes:
 *                0
 *                |
 *                1
 *               / \
 *              2   3
 *             / \  /\
 *            4  5 6  7
 */
static void barrier_polling(const int kthread_id)
{
    ++(thread_barriers[kthread_id].entry);

    const int local_entry = (thread_barriers[kthread_id].entry ) % 2;
    if (NUM_OF_THREADS == 1) {
        return;
    } else if (kthread_id == 0) {
        /* wait for kthread_id 1 finished */
        while (thread_barriers[1].arrived != local_entry) {};

        thread_barriers[1].proceed = local_entry;
    } else if (kthread_id == 1) {
        /* wait for children finished */
        if (NUM_OF_THREADS > 3) {
            while (thread_barriers[2].arrived != local_entry
                    || thread_barriers[3].arrived != local_entry) {};
        } else if (NUM_OF_THREADS > 2) {
            while (thread_barriers[2].arrived != local_entry) {};
        }

        /* signal my arrival */
        thread_barriers[kthread_id].arrived = local_entry;

        while (thread_barriers[kthread_id].proceed != local_entry) {};

        //release children
        if (NUM_OF_THREADS > 3) {
            thread_barriers[2].proceed = thread_barriers[3].proceed = local_entry;
        } else if (NUM_OF_THREADS > 2) {
            thread_barriers[2].proceed = local_entry;
        }
    } else { /* kthread_id > 1 */
        /* wait for children finished! */
        if (NUM_OF_THREADS > kthread_id * 2 + 1) {
            while (thread_barriers[kthread_id * 2].arrived != local_entry
                    || thread_barriers[kthread_id * 2 + 1].arrived != local_entry) {};
        } else if (NUM_OF_THREADS > kthread_id * 2) {
            while (thread_barriers[kthread_id * 2].arrived != local_entry) {};
        }

        //signal my arrival
        thread_barriers[kthread_id].arrived = local_entry;

        while (thread_barriers[kthread_id].proceed != local_entry) {};

        //release children
        if (NUM_OF_THREADS > kthread_id * 2 + 1) {
            thread_barriers[kthread_id * 2].proceed =
                thread_barriers[kthread_id * 2 + 1].proceed = local_entry;
        } else if (NUM_OF_THREADS > kthread_id * 2) {
            thread_barriers[kthread_id * 2].proceed = local_entry;
        }
    } /* end of else */
}  /* end of void barrier_polling(int kthread_id) */

static void barrier_polling_reset(void)
{
    int x = -1;
    for (x = 0; x < NUM_OF_THREADS; ++x) {
        thread_barriers[x].arrived = 0;
        thread_barriers[x].proceed = 0;
        thread_barriers[x].entry = 0;
    }
} /* end of void barrier_polling_reset() */

/* Placement using multi-threads parallely */
static void* try_place_parallel(pthread_data_t* input_args)
{
    int core_affinity = pow(2,
                            input_args->thread_id);
    /*core affinity - locking threads to a particular core*/
    if (pthread_setaffinity_np(pthread_self(),
                               sizeof(core_affinity),
                               &core_affinity) < 0) {
        printf("core affinity error: thread-%d couldn't get Core-%d\n",
               input_args->thread_id, core_affinity);
        exit(-1);
    }

    thread_local_common_paras_t  common_paras;
    int max_pins_per_fb = 0;
    /* initial_common_paras() initial all thread_data variables */
    initial_common_paras(input_args,
                         &max_pins_per_fb,
                         &common_paras);

    thread_local_data_for_swap_t  swap_data;
    /* allocate memory for swap_data */
    alloc_memory_for_swap_data(max_pins_per_fb,
                               &common_paras,
                               &swap_data);

    /* Initial 6 member variables in swap_data */
    initial_swap_data(&common_paras,
                      &swap_data);

    const int kthread_id = common_paras.local_thread_id;
    /* build the fan-in tree for each node for timing update */
    find_fanin_parallel(kthread_id);

    if (pthread_mutex_trylock(&global_data_access.mutex) != 0) {
        barrier_polling(kthread_id);
    } else {
    /*another way of storing data, in columns instead of rows. Why? */
        initial_localvert_grid();

        barrier_polling(kthread_id);

        pthread_mutex_unlock(&global_data_access.mutex);
    }

    /* program specific parameters */
    int prob = PROB; /* 10 */
    int timing_update_threshold = TIMING_UPDATE; /* 5 */
    /* ensures timing update will occurduring first iteration */
    int freq = timing_update_threshold;

    /*=========    NOW STARTING MAIN PLACEMENT  =======*/
    int iter = 0;
    int move_counter = -1;
    while (common_paras.local_temper != 0.0) {
        /*  First update parameters that used for controlloing placement! */
        common_paras.local_temper = *(input_args->t);
        int inner_iter_num = *(input_args->inner_iter_num);
        common_paras.local_crit_exponent = *(input_args->crit_exponent);
        common_paras.local_success_ratio = *(input_args->success_rat);
        common_paras.local_range_limit = *(input_args->range_limit);

        for (iter = 0; iter < inner_iter_num; ++iter) {
        /* FIXME, wait for all processors to finish the previous iteration */
            barrier_polling(kthread_id);

            common_paras.local_delay_cost = *(input_args->delay_cost);
            /* place_delay_value = delay_cost / num_connections; */
            common_paras.local_place_delay_value =
                common_paras.local_delay_cost / common_paras.local_num_conns;

            /*parallel timing update. Only run timing update once per
             * 'timing_update_threadhold' iterations */
            if (freq >= timing_update_threshold) {
                /* It first calculate all tnodes arr_time and req_time parallel */
                perform_timing_analyze_parallel(kthread_id,
                                                &(common_paras.local_net_delay),
                                                input_args,
                                                &(common_paras.local_max_delay));
                /* then calculate all timing_edge' slack value. */
                compute_net_slack_full(kthread_id,
                                       &(common_paras.local_net_slack),
                                       &(common_paras.local_timing_cost),
                                       &(common_paras.local_delay_cost),
                                       common_paras.local_crit_exponent,
                                       common_paras.local_max_delay,
                                       input_args);

                /*reset frequency counter*/
                freq = 0;
            } else {
                /*only let one thread do reset update global variable */
                if (pthread_mutex_trylock(&global_data_access.mutex) != 0) {
                    barrier_polling(kthread_id);
                } else {
                    *(input_args->timing_cost) = 0.0;
                    *(input_args->delay_cost) = 0.0;
                    barrier_polling(kthread_id);
                    /* unlock mutex */
                    pthread_mutex_unlock(&global_data_access.mutex);
                }

                /* no criticality update, but still need to recalculate timing */
                calculate_timing_without_update_crit(kthread_id,
                                                     &(common_paras.local_timing_cost),
                                                     &(common_paras.local_delay_cost),
                                                     input_args);
                ++freq;
            }

            barrier_polling(kthread_id);

            iter_data_update_from_global_to_local_grid(iter,
                                                       input_args,
                                                       &common_paras,
                                                       &swap_data);
            /* clear local counters. But why? */
            if (iter == 0) {
                common_paras.local_success_sum = 0;
                move_counter = 0;
                common_paras.local_av_cost = 0.0;
                common_paras.local_av_bb_cost = 0.0;
                common_paras.local_av_timing_cost = 0.0;
                common_paras.local_av_delay_cost = 0.0;
                common_paras.local_sum_of_squares = 0.0;
            }

            barrier_polling(kthread_id);
            /* Iterate through each sub-regions of the grid. FIXME, each region
             * had 2x2 sub-regions. */
            int row, col;
            for (row = 0; row < 2; ++row) {
                for (col = 0; col < 2; ++col) {
                    try_place_a_subregion(kthread_id,
                                          row, col,
                                          prob,
                                          &move_counter,
                                          &common_paras,
                                          &swap_data);
                }  /* end of for(col = 0; col < 2; ++col) */
            }  /* end of for(row = 0; row < 2; ++row) */

            /*prepare for bb_box calculation*/
            if (pthread_mutex_trylock(&global_data_access.mutex) != 0) {
                barrier_polling(kthread_id);
            } else {
                *(input_args->bb_cost) = 0.;
                barrier_polling(kthread_id);
                pthread_mutex_unlock(&global_data_access.mutex);
            }

            /*bb cost update
             *parallel calculation followed by serial addition in order to avoid
             *floating point roundoff error */
            double wirelength_cost =
                compute_bb_cost_parallel(start_finish_nets[kthread_id].start_sinks,
                                         start_finish_nets[kthread_id].finish_sinks);
            common_paras.local_bb_cost = wirelength_cost;
            partial_bb_results[kthread_id] = wirelength_cost;

            barrier_polling(kthread_id);
            if (kthread_id == 0) {
                wirelength_cost = 0.0;
                int update_count = -1;
                for (update_count = 0; update_count < NUM_OF_THREADS; ++update_count) {
                    wirelength_cost += partial_bb_results[update_count];
                }

                *(input_args->bb_cost) = wirelength_cost;
            }
        } /* end of for(iter = 0; iter < inner_iter_num; ++iter) */

        /*synchronize result*/
        pthread_mutex_lock(&global_data_access.mutex);
        *(input_args->success_sum ) += common_paras.local_success_sum;
        *(input_args->move_lim) += move_counter;
        *(input_args->av_cost) += common_paras.local_av_cost;
        *(input_args->av_bb_cost) += common_paras.local_av_bb_cost;
        *(input_args->av_timing_cost) += common_paras.local_av_timing_cost;
        *(input_args->av_delay_cost) += common_paras.local_av_delay_cost;
        *(input_args->sum_of_squares) += common_paras.local_sum_of_squares;
        pthread_mutex_unlock(&global_data_access.mutex);

        barrier_polling(kthread_id);
        /*data gathering and printing*/
        update_and_print_common_paras(kthread_id,
                                      &move_counter,
                                      &inner_iter_num,
                                      input_args,
                                      &common_paras);

        if (common_paras.local_temper != 0.0) {
            barrier_polling(common_paras.local_thread_id);
        }
    }  /* end of while (t != 0.0) */

    /* free resources */
    int inet = -1;
    const place_algorithm_t kplace_algorithm =
        common_paras.local_placer_opts.place_algorithm;
    if (kplace_algorithm == NET_TIMING_DRIVEN_PLACE ||
            kplace_algorithm == PATH_TIMING_DRIVEN_PLACE) {
        for (inet = 0; inet < num_nets; ++inet) {
            /*add one to the address since it is indexed from 1 not 0 */
            (swap_data.m_local_temp_point_to_point_delay_cost[inet])++;
            free(swap_data.m_local_temp_point_to_point_delay_cost[inet]);

            swap_data.m_local_temp_point_to_point_timing_cost[inet]++;
            free(swap_data.m_local_temp_point_to_point_timing_cost[inet]);
        }

        free(swap_data.m_local_temp_point_to_point_delay_cost);
        swap_data.m_local_temp_point_to_point_delay_cost = NULL;

        free(swap_data.m_local_temp_point_to_point_timing_cost);
        swap_data.m_local_temp_point_to_point_timing_cost = NULL;
    }

    free(swap_data.m_local_block);
    swap_data.m_local_block = NULL;
    free(swap_data.m_local_temp_net_cost);
    swap_data.m_local_temp_net_cost = NULL;
    free(swap_data.m_local_net_cost);
    swap_data.m_local_net_cost = NULL;

    free(swap_data.m_local_bb_coord);
    swap_data.m_local_bb_coord = NULL;
    free(swap_data.m_local_bb_edge);
    swap_data.m_local_bb_edge = NULL;

    free(swap_data.m_nets_to_update);
    swap_data.m_nets_to_update = NULL;

    free(swap_data.m_net_block_moved);
    swap_data.m_net_block_moved = NULL;

    free(swap_data.m_bb_coord_new);
    swap_data.m_bb_coord_new = NULL;

    free(swap_data.m_bb_edge_new);
    swap_data.m_bb_edge_new = NULL;
} /* end of void* try_place_parallel(void* args) */

static void  try_place_a_subregion(const int  kthread_id,
                                   const int  krow,
                                   const int  kcol,
                                   const int  prob,
                                   int*     move_counter,
                                   thread_local_common_paras_t*  common_paras_ptr,
                                   thread_local_data_for_swap_t* swap_data_ptr)
{
    barrier_polling(kthread_id);

    /* update locate Sub-Region from global data, and first
     * update horizontal data, then update vertical data. */
    const int* kregion_x_boundary = common_paras_ptr->local_region_x_boundary;
    const int* kregion_y_boundary = common_paras_ptr->local_region_y_boundary;
    grid_tile_t**  local_grid = swap_data_ptr->m_local_grid;
    local_block_t* local_block = swap_data_ptr->m_local_block;

    update_local_data_from_global(kregion_x_boundary,
                                  kregion_y_boundary,
                                  local_grid,
                                  local_block,
                                  krow,
                                  kcol);

    /*sequentially consider each block within the sub-region*/
    const int hori_start_bound = kregion_x_boundary[krow];
    const int hori_end_bound  =  kregion_x_boundary[krow + 1];
    const int vert_start_bound = kregion_y_boundary[kcol];
    const int vert_end_bound  =  kregion_y_boundary[kcol + 1];
    const boolean kfixed_pins = common_paras_ptr->local_fixed_pins;
    const double kt = common_paras_ptr->local_temper;
    int x_from = 0;
    int y_from = 0;
    int z_from = 0;
    for (x_from = hori_start_bound; x_from < hori_end_bound; ++x_from) {
        for (y_from = vert_start_bound; y_from < vert_end_bound; ++y_from) {
            if ((local_grid[x_from][y_from].type == EMPTY_TYPE)
                  || (kfixed_pins && local_grid[x_from][y_from].type == IO_TYPE)) {
                continue;
            }
            const int kcapacity = local_grid[x_from][y_from].type->capacity;
            for (z_from = 0; z_from < kcapacity; ++z_from) {
                //do not consider empty locations, ie - four corners of the grid or fixed pins
                if (local_grid[x_from][y_from].blocks[z_from] == EMPTY) {
                    continue;
                }

                /* PROB_SKIPPED */
                if (my_irand_parallel(100, kthread_id) >= prob) {
                    ++(*move_counter);
                    int return_val = try_swap_parallel(kt, kthread_id,
                                                       x_from, y_from, z_from,
                                                       krow, kcol,
                                                       common_paras_ptr,
                                                       swap_data_ptr);
                    if (return_val == 1) {
                        ++(common_paras_ptr->local_success_sum);
                        const double ktotal_cost = common_paras_ptr->local_total_cost;
                        common_paras_ptr->local_av_cost += ktotal_cost;
                        common_paras_ptr->local_av_bb_cost += common_paras_ptr->local_bb_cost;
                        common_paras_ptr->local_av_timing_cost += common_paras_ptr->local_timing_cost;
                        common_paras_ptr->local_av_delay_cost += common_paras_ptr->local_delay_cost;
                        common_paras_ptr->local_sum_of_squares +=
                            ktotal_cost * ktotal_cost;
                    }
                } /* end of if(my_irand_parallel(100, kthread_id) >= prob) */
            } /* end of for(z_from = 0; z_from < local_grid[x_from][y_from].type->capacity; ++z_from) */
        } /* end of for(y_from = vert_start_bound; y_from < vert_end_bound; ++y_from) */
    } /* end of for(x_from = hori_start_bound; x_from < hori_end_bound; ++x_from) */

    /*update global data with local changes*/
    update_from_local_to_global(local_block,
                                local_grid,
                                max(0, kregion_x_boundary[krow] - 2),
                                min(num_grid_columns + 2,
                                    kregion_x_boundary[krow + 1] + 2),
                                max(0, kregion_y_boundary[kcol] - 2),
                                min(num_grid_rows + 2,
                                    kregion_y_boundary[kcol + 1] + 2));
} /* end of static void try_place_a_subregion() */

/* FIXME, Important funtion! It initial all imporant parameters that used for *
 * parallel placement.                                                        */
static void initial_common_paras(pthread_data_t*  input_args,
                                 int*  max_pins_per_fb,
                                 thread_local_common_paras_t*  common_paras_ptr)
{
    /*thread-dependent data*/
    common_paras_ptr->local_thread_id = input_args->thread_id;
    common_paras_ptr->local_y_start = input_args->y_start;
    common_paras_ptr->local_y_end = input_args->y_end;
    common_paras_ptr->local_x_start = input_args->x_start;
    common_paras_ptr->local_x_end = input_args->x_end;

    /*placer-depdent data*/
    common_paras_ptr->local_fixed_pins = input_args->fixed_pins;
    common_paras_ptr->local_temper = *(input_args->t);
    common_paras_ptr->local_placer_opts = input_args->placer_opts;

    /* square based partitioning boundary */
    const int ky_start = common_paras_ptr->local_y_start;
    const int ky_end = common_paras_ptr->local_y_end;
    const int kx_start = common_paras_ptr->local_x_start;
    const int kx_end = common_paras_ptr->local_x_end;
    common_paras_ptr->local_region_y_boundary[0] = ky_start;
    common_paras_ptr->local_region_y_boundary[1] =
        ky_start + ((int)(ky_end - ky_start) / 2);
    common_paras_ptr->local_region_y_boundary[2] = ky_end;

    common_paras_ptr->local_region_x_boundary[0] = kx_start;
    common_paras_ptr->local_region_x_boundary[1] =
        kx_start + ((int)(kx_end - kx_start) / 2);
    common_paras_ptr->local_region_x_boundary[2] = kx_end;

    /* other initializations */
    common_paras_ptr->local_first_rlim = (double)max(num_grid_columns,
                                                     num_grid_rows);
    common_paras_ptr->local_range_limit = common_paras_ptr->local_first_rlim;
    common_paras_ptr->local_final_rlim = 1;
    common_paras_ptr->local_inverse_delta_rlim =
        1 / (common_paras_ptr->local_first_rlim - common_paras_ptr->local_final_rlim);

    common_paras_ptr->local_place_delay_value = *(input_args->place_delay_value);
    common_paras_ptr->local_max_delay = *(input_args->max_delay);
    common_paras_ptr->local_num_conns = *(input_args->num_connections);
    common_paras_ptr->local_delay_cost = *(input_args->delay_cost);
    common_paras_ptr->local_net_slack = input_args->net_slack;
    common_paras_ptr->local_net_delay = input_args->net_delay;

    /* max_pins_per_fb initialization*/
    int i = -1;
    for (i = 0; i < num_types; ++i) {
        *max_pins_per_fb = max(*max_pins_per_fb,
                               type_descriptors[i].num_pins);
    }

    /* dynamic workload distribution for timing update initialization*/
    const int knets_assign_to_thread = ceil((double)num_nets / NUM_OF_THREADS);
    const int kthread_id = common_paras_ptr->local_thread_id;
    start_finish_nets[kthread_id].start_edge = kthread_id * knets_assign_to_thread;
    start_finish_nets[kthread_id].finish_edge =
        min(num_nets, (kthread_id + 1) * knets_assign_to_thread);

    start_finish_nets[kthread_id].start_sinks =
        start_finish_nets[kthread_id].start_edge;
    start_finish_nets[kthread_id].finish_sinks =
        start_finish_nets[kthread_id].finish_edge;

    start_finish_nets[kthread_id].edge_partition_size =
        (start_finish_nets[kthread_id].finish_edge
             - start_finish_nets[kthread_id].start_edge);
    start_finish_nets[kthread_id].sink_partition_size =
        (start_finish_nets[kthread_id].finish_sinks
             - start_finish_nets[kthread_id].start_sinks);

    start_finish_nets[kthread_id].counter_sink = 0;
    start_finish_nets[kthread_id].counter_edge = 0;
} /* end of void initial_common_paras() */

static void alloc_memory_for_swap_data(const int max_pins_per_fb,
                                       thread_local_common_paras_t*  common_paras_ptr,
                                       thread_local_data_for_swap_t* swap_data_ptr)
{
    /* local_grid[col][row] */
    swap_data_ptr->m_local_grid =
            (grid_tile_t**)alloc_matrix(0, (num_grid_columns + 1),
                                        0, (num_grid_rows + 1),
                                        sizeof(grid_tile_t));

    swap_data_ptr->m_local_block =
            (local_block_t*)malloc(num_blocks * sizeof(local_block_t));

    /* Why did this array allocate 2 * max_pins_per_fb? */
    swap_data_ptr->m_bb_coord_new =
            (bbox_t*)my_malloc(2 * max_pins_per_fb * sizeof(bbox_t));
    swap_data_ptr->m_bb_edge_new =
            (bbox_t*)my_malloc(2 * max_pins_per_fb * sizeof(bbox_t));

    swap_data_ptr->m_nets_to_update =
            (int*)my_malloc(2 * max_pins_per_fb * sizeof(int));
    swap_data_ptr->m_net_block_moved =
            (int*)my_malloc(2 * max_pins_per_fb * sizeof(int));

    swap_data_ptr->m_local_bb_coord = (bbox_t*)my_malloc(num_nets * sizeof(bbox_t));
    swap_data_ptr->m_local_bb_edge = (bbox_t*)my_malloc(num_nets * sizeof(bbox_t));

    swap_data_ptr->m_local_temp_net_cost = (double*)malloc(num_nets * sizeof(double));
    swap_data_ptr->m_local_net_cost = (double*)malloc(num_nets * sizeof(double));
    /* point_to_point_delay_cost & point_to_point_timing_cost are not updated.
     * Hence, no local copies of these are needed, and global data is read
     * during placement. Global data is updated once per iter */
    const place_algorithm_t kplace_algorithm =
        common_paras_ptr->local_placer_opts.place_algorithm;
    if (kplace_algorithm == NET_TIMING_DRIVEN_PLACE
            || kplace_algorithm == PATH_TIMING_DRIVEN_PLACE) {
        swap_data_ptr->m_local_temp_point_to_point_delay_cost =
            (double**)my_malloc(num_nets * sizeof(double*));
        swap_data_ptr->m_local_temp_point_to_point_timing_cost =
            (double**)my_malloc(num_nets * sizeof(double*));

        int inet = -1;
        for (inet = 0; inet < num_nets; ++inet) {
            (swap_data_ptr->m_local_temp_point_to_point_delay_cost)[inet] =
                    (double*)my_malloc(net[inet].num_sinks * sizeof(double));
            --(swap_data_ptr->m_local_temp_point_to_point_delay_cost)[inet];

            (swap_data_ptr->m_local_temp_point_to_point_timing_cost)[inet] =
                (double*)my_malloc(net[inet].num_sinks * sizeof(double));
            --(swap_data_ptr->m_local_temp_point_to_point_timing_cost)[inet];
        }
    }
} /* end of static void alloc_memory_for_swap_data() */


/*  Parallel Placement functions Startings....     */
/*partition nets evenly based on number of edges each net is connected to(that is sub-connections). */
static void balance_two_consecutive_threads_edge(const int kthread_id)
{
    ++(start_finish_nets[kthread_id].counter_edge);
    unsigned long work_in_this_region = start_finish_nets[kthread_id].edges_in_this_partition;
    unsigned long work_in_next_region = start_finish_nets[kthread_id + 1].edges_in_this_partition;

    /*if next partition has more work (edges)
     *adjust the boundary according to % of difference */
    int net_shift = 0;
    double exceed_ratio = 0.0;
    int edge_partition_size = 0;
    if (work_in_this_region < work_in_next_region) {
        exceed_ratio = (work_in_next_region - work_in_this_region) / work_in_this_region;
        edge_partition_size = start_finish_nets[kthread_id + 1].edge_partition_size;
        net_shift = (int)(min(1, exceed_ratio) * edge_partition_size * 0.25);

        if (net_shift == 0 && (start_finish_nets[kthread_id + 1].edge_partition_size >= 2)) {
            net_shift = 1;
        }

        start_finish_nets[kthread_id].finish_edge += net_shift;
        start_finish_nets[kthread_id + 1].start_edge += net_shift;
    } else if (work_in_this_region > work_in_next_region) {
        /* if this partition has more edges
         * adjust the boundary according to % of difference */
        exceed_ratio = (work_in_this_region - work_in_next_region) / work_in_next_region;
        edge_partition_size = start_finish_nets[kthread_id].edge_partition_size;
        net_shift = (int)(min(1, exceed_ratio) * edge_partition_size * 0.25);

        if (net_shift == 0 && start_finish_nets[kthread_id].edge_partition_size >= 2) {
            net_shift = 1;
        }

        start_finish_nets[kthread_id].finish_edge -= net_shift;
        start_finish_nets[kthread_id + 1].start_edge -= net_shift;
    } else {
        /* No operations */
    }

    assert(start_finish_nets[kthread_id].finish_edge
             > start_finish_nets[kthread_id].start_edge);
} /* end of void balance_two_consecutive_threads_edge(int kthread_id)  */

/* Copy memory from bb_num_on_edges(or bb_coords) to *
 * local_bb_edge(local_bb_coord) */
static void initial_swap_data(thread_local_common_paras_t*  common_paras_ptr,
                              thread_local_data_for_swap_t* swap_data_ptr)
{
    memcpy(swap_data_ptr->m_local_bb_edge,
           bb_num_on_edges,
           num_nets * sizeof(bbox_t));
    memcpy(swap_data_ptr->m_local_bb_coord,
           bb_coords,
           num_nets * sizeof(bbox_t));

    int x, y, z;
    for (x = 0; x < num_nets; ++x) {
        swap_data_ptr->m_local_temp_net_cost[x] = temp_net_cost[x];
        swap_data_ptr->m_local_net_cost[x] = net_cost[x];
    }

    /* extend sub-region[start_boundary-2..end_boundary_x+2] */
    grid_tile_t** local_grid = swap_data_ptr->m_local_grid;
    const int* kregion_x_boundary = common_paras_ptr->local_region_x_boundary;
    for (x = kregion_x_boundary[0] - 2;
            x < kregion_x_boundary[2] + 2 && x < (num_grid_columns + 2); ++x) {
        /*  takes care of proc #1, where start_end_boundary = 0 */
        if (x < 0) {
            x = 0;
        }
        /* Why not set y as Extend-SubRegion[region_y_boundary-2..region_y_boundary+2] */
        for (y = 0; y <= (num_grid_rows + 1); ++y) {
            local_grid[x][y].type = grid[x][y].type;
            local_grid[x][y].usage = grid[x][y].usage;
            local_grid[x][y].offset = grid[x][y].offset;

            local_grid[x][y].blocks =
                (int*)my_malloc(sizeof(int) * grid[x][y].type->capacity);

            for (z = 0; z < grid[x][y].type->capacity; ++z) {
                local_grid[x][y].blocks[z] = grid[x][y].blocks[z];
            }
        }
    }

    local_block_t* local_block = swap_data_ptr->m_local_block;
    for (x = 0; x < num_blocks; ++x) {
        local_block[x].x = block[x].x;
        local_block[x].y = block[x].y;
        local_block[x].z = block[x].z;
    }
}  /* end of void initial_swap_data(bbox_t* local_bb_edge,...) */


/*partition nets evenly based on number of sinks each net is connected to*/
static void balance_two_consecutive_threads_sinks(const int kthread_id)
{
    ++(start_finish_nets[kthread_id].counter_sink);
    unsigned long work_in_this_region = start_finish_nets[kthread_id].sinks_in_this_partition;
    unsigned long work_in_next_region = start_finish_nets[kthread_id + 1].sinks_in_this_partition;

    /*if next partition has more edges
     *adjust the boundary according to % of difference */
    int net_shift = 0;
    double exceed_ratio = 0.0;
    int sink_partition_size = 0;
    if (work_in_this_region < work_in_next_region) {
        exceed_ratio = (work_in_next_region - work_in_this_region) / work_in_this_region;
        sink_partition_size = start_finish_nets[kthread_id + 1].sink_partition_size;
        net_shift = (int)(min(1, exceed_ratio) * sink_partition_size * 0.25);

        start_finish_nets[kthread_id].finish_sinks += net_shift;
        start_finish_nets[kthread_id + 1].start_sinks += net_shift;
    } else if (work_in_this_region > work_in_next_region) {
        /*if this partition has more edges
         *adjust the boundary according to % of difference */
        exceed_ratio = (work_in_this_region - work_in_next_region) / work_in_next_region;
        sink_partition_size = start_finish_nets[kthread_id].sink_partition_size;
        net_shift = (int)(min(1, exceed_ratio) * sink_partition_size * 0.25);

        start_finish_nets[kthread_id].finish_sinks -= net_shift;
        start_finish_nets[kthread_id + 1].start_sinks -= net_shift;
    } else {
        /* No operations */
    }

    assert (start_finish_nets[kthread_id].finish_sinks
              > start_finish_nets[kthread_id].start_sinks);
}  /* end of void balance_two_consecutive_threads_sinks(int kthread_id) */


/*finds the fanin to a particular node*/
static void find_fanin_parallel(const int kthread_id)
{
    int tnodes_assign_to_thread = ceil((double)num_tnodes / NUM_OF_THREADS);
    int start_tnode = kthread_id * tnodes_assign_to_thread;
    int finish_tnode = min((kthread_id + 1) * tnodes_assign_to_thread,
                           num_tnodes);
    int i = 0;
    for (i = start_tnode; i < finish_tnode; ++i) {
        int num_edges = tnode[i].num_edges;
        t_tedge* tedge = tnode[i].out_edges;

        int iedge = 0;
        for (iedge = 0; iedge < num_edges; ++iedge) {
            int to_node = tedge[iedge].to_node;
            /* FIXME, don't use ++tnode[to_node].num_parents */
            int counter = tnode[to_node].num_parents++;
            tnode[to_node].in_edges[counter].to_node = i;
        }
    }
} /* end of void find_fanin_parallel(int kthread_id) */


static void initial_localvert_grid()
{
    /* grid[col][row] was column-based, but localvert_grid[col][row] was
     * row-based, it vertical to grid. */
    localvert_grid = (grid_tile_t**)alloc_matrix(0, (num_grid_columns + 1),
                                                 0, (num_grid_rows + 1),
                                                 sizeof(grid_tile_t));
    int col, row, cap;
    for (col = 0; col < (num_grid_columns + 2); ++col) {
        for (row = 0; row < (num_grid_rows + 2); ++row) {
            localvert_grid[row][col].type = grid[col][row].type;
            localvert_grid[row][col].usage = grid[col][row].usage;
            localvert_grid[row][col].offset = grid[col][row].offset;

            localvert_grid[row][col].blocks =
                (int*)my_malloc(sizeof(int) * grid[col][row].type->capacity);

            for (cap = 0; cap < grid[col][row].type->capacity; ++cap) {
                localvert_grid[row][col].blocks[cap] = grid[col][row].blocks[cap];
            }
        }
    }
} /* end of void initial_localvert_grid() */


/* FIXME, important for update timing parallel */
static void perform_timing_analyze_parallel(const int  kthread_id,
                                            double*** net_delay,
                                            pthread_data_t* input_args,
                                            double*   max_delay)
{
    /*dynamic workload setup*/
    const int counter_sink = start_finish_nets[kthread_id].counter_sink;
    if (kthread_id != NUM_OF_THREADS - 1
          && counter_sink < (num_nets / PARITION_UPDATE)) {
        const int partition_size = (start_finish_nets[kthread_id].finish_sinks
                                       - start_finish_nets[kthread_id].start_sinks);
            start_finish_nets[kthread_id].sink_partition_size = partition_size;
        }

    /* loads the net_delay, and return the total number of sinks visited for dynamic workload */
    const int cur_thread_start_sinks = start_finish_nets[kthread_id].start_sinks;
    const int cur_thread_finish_sinks = start_finish_nets[kthread_id].finish_sinks;
    start_finish_nets[kthread_id].sinks_in_this_partition =
                   load_timing_graph_net_delays_parallel(*net_delay,
                                                         cur_thread_start_sinks,
                                                         cur_thread_finish_sinks);

    barrier_polling(kthread_id);
    /* update dynamic workload distribution*/
    if (kthread_id != NUM_OF_THREADS - 1 && counter_sink < num_nets / PARITION_UPDATE) {
        balance_two_consecutive_threads_sinks(kthread_id);
    }
    /*  reset global variables  */
    if (kthread_id == 0) {
        *(input_args->timing_cost) = 0;
        *(input_args->delay_cost) = 0;
        *(input_args->max_delay) = 0.0;
    }

    /*  Load-net-slack starting....   */
    /* part 1 *
     * Reset all arrival times to -ve infinity. Can't just set to zero or the *
     * constant propagation(constant generators work at -ve infinity) won't  *
     * work.                                                                 */
    int tnodes_assign_to_thread = ceil((double)num_tnodes / NUM_OF_THREADS);
    int start_node = kthread_id * tnodes_assign_to_thread;
    int finish_node = min((kthread_id + 1) * tnodes_assign_to_thread, num_tnodes);
    int i = 0;
    for (i = start_node; i < finish_node; ++i) {
        tnode[i].arr_time = T_CONSTANT_GENERATOR; /* -1000 */
    }

    barrier_polling(kthread_id);
    /* Part 2 *
     * reset arrivial time for all nodes at level 0. */
    int tnodes_at_level0 = tnodes_at_level[0].nelem;
    tnodes_assign_to_thread = ceil((double)tnodes_at_level0 / NUM_OF_THREADS);
    start_node = kthread_id * tnodes_assign_to_thread;
    finish_node = min((kthread_id + 1) * tnodes_assign_to_thread,
                       tnodes_at_level0);
    for (i = start_node; i < finish_node; ++i) {
        int node_index = tnodes_at_level[0].list[i];
        tnode[node_index].arr_time = 0.0;
    }
    barrier_polling(kthread_id);

    /* Part 3, compute all tnodes arrival_time parallely! And find out the *
     * Critical_delay. The functions are parallelized on a per-level basis *
     * each processor will receive a min. of 100 nodes to work with.       *
     * If there are not enough nodes to distrubute to all processors,
     * processor with larger kthread_id will remain idle.       */
    *max_delay = 0.0;
    int ilevel = 0;
    for (ilevel = 1; ilevel < num_tnode_levels; ++ilevel) {
        barrier_polling(kthread_id);
        int num_at_level = tnodes_at_level[ilevel].nelem;
        int num_of_thread_used = ceil((double)num_at_level / 100);
        tnodes_assign_to_thread = ceil((double)num_at_level /
                                       min(NUM_OF_THREADS, num_of_thread_used));

        /* for a big enough partition */
        if (kthread_id < num_of_thread_used) {
            start_node = kthread_id * tnodes_assign_to_thread;
            finish_node = min((kthread_id + 1) * tnodes_assign_to_thread,
                               num_at_level);
            /* the following line does the actual work.
               rest of the stuff above is for the workload distrubtion */
            *max_delay = max(*max_delay,
                             calc_tnodes_arr_time_parallel(start_node,
                                                           finish_node,
                                                           ilevel));
        }
    }  /* end of for(ilevel = 1; ilevel < num_tnode_levels; ++ilevel) */

    /* the MAX_DELAY value is written back */
    pthread_mutex_lock(&global_data_access.mutex);
    *(input_args->max_delay) = max(*(input_args->max_delay), *max_delay);
    pthread_mutex_unlock(&global_data_access.mutex);

    /* Part 4, compute all tnodes required_time parallely!  *
     * same concept as part 3, but for a different function */
    barrier_polling(kthread_id);
    *max_delay = *(input_args->max_delay);
    for (ilevel = num_tnode_levels - 1; ilevel >= 0; --ilevel) {
        int num_at_level = tnodes_at_level[ilevel].nelem;
        int num_of_thread_used = ceil((double)num_at_level / 100);
        tnodes_assign_to_thread = ceil((double)num_at_level /
                                       min(NUM_OF_THREADS, num_of_thread_used));

        /* for a big enough partition */
        if (kthread_id < num_of_thread_used) {
            start_node = kthread_id * tnodes_assign_to_thread;
            finish_node = min((kthread_id + 1) * tnodes_assign_to_thread,
                               num_at_level);
            calc_tnodes_req_time_parallel(*max_delay,
                                          start_node,
                                          finish_node,
                                          ilevel);
        }
        barrier_polling(kthread_id);
    }  /* end of for(ilevel = num_tnode_levels - 1; ilevel >= 0; --ilevel)*/
}  /* end of void perform_timing_analyze_parallel(int kthread_id,...)  */

/* FIXME, If I want to compute net_slack, first I must calcuatel all edge's delay value,
 * then calcuate all vertexes' arr_time and req_time. Last compute slack       */
static void compute_net_slack_full(const int kthread_id,
                                   double***  net_slack,
                                   double*  timing_cost,
                                   double*  delay_cost,
                                   double  crit_exponent,
                                   double  max_delay,
                                   pthread_data_t* input_args)
{
    /* part 5
     * compute net slacks */
    const int thread_start_edge = start_finish_nets[kthread_id].start_edge;
    const int thread_finish_edge = start_finish_nets[kthread_id].finish_edge;
    const int partition_size = thread_finish_edge - thread_start_edge;

    const int thread_counter_edge = start_finish_nets[kthread_id].counter_edge;
    if (thread_counter_edge < (num_nets / PARITION_UPDATE)) {
        start_finish_nets[kthread_id].edge_partition_size = partition_size;
    }

    /* Compute [thread_start_edge, thread_finish_edge] nets' slack OK! */
    start_finish_nets[kthread_id].edges_in_this_partition =
                                 compute_net_slacks_parallel(*net_slack,
                                                             thread_start_edge,
                                                             thread_finish_edge);
    barrier_polling(kthread_id);
    /* dynamic workload distribution *
     * equalize partition */
    if (kthread_id != NUM_OF_THREADS - 1
          && thread_counter_edge < (num_nets / PARITION_UPDATE)) {
        /* update and balance the thread and (thread + 1)'s start_edge and
         * finish_edge */
        balance_two_consecutive_threads_edge(kthread_id);
    }

    const int thread_start_sinks = start_finish_nets[kthread_id].start_sinks;
    const int thread_finish_sinks = start_finish_nets[kthread_id].finish_sinks;
    const int thread_counter_sink = start_finish_nets[kthread_id].counter_sink;
    if (kthread_id != NUM_OF_THREADS - 1
          && thread_counter_sink < (num_nets / PARITION_UPDATE)) {
        start_finish_nets[kthread_id].sink_partition_size =
                              thread_finish_sinks - thread_start_sinks;
    }

    /* Part 6, compute timing_driven cost Parallel */
    start_finish_nets[kthread_id].sinks_in_this_partition =
                            compute_td_costs_parallel_with_update_crit(timing_cost,
                                                                       delay_cost,
                                                                       *net_slack,
                                                                       max_delay,
                                                                       crit_exponent,
                                                                       thread_start_sinks,
                                                                       thread_finish_sinks);

    /* write back the partial timing and Tdel cost to the global variable */
    /* Why did author using these 2 following variables? */
    partial_timing_results[kthread_id] = *timing_cost;
    partial_delay_results[kthread_id] = *delay_cost;

    barrier_polling(kthread_id);
    /* master thread sums up the partial values */
    if (kthread_id == 0) {
        *timing_cost = 0.0;
        *delay_cost = 0.0;
        int thread_idx = 0;
        for (thread_idx = 0; thread_idx < NUM_OF_THREADS; ++thread_idx) {
            *timing_cost += partial_timing_results[thread_idx];
            *delay_cost += partial_delay_results[thread_idx];
        }

        *(input_args->timing_cost) = *timing_cost;
        *(input_args->delay_cost) = *delay_cost;
    }  /* end of if(kthread_id == 0) */

    barrier_polling(kthread_id);
    /* Update partition size  */
    if (kthread_id != NUM_OF_THREADS - 1
          && thread_counter_sink < num_nets / PARITION_UPDATE) {
        balance_two_consecutive_threads_sinks(kthread_id);
    }
}  /* end of void compute_net_slack_full(int kthread_id,..) */

static void calculate_timing_without_update_crit(const int kthread_id,
                                                 double* timing_cost,
                                                 double* delay_cost,
                                                 pthread_data_t* input_args)
{
    const int thread_counter_sink = start_finish_nets[kthread_id].counter_sink;
    const int thread_finish_sinks = start_finish_nets[kthread_id].finish_sinks;
    const int thread_start_sinks = start_finish_nets[kthread_id].start_sinks;
    if (kthread_id != NUM_OF_THREADS - 1 && thread_counter_sink < num_nets / PARITION_UPDATE) {
        const int partition_size = thread_finish_sinks - thread_start_sinks;
        start_finish_nets[kthread_id].sink_partition_size = partition_size;
    }

    start_finish_nets[kthread_id].sinks_in_this_partition =
                    compute_td_costs_parallel_without_update_crit(timing_cost,
                                                                  delay_cost,
                                                                  thread_start_sinks,
                                                                  thread_finish_sinks);
    pthread_mutex_lock(&global_data_access.mutex);
    *(input_args->timing_cost) += *timing_cost;
    *(input_args->delay_cost) += *delay_cost;
    pthread_mutex_unlock(&global_data_access.mutex);
    barrier_polling(kthread_id);

    if (kthread_id != NUM_OF_THREADS - 1 && thread_counter_sink < num_nets / PARITION_UPDATE) {
        balance_two_consecutive_threads_sinks(kthread_id);
    }
}  /* end of static void calculate_timing_without_update_crit(int kthread_id,..) */


static void iter_data_update_from_global_to_local_grid(const int kiter,
                                                       pthread_data_t* input_args,
                                                       thread_local_common_paras_t*  common_paras_ptr,
                                                       thread_local_data_for_swap_t* swap_data_ptr)
{
    /* Regions in first row of the region grid */
    const int  kthread_id = common_paras_ptr->local_thread_id;
    const int* kregion_x_boundary = common_paras_ptr->local_region_x_boundary;
    const int* kregion_y_boundary = common_paras_ptr->local_region_y_boundary;
    const place_algorithm_t kplace_algorithm =
        common_paras_ptr->local_placer_opts.place_algorithm;
    if (kthread_id / sqrt(NUM_OF_THREADS) == 0) {
    /* Attention, current NUM_OF_THREADS == 4, so when kthread_id = 0 or 1, *
     * the Regions was in first row. Later, I will use Intel Xeon 4-core   *
     * 8-threads CPU, I'd like to use 8 threads parallel. When I set 4x2,  *
     * that is 4 cols in each row, and totally 2 rows. You can see x_partion*
     * and y_partition in place.c. Sqrt(8) = 3.464, when kthread_id = 0,1,2,3,
     * the regions was in first row. But when I set 2x4 for 8 threads(4 rows
     * and 2 cols), this may be error! */
        if (kplace_algorithm == NET_TIMING_DRIVEN_PLACE ||
                kplace_algorithm == PATH_TIMING_DRIVEN_PLACE) {
            *(input_args->cost) = common_paras_ptr->local_total_cost = 1.0;
        }
        /* all average cost initial as 0.0 */
        if (kiter == 0 && kthread_id == 0) {
            *(input_args->av_cost) = 0.0;
            *(input_args->av_bb_cost) = 0.0;
            *(input_args->av_timing_cost) = 0.0;
            *(input_args->av_delay_cost) = 0.0;
            *(input_args->sum_of_squares) = 0.0;
            *(input_args->success_sum) = 0.0;
            *(input_args->move_lim) = 0;
        }
    } else {
    /* update local data from global for top rows of each private region */
        update_from_global_to_local_grid_only(swap_data_ptr->m_local_grid,
                                              max(0,  kregion_x_boundary[0] - 2),
                                              kregion_x_boundary[0] + 2,
                                              max(kregion_y_boundary[0] - 2 , 0),
                                              min(kregion_y_boundary[2] + 2,
                                                  num_grid_rows + 2));
    }

    /* global grid to local block grid update */
    int x = -1;
    local_block_t* local_block = swap_data_ptr->m_local_block;
    for (x = 0; x < num_blocks; ++x) {
        local_block[x].x = block[x].x;
        local_block[x].y = block[x].y;
        local_block[x].z = block[x].z;
    }

    /* timing update. */
    if (kplace_algorithm == NET_TIMING_DRIVEN_PLACE ||
            kplace_algorithm == PATH_TIMING_DRIVEN_PLACE) {
        common_paras_ptr->local_total_cost = 1.0;

        common_paras_ptr->local_timing_cost = *(input_args->timing_cost);
        common_paras_ptr->local_inverse_prev_timing_cost =
            1 / common_paras_ptr->local_timing_cost;
        common_paras_ptr->local_delay_cost = *(input_args->delay_cost);
    }

    common_paras_ptr->local_bb_cost = *(input_args->bb_cost);
    common_paras_ptr->local_inverse_prev_bb_cost =
        1 / common_paras_ptr->local_bb_cost;

    memcpy(swap_data_ptr->m_local_bb_edge,
           bb_num_on_edges,
           num_nets * sizeof(bbox_t));
    memcpy(swap_data_ptr->m_local_bb_coord,
           bb_coords,
           num_nets * sizeof(bbox_t));
    memcpy(swap_data_ptr->m_local_temp_net_cost,
           temp_net_cost,
           num_nets * sizeof(double));
    memcpy(swap_data_ptr->m_local_net_cost,
           net_cost,
           num_nets * sizeof(double));
} /* end of void iter_data_update_from_global_to_local_grid(int kthread_id... ) */

/* Update local sub-region(in a Region) horizontal and vertical seperately */
static void update_local_data_from_global(const int* region_x_boundary,
                                          const int* region_y_boundary,
                                          grid_tile_t**  local_grid,
                                          local_block_t* local_block,
                                          const int krow,
                                          const int kcol)
{
    /* update local grid information, first was horizontal direction,
     * then was vertical direction. */
    if (krow == 0 && kcol == 0) {
        /* update top of the strip(horizontal direction) */
        update_from_global_to_local_hori(local_block,
                                         local_grid,
                                         max(2, region_x_boundary[0] - 2),
                                         region_x_boundary[0] + 2,
                                         max(0, region_y_boundary[0] + 2),
                                         region_y_boundary[1] + 2);
        /* update left side of the strip(vertical direction) */
        update_from_global_to_local_vert(local_block,
                                         local_grid,
                                         max(0, region_x_boundary[0] - 2),
                                         region_x_boundary[1] + 2,
                                         max(2, region_y_boundary[0] - 2),
                                         region_y_boundary[0] + 2);
    } else if (krow == 0 && kcol == 1) {
        /* update top of the strip(horizontal direction) */
        update_from_global_to_local_hori(local_block,
                                         local_grid,
                                         max(2, region_x_boundary[0] - 2),
                                         region_x_boundary[0] + 2,
                                         region_y_boundary[1] + 2,
                                         min(num_grid_rows + 2,
                                             region_y_boundary[2] + 2));
        /* update right side of the strip(vertical direction) */
        update_from_global_to_local_vert(local_block,
                                         local_grid,
                                         max(0, region_x_boundary[0] - 2),
                                         region_x_boundary[1] + 2,
                                         region_y_boundary[2] - 2,
                                         min(num_grid_rows - 2,
                                             region_y_boundary[2] + 2));
    } else if (krow == 1 && kcol == 0) {
        /* update bottom of the strip(horizontal directions) */
        update_from_global_to_local_hori(local_block,
                                         local_grid,
                                         region_x_boundary[2] - 2,
                                         min(num_grid_columns - 2,
                                             region_x_boundary[2] + 2),
                                         max(0, region_y_boundary[0] + 2),
                                         region_y_boundary[1] + 2);
        /* update left side strip(vertical directions) */
        update_from_global_to_local_vert(local_block,
                                         local_grid,
                                         region_x_boundary[1] - 2,
                                         min(num_grid_columns + 2,
                                             region_x_boundary[2] + 2),
                                         max(0, region_y_boundary[0] - 2),
                                         region_y_boundary[0] + 2);
    } else if (krow == 1 && kcol == 1) {
        /* update bottom of the strip(horizontal direction) */
        update_from_global_to_local_hori(local_block,
                                         local_grid,
                                         region_x_boundary[2] - 2,
                                         min(num_grid_columns - 2,
                                             region_x_boundary[2] + 2),
                                         region_y_boundary[1] + 2,
                                         min(num_grid_columns + 2,
                                             region_y_boundary[2] + 2));
        /* update right side strip(vertical direction) */
        update_from_global_to_local_vert(local_block,
                                         local_grid,
                                         region_x_boundary[1] - 2,
                                         min(num_grid_columns + 2,
                                             region_x_boundary[2] + 2),
                                         region_y_boundary[2] - 2,
                                         min(num_grid_rows - 2,
                                             region_y_boundary[2] + 2));
    } else {
        printf("incorrect row,col combination in inner loop: (%d, %d)", krow, kcol);
    }
} /* end of void update_local_data_from_global(int* region_x_boundary) */


/* Update the temperature according to the annealing schedule selected. */
static void update_t_parallel(double* t,
                              int*  inner_iter_num,
                              double range_limit,
                              double success_rat,
                              annealing_sched_t annealing_sched)
{
    if (annealing_sched.type == USER_SCHED) {
        *t = annealing_sched.alpha_t * (*t);
    } else {
        /* AUTO_SCHED */
        *inner_iter_num = annealing_sched.inner_num;
        if (success_rat > 0.96) {
            *t = (*t) * 0.5;
        } else if (success_rat > 0.8) {
            *t = (*t) * 0.9;
        } else if (success_rat > 0.15 || range_limit > 1.) {
            *t = (*t) * 0.9;
            *inner_iter_num = annealing_sched.inner_num / 4;
        } else {
            *t = (*t) * 0.6;
            *inner_iter_num = annealing_sched.inner_num / 20;
        }
    }
} /* end of static void update_t_parallel(double* t,..) */

/* Puts a list of all the nets connected to from_block and to_block into  *
 * nets_to_update.  Returns the number of affected nets.  Net_block_moved *
 * is either FROM, TO or FROM_AND_TO -- the block connected to this net   *
 * that has moved.                                                        */
static int find_affected_nets_parallel(int* nets_to_update,
                                       int* net_block_moved,
                                       int from_block,
                                       int to_block,
                                       int num_of_pins,
                                       double* local_temp_net_cost)
{
    int affected_index = 0;
    int k, inet, count;
    for (k = 0; k < num_of_pins; ++k) {
        inet = block[from_block].nets[k];
        if (OPEN == inet || TRUE == net[inet].is_global) {
            continue;
        }
        /* This is here in case the same block connects to a net twice. */
        if (local_temp_net_cost[inet] > 0.0) {
            continue;
        }

        nets_to_update[affected_index] = inet;
        net_block_moved[affected_index] = FROM;
        ++affected_index;
        local_temp_net_cost[inet] = 1.0; /* Flag to say we've marked this net. */
    }  /* end of for (k = 0; k < num_of_pins; ++k) */

    if (to_block != EMPTY) {
        for (k = 0; k < num_of_pins; ++k) {
            inet = block[to_block].nets[k];
            if (OPEN == inet || TRUE == net[inet].is_global) {
                continue;
            }

            if (local_temp_net_cost[inet] > 0.0) {
                /* Net already marked. */
                for (count = 0; count < affected_index; ++count) {
                    if (nets_to_update[count] == inet) {
                        if (net_block_moved[count] == FROM) {
                            net_block_moved[count] = FROM_AND_TO;
                        }
                        break;
                    }
                }

#ifdef DEBUG
                if (count > affected_index) {
                    printf("Error in find_affected_nets -- count = %d,"
                     " affected index = %d.\n", count,
                     affected_index);
                    exit(1);
                }
#endif
            } else {
                /* Net not marked yet. */
                nets_to_update[affected_index] = inet;
                net_block_moved[affected_index] = TO;
                ++affected_index;
                local_temp_net_cost[inet] = 1.; /* Flag means we've  marked net. */
            }
        }
    }  /* end of if(to_block != NULL) */

    return affected_index;
}  /* end of static int find_affected_nets_parallel(int* nets_to_update,) */

static int assess_swap_parallel(double delta_c,
                                double t,
                                int local_seed)
{
    /* Returns: 1->move accepted, 0->rejected. */
    int accept = -1;
    if (delta_c <= 0) {
#ifdef SPEC  /* Reduce variation in final solution due to round off */
        fnum = my_frand_parallel(local_seed);
#endif
        accept = 1;
        return (accept);
    }

    if (t == 0.) {
        return (0);
    }

    double fnum = my_frand_parallel(local_seed);
    double prob_fac = exp(-delta_c / t);
    if (prob_fac > fnum) {
        accept = 1;
    } else {
        accept = 0;
    }

    return (accept);
}  /* end of int assess_swap_parallel(double delta_c,) */

static boolean find_to_block_parallel(int x_from,
                                      int y_from,
                                      block_type_ptr type,
                                      int* x_to,
                                      int* y_to,
                                      int kthread_id,
                                      int xmin, int xmax,
                                      int ymin, int ymax)
{
    /* Returns the location to which I want to swap within the range (xmin,ymin) & (xmax,ymax)*/
    int choice;
    do {
        /* Until (x_to, y_to) different from (x_from, y_from) */
        if (type == IO_TYPE) {
            /* io_block to be moved. */
            /*left bottom corner*/
            if (xmin == 0 && ymin == 0) {
                /*the target is either located on x = 0 or y = 0*/
                if (my_irand_parallel(1, kthread_id)) {
                    /*target is located on x = 0*/
                    *x_to = 0;
                    choice = my_irand_parallel(ymax - ymin, kthread_id);
                    *y_to = max(1, choice);
                } else {
                    /*target is located on y = 0*/
                    choice = my_irand_parallel(xmax - xmin, kthread_id);
                    *x_to = max(1, choice);
                    *y_to = 0;
                }
            } else if (xmax == num_grid_columns + 1 && ymin == 0) {
                /* For Bottom-Right corner*/
                /*the target is either located on x = 0 or y = 0*/
                if (my_irand_parallel(1, kthread_id)) {
                    /*target is located on x = num_grid_columns + 1*/
                    *x_to = num_grid_columns + 1;
                    choice = my_irand_parallel(ymax - ymin, kthread_id);
                    *y_to = max(1, choice);
                } else {
                    /*target is located on y = 0*/
                    choice = xmin + my_irand_parallel(xmax - xmin, kthread_id);
                    *x_to = min(num_grid_columns, choice);
                    *y_to = 0;
                }
            } else if (xmin == 0 && ymax == num_grid_rows + 1) {
                /* For Top-Left corner*/
                /*the target is either located on x = 0 or y = 0*/
                if (my_irand_parallel(1, kthread_id)) {
                    /*target is located on x = 0*/
                    *x_to = 0;
                    choice = ymin + my_irand_parallel(ymax - ymin, kthread_id);
                    *y_to = min(num_grid_rows, choice);
                } else {
                    /*target is located on y = num_grid_rows+1*/
                    choice = my_irand_parallel(xmax - xmin, kthread_id);
                    *x_to = max(1, choice);
                    *y_to = num_grid_rows + 1;
                }
            } else if (xmax == num_grid_columns + 1 && ymax == num_grid_rows + 1) {
                /* For Top-Right corner*/
                /*the target is either located on x = 0 or y = 0*/
                if (my_irand_parallel(1, kthread_id)) {
                    /*target is located on x = num_grid_columns + 1*/
                    *x_to = num_grid_columns + 1;
                    choice = ymin + my_irand_parallel(ymax - ymin, kthread_id);
                    *y_to = min(num_grid_rows, choice);
                } else {
                    /*target is located on y = 0*/
                    choice = xmin + my_irand_parallel(xmax - xmin, kthread_id);
                    *x_to = min(num_grid_columns, choice);
                    *y_to = num_grid_rows + 1;
                }
            } else if (xmin == 0 || xmax == num_grid_columns + 1) {
                *x_to = x_from;
                *y_to = ymin + my_irand_parallel(ymax - ymin, kthread_id);
            } else if (ymin == 0 || ymax == num_grid_rows + 1) {
                *x_to = xmin + my_irand_parallel(xmax - xmin, kthread_id);
                *y_to = y_from;
            } else {
                printf("came to a wrong loop\n");
                exit(-1);
            }

            assert(type == grid[*x_to][*y_to].type);
        } else {
            /* For other types except IO_TYPES */
            /* generate a {x_offset, y_offset} pairs that between (xmin, xmax) *
             * (ymin, ymax). */
            int x_offset = my_irand_parallel(xmax - xmin, kthread_id);
            int y_offset = my_irand_parallel(ymax - ymin, kthread_id);

            *x_to = x_offset + xmin;
            *y_to = y_offset + ymin;

            /* make sure the destination is not an IO */
            *x_to = max(1, *x_to);
            *x_to = min(num_grid_columns, *x_to);
            *y_to = max(1, *y_to);
            *y_to = min(num_grid_rows, *y_to);

            assert(*x_to >= 1 && *x_to <= num_grid_columns);
            assert(*y_to >= 1 && *y_to <= num_grid_rows);
        }
    } while ((x_from == *x_to) && (y_from == *y_to));

    assert(type == grid[*x_to][*y_to].type);
    return TRUE;
}  /* end of static boolean find_to_block_parallel(int x_from,) */

static double comp_td_point_to_point_delay_parallel(int inet,
                                                    int ipin,
                                                    local_block_t* local_block)
{
    /*returns the Tdel of one point to point connection */
    int source_block = net[inet].node_block[0];
    int sink_block = net[inet].node_block[ipin];

    block_type_ptr source_type = block[source_block].type;
    block_type_ptr sink_type = block[sink_block].type;
    assert(source_type != NULL && sink_type != NULL);

    int delta_x = abs(local_block[sink_block].x - local_block[source_block].x);
    int delta_y = abs(local_block[sink_block].y - local_block[source_block].y);

    /* TODO low priority: Could be merged into one look-up table */
    /* Note: This heuristic is terrible on Quality of Results.
     * A much better heuristic is to create a more comprehensive lookup table but
     * it's too late in the release cycle to do this.  Pushing until the next release */
    double delay_source_to_sink = 0.0;
    if (source_type == IO_TYPE) {
        if (sink_type == IO_TYPE) {
            delay_source_to_sink = delta_inpad_to_outpad[delta_x][delta_y];
        } else {
            delay_source_to_sink = delta_inpad_to_clb[delta_x][delta_y];
        }
    } else {
        if (sink_type == IO_TYPE) {
            delay_source_to_sink = delta_clb_to_outpad[delta_x][delta_y];
        } else {
            delay_source_to_sink = delta_clb_to_clb[delta_x][delta_y];
        }
    }

    if (delay_source_to_sink < 0) {
        printf
        ("Error in comp_td_point_to_point_delay in place.c, bad delay_source_to_sink value\n");
        exit(1);
    }

    if (delay_source_to_sink < 0.) {
        printf
        ("Error in comp_td_point_to_point_delay in place.c, Tdel is less than 0\n");
        exit(1);
    }

    return (delay_source_to_sink);
}


/*a net that is being driven by a moved block must have all of its  */
/*sink timing costs recomputed. A net that is driving a moved block */
/*must only have the timing cost on the connection driving the input_args */
/*pin computed */
static void compute_delta_td_cost_parallel(const int kfrom_block,
                                           const int kto_block,
                                           int num_of_pins,
                                           double* delta_timing,
                                           double* delta_delay,
                                           local_block_t* local_block,
                                           double** local_temp_point_to_point_timing_cost,
                                           double** local_temp_point_to_point_delay_cost)
{
    double delta_timing_cost = 0.0;
    double delta_delay_cost = 0.0;
    double temp_delay = 0.0;
    /* int inet, pin_index, net_pin, ipin; */
    int pin_index = -1;
    int ipin = 0;
    for (pin_index = 0; pin_index < num_of_pins; ++pin_index) {
        const int inet = block[kfrom_block].nets[pin_index];
        if (OPEN == inet || TRUE == net[inet].is_global) {
            continue;
        }

        const int net_pin = net_pin_index[kfrom_block][pin_index];
        if (net_pin != 0) {
        /* This net_pin is not a driver_pin, so this net drived a moved block. *
         * if this net is being driven by a block that has moved, we do not  *
         * need to compute the change in the timing cost (here) since it will *
         * be computed in the fanout of the net on the driving block, also  *
         * computing it here would double count the change, and mess up the  *
         * delta_timing_cost value */
            if (net[inet].node_block[0] != kto_block
                    && net[inet].node_block[0] != kfrom_block) {
            /* In this situation, the drivering node of this net neither *
             * kfrom_block nor kto_block */
                temp_delay = comp_td_point_to_point_delay_parallel(inet,
                                                                   net_pin,
                                                                   local_block);

                local_temp_point_to_point_delay_cost[inet][net_pin] = temp_delay;
                local_temp_point_to_point_timing_cost[inet][net_pin] =
                                  timing_place_crit[inet][net_pin] * temp_delay;

                delta_delay_cost +=
                            (local_temp_point_to_point_delay_cost[inet][net_pin]
                               - point_to_point_delay_cost[inet][net_pin]);
                delta_timing_cost +=
                    (local_temp_point_to_point_timing_cost[inet][net_pin]
                       - point_to_point_timing_cost[inet][net_pin]);
            }
        } else { /* net_pin == 0 */
        /* this net is being driven by a moved block, recompute *
         * all point to point connections on this net. */
            for (ipin = 1; ipin <= net[inet].num_sinks; ++ipin) {
                temp_delay = comp_td_point_to_point_delay_parallel(inet,
                                                                   ipin,
                                                                   local_block);
                local_temp_point_to_point_delay_cost[inet][ipin] = temp_delay;
                local_temp_point_to_point_timing_cost[inet][ipin] =
                                    timing_place_crit[inet][ipin] * temp_delay;

                delta_delay_cost +=
                    (local_temp_point_to_point_delay_cost[inet][ipin]
                       - point_to_point_delay_cost[inet][ipin]);
                delta_timing_cost +=
                    (local_temp_point_to_point_timing_cost[inet][ipin]
                       - point_to_point_timing_cost[inet][ipin]);
            }
        }
    }  /* end of for (pin_index = 0; pin_index < num_of_pins; ++pin_index) */

    /* end of kfrom_block, then consider the kto_block. It was similar with former program. */
    if (kto_block != EMPTY) {
        for (pin_index = 0; pin_index < num_of_pins; ++pin_index) {
            const int inet = block[kto_block].nets[pin_index];
            if (OPEN == inet || TRUE == net[inet].is_global) {
                continue;
            }
            /* just considering the nets connected to kto_block */
            const int net_pin = net_pin_index[kto_block][pin_index];
            if (net_pin != 0) {
            /* This net is driving moved kto_block! *
             * If this net is being driven by a block that has moved, we do not *
             * need to compute the change in the timing cost (here) since it was *
             * computed in the fanout of the net on the driving block, also    *
             * computing it here would double count the change, and mess up the *
             * delta_timing_cost value */
                if (net[inet].node_block[0] != kto_block
                        && net[inet].node_block[0] != kfrom_block) {
                    temp_delay = comp_td_point_to_point_delay_parallel(inet,
                                                                       net_pin,
                                                                       local_block);

                    local_temp_point_to_point_delay_cost[inet][net_pin] = temp_delay;
                    local_temp_point_to_point_timing_cost[inet][net_pin] =
                                timing_place_crit[inet][net_pin] * temp_delay;

                    delta_delay_cost +=
                        (local_temp_point_to_point_delay_cost[inet][net_pin]
                           - point_to_point_delay_cost[inet][net_pin]);
                    delta_timing_cost +=
                        (local_temp_point_to_point_timing_cost[inet][net_pin]
                           - point_to_point_timing_cost[inet][net_pin]);
                }
            } else {  /* net_pin was driver_pin, so kto_block was a driver block */
                /*this net is being driven by a moved block, recompute */
                /*all point to point connections on this net. */
                for (ipin = 1; ipin <= net[inet].num_sinks; ++ipin) {
                    temp_delay = comp_td_point_to_point_delay_parallel(inet,
                                                                       ipin,
                                                                       local_block);

                    local_temp_point_to_point_delay_cost[inet][ipin] = temp_delay;
                    local_temp_point_to_point_timing_cost[inet][ipin] =
                                timing_place_crit[inet][ipin] * temp_delay;

                    delta_delay_cost +=
                        (local_temp_point_to_point_delay_cost[inet][ipin]
                           - point_to_point_delay_cost[inet][ipin]);
                    delta_timing_cost +=
                        (local_temp_point_to_point_timing_cost[inet][ipin]
                          - point_to_point_timing_cost[inet][ipin]);
                }
            }  /* end of else(net_pin was a driver_pin) */
        }  /* end of for (pin_index = 0; pin_index < num_of_pins; ++pin_index) */
    }  /* end of if (kto_block != EMPTY) */

    *delta_timing = delta_timing_cost;
    *delta_delay = delta_delay_cost;
}  /* end of void compute_delta_td_cost_parallel(...) */

/*computes the cost (from scratch) due to the delays and criticalities*
 *on all point to point connections, we define the timing cost of     *
 *each connection as criticality * Tdel */
static unsigned long compute_td_costs_parallel_without_update_crit(double* timing_cost,
                                                                   double* connection_delay_sum,
                                                                   int start_net,
                                                                   int finish_net)
{
    double loc_timing_cost = 0.0;
    double loc_connection_delay_sum = 0.0;
    unsigned long local_work = 0;

    int inet = 0;
    for (inet = start_net; inet < finish_net; ++inet) {
        /* for each net ... */
        if (net[inet].is_global == FALSE) {
            /* Do only if not global. */
            const int sinks = net[inet].num_sinks;
            local_work += sinks;

            int ipin = 0;
            for (ipin = 1; ipin <= net[inet].num_sinks; ++ipin) {
                double temp_delay_cost = comp_td_point_to_point_delay(inet,
                                                                     ipin);
                double temp_timing_cost =
                    temp_delay_cost * timing_place_crit[inet][ipin];

                loc_connection_delay_sum += temp_delay_cost;
                point_to_point_delay_cost[inet][ipin] = temp_delay_cost;
                temp_point_to_point_delay_cost[inet][ipin] = -1; /*undefined */

                point_to_point_timing_cost[inet][ipin] = temp_timing_cost;
                temp_point_to_point_timing_cost[inet][ipin] = -1;   /*undefined */
                loc_timing_cost += temp_timing_cost;
            } /* for(ipin = 1; ipin <= net[inet].num_sinks; ++ipin) */
        }  /* end of if(net[inet].is_global == FALSE) */
    }  /* end of for(inet = start_net; inet < finish_net; ++inet) */

    *timing_cost = loc_timing_cost;
    *connection_delay_sum = loc_connection_delay_sum;

    return local_work;
}  /* end of static unsigned long compute_td_costs_parallel_without_update_crit(double* timing_cost,) */

/*computes the cost(from scratch) due to the delays and criticalities*
 *on all point to point connections, we define the timing cost of     *
 *each connection as criticality * Tdel */
static unsigned long compute_td_costs_parallel_with_update_crit(double* timing_cost,
                                                                double* connection_delay_sum,
                                                                double** net_slack,
                                                                double max_delay,
                                                                double crit_exponent,
                                                                int start_net,
                                                                int finish_net)
{   /* Just like compute_timing_driven_cost in VPR4.3_double */
    double loc_timing_cost = 0.0;
    double loc_connection_delay_sum = 0.0;
    unsigned long local_work = 0;
    int inet = 0;
    /* for each net between [start_net, finish_net)... */
    for (inet = start_net; inet < finish_net; ++inet) {
        if (!net[inet].is_global) {
            /* Do only if not global. */
            const int sinks = net[inet].num_sinks;
            local_work += sinks;

            int ipin = 0;
            for (ipin = 1; ipin <= sinks; ++ipin) {
                double pin_crit = max(1 - net_slack[inet][ipin] / max_delay,
                                     0.0);
                timing_place_crit[inet][ipin] = pow(pin_crit,
                                                    crit_exponent);
                double temp_delay_cost = comp_td_point_to_point_delay(inet,
                                                                     ipin);
                double temp_timing_cost =
                        temp_delay_cost * timing_place_crit[inet][ipin];

                loc_connection_delay_sum += temp_delay_cost;
                point_to_point_delay_cost[inet][ipin] = temp_delay_cost;
                temp_point_to_point_delay_cost[inet][ipin] = -1;

                point_to_point_timing_cost[inet][ipin] = temp_timing_cost;
                temp_point_to_point_timing_cost[inet][ipin] = -1;
                loc_timing_cost += temp_timing_cost;
            } /* end of for (ipin = 1; ipin <= sinks; ++ipin) */
        } /* if (!net[inet].is_global) */
    } /* end of for (inet = start_net; inet < finish_net; ++inet) */

    *timing_cost = loc_timing_cost;
    *connection_delay_sum = loc_connection_delay_sum;

    return local_work;
}  /* end of static unsigned long compute_td_costs_parallel_with_update_crit(double* timing_cost,) */

/* Finds the cost from scratch.  Done only when the placement   *
 * has been radically changed (i.e. after initial placement).   *
 * Otherwise find the cost change incrementally.  If method     *
 * check is NORMAL, we find bounding boxes that are updateable  *
 * for the larger nets.  If method is CHECK, all bounding boxes *
 * are found via the non_updateable_bb routine, to provide a    *
 * cost which can be used to check the correctness of the       *
 * other routine.                                               */
static double compute_bb_cost_parallel(const int kstart_net,
                                       const int kend_net)
{
    double cost = 0;
    int k = 0;
    for (k = kstart_net; k < kend_net; ++k) {
        /* for each net ... */
        if (net[k].is_global == FALSE) {
            /* Do only if not global. */
            /* Small nets don't use incremental updating on their bounding boxes, *
             * so they can use a fast bounding box calculator.                    */
            get_non_updateable_bb(k, &bb_coords[k]);

            net_cost[k] = get_net_cost(k, &bb_coords[k]);
            cost += net_cost[k];
        }
    }
    return cost;
}  /* end of static double compute_bb_cost_parallel(int start_net,) */

/* This routine finds the bounding box of each net from scratch (i.e.    *
 * from only the block location information).  It updates both the       *
 * coordinate and number of blocks on each tedge information.  It        *
 * should only be called when the bounding box information is not valid. */
static void get_bb_from_scratch_parallel(int inet,
                                         bbox_t* coords,
                                         bbox_t* num_on_edges,
                                         local_block_t* local_block)
{
    /* I need a list of blocks to which this net connects, with no block listed *
     * more than once, in order to get a proper count of the number on the tedge *
     * of the bounding box.                                                     */
    int n_pins = -1;
    int* plist = NULL;
    if (duplicate_pins[inet] == 0) {
        plist = net[inet].node_block;
        n_pins = net[inet].num_sinks + 1;
    } else {
        plist = unique_pin_list[inet];
        n_pins = (net[inet].num_sinks + 1) - duplicate_pins[inet];
    }

    int x = local_block[plist[0]].x;
    int y = local_block[plist[0]].y;
    x = max(min(x, num_grid_columns), 1);
    y = max(min(y, num_grid_rows), 1);

    int xmin = x;
    int ymin = y;
    int xmax = x;
    int ymax = y;
    int xmin_edge = 1;
    int ymin_edge = 1;
    int xmax_edge = 1;
    int ymax_edge = 1;

    int ipin = -1;
    int block_num = 0;
    for (ipin = 1; ipin < n_pins; ++ipin) {
        block_num = plist[ipin];
        x = local_block[block_num].x;
        y = local_block[block_num].y;

        /* Code below counts IO blocks as being within the 1..num_grid_columns,
         * 1..num_grid_rows clb array. *
         * This is because channels do not go out of the 0..num_grid_columns,  *
         * 0..num_grid_rows range, and I always take all channels impinging on *
         * the bounding box to be within that bounding box. Hence, this      *
         * "movement" of IO blocks does not affect the which channels are    *
         * included within the bounding box, and it simplifies the code a lot.*/
        x = max(min(x, num_grid_columns), 1);
        y = max(min(y, num_grid_rows), 1);

        if (x == xmin) {
            ++xmin_edge;
        }
        if (x == xmax) {
            /* Recall that xmin could equal xmax -- don't use else */
            ++xmax_edge;
        } else if (x < xmin) {
            xmin = x;
            xmin_edge = 1;
        } else if (x > xmax) {
            xmax = x;
            xmax_edge = 1;
        }

        if (y == ymin) {
            ++ymin_edge;
        }
        if (y == ymax) {
            ++ymax_edge;
        } else if (y < ymin) {
            ymin = y;
            ymin_edge = 1;
        } else if (y > ymax) {
            ymax = y;
            ymax_edge = 1;
        }
    } /*end of for (ipin = 1; ipin < num_pins; ++ipin) */

    /* Copy the coordinates and number on edges information into the proper *
     * structures.                                                          */
    coords->xmin = xmin;
    coords->xmax = xmax;
    coords->ymin = ymin;
    coords->ymax = ymax;

    num_on_edges->xmin = xmin_edge;
    num_on_edges->xmax = xmax_edge;
    num_on_edges->ymin = ymin_edge;
    num_on_edges->ymax = ymax_edge;
}  /* end of void get_bb_from_scratch_parallel(int inet,...)  */

/* Finds the bounding-box of a net and stores its coordinates in the *
 * bb_coord_new data structure. This routine should only be called   *
 * for small nets, since it does not determine enough information for *
 * the bounding-box to be updated incrementally later.                *
 * Currently assumes channels on both sides of the CLBs forming the  *
 * edges of the bounding box can be used. Essentially, I am assuming *
 * the pins always lie on the outside of the bounding box.           */
static void get_non_updateable_bb_parallel(int inet,
                                    bbox_t* bb_coord_new_ptr,
                                    local_block_t* local_block_ptr)
{
    /* current (x,y) was coordinate of inet's driver block */
    int x = local_block_ptr[net[inet].node_block[0]].x;
    int y = local_block_ptr[net[inet].node_block[0]].y;

    /* [xmin, xmax] and [ymin, ymax] record the bounding-box boundary of inet */
    int xmin = x;
    int ymin = y;
    int xmax = x;
    int ymax = y;

    int k = 0;
    for (k = 1; k < (net[inet].num_sinks + 1); ++k) {
        x = local_block_ptr[net[inet].node_block[k]].x;
        y = local_block_ptr[net[inet].node_block[k]].y;

        if (x < xmin) {
            xmin = x;
        } else if (x > xmax) {
            xmax = x;
        }

        if (y < ymin) {
            ymin = y;
        } else if (y > ymax) {
            ymax = y;
        }
    }

    /* Now I've found the coordinates of the bounding box. There are no  *
     * channels beyond num_grid_columns and num_grid_rows, so I want to clip *
     * to that. As well, since I'll always include the channel immediately *
     * below and the channel immediately to the left of the bounding box, *
     * I want to clip to 1 in both directions as well(since minimum channel *
     * index is 0). See route.c for a channel diagram.                     */
    bb_coord_new_ptr->xmin = max(min(xmin, num_grid_columns), 1);
    bb_coord_new_ptr->ymin = max(min(ymin, num_grid_rows), 1);
    bb_coord_new_ptr->xmax = max(min(xmax, num_grid_columns), 1);
    bb_coord_new_ptr->ymax = max(min(ymax, num_grid_rows), 1);
}  /* end of static void get_non_updateable_bb_parallel(int inet, ) */


/* Updates the bounding-box of a net by storing its coordinates in   *
 * the bb_coord_new data structure and the number of blocks on each  *
 * tedge in the bb_edge_new data structure. This routine should only *
 * be called for large nets(sinks > 4), since it has some overhead   *
 * relative to just doing a brute-force bounding box calculation. The *
 * bounding-box coordinate and tedge information for inet must be     *
 * valid before this routine is called.                               *
 * Currently assumes channels on both sides of the CLBs forming the  *
 * edges of the bounding-box can be used. Essentially, I am assuming *
 * the pins always lie on the outside of the bounding box.           */
static void update_bb_parallel(int inet,
                               bbox_t* local_bb_coord_ptr,
                               bbox_t* local_bb_edge_ptr,
                               bbox_t* bb_coord_new_ptr,
                               bbox_t* bb_edge_new_ptr,
                               int xold, int yold,
                               int xnew, int ynew,
                               local_block_t* local_block)
{
    /* IO blocks are considered to be one cell in for simplicity. */
    xnew = max(min(xnew, num_grid_columns), 1);
    ynew = max(min(ynew, num_grid_rows), 1);
    xold = max(min(xold, num_grid_columns), 1);
    yold = max(min(yold, num_grid_rows), 1);

    /*======   First update x-coordinate, then update y-coordinate like ======*
     *======     x-coordinate                                           ======*/
    /* Check if I can update the bounding box incrementally. */
    if (xnew < xold) {
        /* (1) Move to left. */
        /* Update the xmax fields for coordinates and number of edges first. */
        if (xold == local_bb_coord_ptr[inet].xmax) {
            /* Old position at xmax. */
            if (local_bb_edge_ptr[inet].xmax == 1) {
                get_bb_from_scratch_parallel(inet,
                                             bb_coord_new_ptr,
                                             bb_edge_new_ptr,
                                             local_block);
                return;
            } else {
                bb_edge_new_ptr->xmax = local_bb_edge_ptr[inet].xmax - 1;
                bb_coord_new_ptr->xmax = local_bb_coord_ptr[inet].xmax;
            }
        } else {
            /* Move to left, old postion was not at xmax. */
            bb_coord_new_ptr->xmax = local_bb_coord_ptr[inet].xmax;
            bb_edge_new_ptr->xmax = local_bb_edge_ptr[inet].xmax;
        }

        /* Now do the xmin fields for coordinates and number of edges. */
        if (xnew < local_bb_coord_ptr[inet].xmin) {
            /* Moved past xmin */
            bb_coord_new_ptr->xmin = xnew;
            bb_edge_new_ptr->xmin = 1;
        } else if (xnew == local_bb_coord_ptr[inet].xmin) {
            /* Moved to xmin */
            bb_coord_new_ptr->xmin = xnew;
            bb_edge_new_ptr->xmin = local_bb_edge_ptr[inet].xmin + 1;
        } else {
            /* Xmin unchanged. */
            bb_coord_new_ptr->xmin = local_bb_coord_ptr[inet].xmin;
            bb_edge_new_ptr->xmin = local_bb_edge_ptr[inet].xmin;
        }
        /* End of move to left case. */
    } else if (xnew > xold) {
        /* Move to right. */
        /* Update the xmin fields for coordinates and number of edges first. */
        if (xold == local_bb_coord_ptr[inet].xmin) {
            /* Old position at xmin. */
            if (local_bb_edge_ptr[inet].xmin == 1) {
                get_bb_from_scratch_parallel(inet,
                                             bb_coord_new_ptr,
                                             bb_edge_new_ptr,
                                             local_block);
                return;
            } else {
                bb_edge_new_ptr->xmin = local_bb_edge_ptr[inet].xmin - 1;
                bb_coord_new_ptr->xmin = local_bb_coord_ptr[inet].xmin;
            }
        } else {
            /* Move to right, old position was not at xmin. */
            bb_coord_new_ptr->xmin = local_bb_coord_ptr[inet].xmin;
            bb_edge_new_ptr->xmin = local_bb_edge_ptr[inet].xmin;
        }

        /* Now do the xmax fields for coordinates and number of edges. */
        if (xnew > local_bb_coord_ptr[inet].xmax) {
            /* Moved past xmax. */
            bb_coord_new_ptr->xmax = xnew;
            bb_edge_new_ptr->xmax = 1;
        } else if (xnew == local_bb_coord_ptr[inet].xmax) {
            /* Moved to xmax */
            bb_coord_new_ptr->xmax = xnew;
            bb_edge_new_ptr->xmax = local_bb_edge_ptr[inet].xmax + 1;
        } else {
            /* Xmax unchanged. */
            bb_coord_new_ptr->xmax = local_bb_coord_ptr[inet].xmax;
            bb_edge_new_ptr->xmax = local_bb_edge_ptr[inet].xmax;
        }
    /* End of move to right case. */
    } else {
        /* xnew == xold -- no x motion. */
        bb_coord_new_ptr->xmin = local_bb_coord_ptr[inet].xmin;
        bb_coord_new_ptr->xmax = local_bb_coord_ptr[inet].xmax;
        bb_edge_new_ptr->xmin = local_bb_edge_ptr[inet].xmin;
        bb_edge_new_ptr->xmax = local_bb_edge_ptr[inet].xmax;
    }

    /* Now account for the y-direction motion. */
    if (ynew < yold) {
        /* Move down. */
        /* Update the ymax fields for coordinates and number of edges first. */
        if (yold == local_bb_coord_ptr[inet].ymax) {
            /* Old position at ymax. */
            if (local_bb_edge_ptr[inet].ymax == 1) {
                get_bb_from_scratch_parallel(inet,
                                             bb_coord_new_ptr,
                                             bb_edge_new_ptr,
                                             local_block);
                return;
            } else {
                bb_edge_new_ptr->ymax = local_bb_edge_ptr[inet].ymax - 1;
                bb_coord_new_ptr->ymax = local_bb_coord_ptr[inet].ymax;
            }
        } else {
            /* Move down, old postion was not at ymax. */
            bb_coord_new_ptr->ymax = local_bb_coord_ptr[inet].ymax;
            bb_edge_new_ptr->ymax = local_bb_edge_ptr[inet].ymax;
        }

        /* Now do the ymin fields for coordinates and number of edges. */
        if (ynew < local_bb_coord_ptr[inet].ymin) {
            /* Moved past ymin */
            bb_coord_new_ptr->ymin = ynew;
            bb_edge_new_ptr->ymin = 1;
        } else if (ynew == local_bb_coord_ptr[inet].ymin) {
            /* Moved to ymin */
            bb_coord_new_ptr->ymin = ynew;
            bb_edge_new_ptr->ymin = local_bb_edge_ptr[inet].ymin + 1;
        } else {
            /* ymin unchanged. */
            bb_coord_new_ptr->ymin = local_bb_coord_ptr[inet].ymin;
            bb_edge_new_ptr->ymin = local_bb_edge_ptr[inet].ymin;
        }
    /* End of Move Down case. */
    } else if (ynew > yold) {
        /* Moved up. */
        /* Update the ymin fields for coordinates and number of edges first. */
        if (yold == local_bb_coord_ptr[inet].ymin) {
            /* Old position at ymin. */
            if (local_bb_edge_ptr[inet].ymin == 1) {
                get_bb_from_scratch_parallel(inet,
                                             bb_coord_new_ptr,
                                             bb_edge_new_ptr,
                                             local_block);
                return;
            } else {
                bb_edge_new_ptr->ymin = local_bb_edge_ptr[inet].ymin - 1;
                bb_coord_new_ptr->ymin = local_bb_coord_ptr[inet].ymin;
            }
        } else {
            /* Moved up, old position was not at ymin. */
            bb_coord_new_ptr->ymin = local_bb_coord_ptr[inet].ymin;
            bb_edge_new_ptr->ymin = local_bb_edge_ptr[inet].ymin;
        }

        /* Now do the ymax fields for coordinates and number of edges. */
        if (ynew > local_bb_coord_ptr[inet].ymax) {
            /* Moved past ymax. */
            bb_coord_new_ptr->ymax = ynew;
            bb_edge_new_ptr->ymax = 1;
        } else if (ynew == local_bb_coord_ptr[inet].ymax) {
            /* Moved to ymax */
            bb_coord_new_ptr->ymax = ynew;
            bb_edge_new_ptr->ymax = local_bb_edge_ptr[inet].ymax + 1;
        } else {
            /* ymax unchanged. */
            bb_coord_new_ptr->ymax = local_bb_coord_ptr[inet].ymax;
            bb_edge_new_ptr->ymax = local_bb_edge_ptr[inet].ymax;
        }
        /* End of move up case. */
    } else {
        /* ynew == yold -- no y motion. */
        bb_coord_new_ptr->ymin = local_bb_coord_ptr[inet].ymin;
        bb_coord_new_ptr->ymax = local_bb_coord_ptr[inet].ymax;
        bb_edge_new_ptr->ymin = local_bb_edge_ptr[inet].ymin;
        bb_edge_new_ptr->ymax = local_bb_edge_ptr[inet].ymax;
    }
}  /* end of static void update_bb_parallel(int inet,...) */

/* I insisted that the most important parameter was the kthread_id */
/* try swap a pair of blocks in each Extend-SubRegion by each thread parallely */
static int  try_swap_parallel(const double kt,
                              const int kthread_id,
                              const int kx_from,
                              const int ky_from,
                              const int kz_from,
                              const int krow,
                              const int kcol,
                              thread_local_common_paras_t*  common_paras_ptr,
                              thread_local_data_for_swap_t* swap_data_ptr)
{
    /* the flow of try_swap_parallel was that: (1) find to_block, it will swap *
     * with to_block */
    /*constrain the swap region*/
    const double krange_limit = common_paras_ptr->local_range_limit;
    int limit = min(10, (int)krange_limit);

    const int* kregion_x_boundary = common_paras_ptr->local_region_x_boundary;
    const int* kregion_y_boundary = common_paras_ptr->local_region_y_boundary;
    int x_min = max(kregion_x_boundary[krow] - 2, 0);
    int x_max = min(kregion_x_boundary[krow + 1] + 1,
                    num_grid_columns + 1);
    int y_min = max(kregion_y_boundary[kcol] - 2, 0);
    int y_max = min(kregion_y_boundary[kcol + 1] + 1,
                    num_grid_rows + 1);

    x_min = max(x_min, kx_from - limit);
    x_max = min(x_max, kx_from + limit);
    y_min = max(y_min, ky_from - limit);
    y_max = min(y_max, ky_from + limit);

    /* First, find a block within the current swap region*/
    double delay_delta_c = 0.0;
    int x_to = 0;
    int y_to = 0;
    grid_tile_t** local_grid = swap_data_ptr->m_local_grid;
    const int kfrom_block = local_grid[kx_from][ky_from].blocks[kz_from];
    find_to_block_parallel(kx_from,
                           ky_from,
                           block[kfrom_block].type,
                           &x_to,
                           &y_to,
                           kthread_id,
                           x_min,
                           x_max,
                           y_min,
                           y_max);
    /* Ensure that from_block and to_block are same type */
    assert(local_grid[kx_from][ky_from].type == local_grid[x_to][y_to].type);

    /* then find a location for to_grid tile */
    int z_to = 0;
    if (local_grid[x_to][y_to].type->capacity > 1) {
        int to_grid_capacity = local_grid[x_to][y_to].type->capacity - 1;
        z_to = my_irand_parallel(to_grid_capacity,
                                 kthread_id);
    }

    /* Second, swap the from_block and to_block location */
    /* The Swap a pair of locations */
    local_block_t*  local_block = swap_data_ptr->m_local_block;
    int to_block = EMPTY;
    if (local_grid[x_to][y_to].blocks[z_to] == EMPTY) {
        /* Moving from_block to an empty location */
        to_block = EMPTY;
        local_block[kfrom_block].x = x_to;
        local_block[kfrom_block].y = y_to;
        local_block[kfrom_block].z = z_to;
    } else {
        /* Swapping two non-empty location */
        to_block = local_grid[x_to][y_to].blocks[z_to];
        local_block[to_block].x = kx_from;
        local_block[to_block].y = ky_from;
        local_block[to_block].z = kz_from;

        local_block[kfrom_block].x = x_to;
        local_block[kfrom_block].y = y_to;
        local_block[kfrom_block].z = z_to;
    }

    /*------------------------  Third, compute the swap cost ----------------*/
    /* Change in cost due to this swap. */
    double bb_delta_c = 0;
    int num_of_pins = block[kfrom_block].type->num_pins;
    /* (3.1). I must found out the affected nets, it restored in *
     * int* nets_to_update array. */
    int* nets_to_update = swap_data_ptr->m_nets_to_update;
    int* net_block_moved = swap_data_ptr->m_net_block_moved;
    double* local_temp_net_cost = swap_data_ptr->m_local_temp_net_cost;
    const int knum_nets_affected = find_affected_nets_parallel(nets_to_update,
                                                               net_block_moved,
                                                               kfrom_block,
                                                               to_block,
                                                               num_of_pins,
                                                               local_temp_net_cost);
    /* (3.2) compute wirelength-cost parallel */
    int bb_index = 0;
    int k = -1;
    bbox_t*  bb_coord_new = swap_data_ptr->m_bb_coord_new;
    bbox_t*  bb_edge_new = swap_data_ptr->m_bb_edge_new;

    bbox_t*  local_bb_coord = swap_data_ptr->m_local_bb_coord;
    bbox_t*  local_bb_edge = swap_data_ptr->m_local_bb_edge;

    double*  local_net_cost = swap_data_ptr->m_local_net_cost;
    const place_cong_types_t kplace_cost_type =
        common_paras_ptr->local_placer_opts.place_cost_type;
    for (k = 0; k < knum_nets_affected; ++k) {
        int inet = nets_to_update[k];
        if (net_block_moved[k] == FROM_AND_TO) {
            continue;
        }

        if (net[inet].num_sinks < SMALL_NET) {
            get_non_updateable_bb_parallel(inet,
                                           &bb_coord_new[bb_index],
                                           local_block);
        } else {
            if (net_block_moved[k] == FROM) {
                update_bb_parallel(inet,
                                   local_bb_coord,
                                   local_bb_edge,
                                   &bb_coord_new[bb_index],
                                   &bb_edge_new[bb_index],
                                   kx_from, ky_from,
                                   x_to, y_to,
                                   local_block);
            } else {
                update_bb_parallel(inet,
                                   local_bb_coord,
                                   local_bb_edge,
                                   &bb_coord_new[bb_index],
                                   &bb_edge_new[bb_index],
                                   x_to, y_to,
                                   kx_from, ky_from,
                                   local_block);
            }
        }  /* end of else(num_sinks >= SMALL_NET) */

        if (kplace_cost_type != NONLINEAR_CONG) {
            local_temp_net_cost[inet] = get_net_cost(inet,
                                                     &bb_coord_new[bb_index]);
            bb_delta_c += (local_temp_net_cost[inet] - local_net_cost[inet]);
        } else {
            printf("can't do nonlinear_cong\n");
            exit(-1);
        }

        ++bb_index;
    } /* end of for(k = 0; k < num_nets_affected; ++k) */

    /* (3.3) Calcualte the timing_cost parallel */
    double timing_delta_c = 0.0;
    double delta_c = 0.0;
    const place_algorithm_t kplace_algorithm =
        common_paras_ptr->local_placer_opts.place_algorithm;
    const double ktiming_tradeoff =
        common_paras_ptr->local_placer_opts.timing_tradeoff;
    const double kinverse_prev_bb_cost =
        common_paras_ptr->local_inverse_prev_bb_cost;
    const double kinverse_prev_timing_cost =
        common_paras_ptr->local_inverse_prev_timing_cost;
    double**  local_temp_point_to_point_timing_cost =
        swap_data_ptr->m_local_temp_point_to_point_timing_cost;
    double**  local_temp_point_to_point_delay_cost =
        swap_data_ptr->m_local_temp_point_to_point_delay_cost;
    if (kplace_algorithm == NET_TIMING_DRIVEN_PLACE ||
            kplace_algorithm == PATH_TIMING_DRIVEN_PLACE) {
        /* When compute timing_driven_cost(no matteer parallel or sequential),
         * It should distinguish the driver_pin and not driver_pins */
        compute_delta_td_cost_parallel(kfrom_block,
                                       to_block,
                                       num_of_pins,
                                       &timing_delta_c,
                                       &delay_delta_c,
                                       local_block,
                                       local_temp_point_to_point_timing_cost,
                                       local_temp_point_to_point_delay_cost);

        const double knormal_bb_cost = bb_delta_c * kinverse_prev_bb_cost;
        const double knormal_timing_cost = timing_delta_c * kinverse_prev_timing_cost;
        delta_c = (1 - ktiming_tradeoff) * knormal_bb_cost +
                       ktiming_tradeoff * knormal_timing_cost;
    } else {
        delta_c = bb_delta_c;
    }

    /* Forth, After calcuate placement cost, then assess swap *
     * 1->move accepted, 0->rejected.                  */
    int keep_switch = assess_swap_parallel(delta_c,
                                           kt,
                                           kthread_id);
    if (keep_switch) {
        /* Swap successful! first update cost value. */
        common_paras_ptr->local_total_cost += delta_c;
        common_paras_ptr->local_bb_cost +=  bb_delta_c;
        if (kplace_algorithm == NET_TIMING_DRIVEN_PLACE
                || kplace_algorithm == PATH_TIMING_DRIVEN_PLACE) {
            common_paras_ptr->local_timing_cost += timing_delta_c;
            common_paras_ptr->local_delay_cost += delay_delta_c;
        }

        /* then update affected nets */
        bb_index = 0;
        for (k = 0; k < knum_nets_affected; ++k) {
            int inet = nets_to_update[k];
            if (net_block_moved[k] == FROM_AND_TO) {
                local_temp_net_cost[inet] = -1;
                continue;
            }

            local_bb_coord[inet] = bb_coord_new[bb_index];
            if (net[inet].num_sinks >= SMALL_NET) {
                local_bb_edge[inet] = bb_edge_new[bb_index];
            }
            ++bb_index;

            local_net_cost[inet] = local_temp_net_cost[inet];
            local_temp_net_cost[inet] = -1;
        }

        local_grid[x_to][y_to].blocks[z_to] = kfrom_block;
        local_grid[kx_from][ky_from].blocks[kz_from] = to_block;

        if (EMPTY == to_block) {
            ++(local_grid[x_to][y_to].usage);
            --(local_grid[kx_from][ky_from].usage);
        }
    } else {
        for (k = 0; k < knum_nets_affected; ++k) {
            int inet = nets_to_update[k];
            local_temp_net_cost[inet] = -1;
        }

        local_block[kfrom_block].x = kx_from;
        local_block[kfrom_block].y = ky_from;
        local_block[kfrom_block].z = kz_from;

        if (to_block != EMPTY) {
            local_block[to_block].x = x_to;
            local_block[to_block].y = y_to;
            local_block[to_block].z = z_to;
        }
    }  /* end of else(swap failure!) */

    return keep_switch;
} /* end of static void try_swap_parallel(double t,) */

/*update the global variables from local variables*/
static void update_from_local_to_global(local_block_t* local_block,
                                        grid_tile_t** local_grid,
                                        int x_start, int x_end,
                                        int y_start, int y_end)
{
    int x, y, z, block_moved;
    for (x = x_start; x < x_end; ++x) {
        for (y = y_start; y < y_end; ++y) {
            if (local_grid[x][y].type != EMPTY_TYPE) {
                if (y_end - y < 5 || y - y_start < 5) {
                    localvert_grid[y][x].usage = local_grid[x][y].usage;
                }

                grid[x][y].usage = local_grid[x][y].usage;
                for (z = 0; z < grid[x][y].type->capacity; ++z) {
                    if (local_grid[x][y].blocks[z] != grid[x][y].blocks[z]) {
                        /*block has been moved*/
                        block_moved = local_grid[x][y].blocks[z];

                        //if (y_end - y < 5 && y_end != num_grid_rows + 1 || y - y_start < 5 && y_start != 0)
                        if (y_end - y < 5 || y - y_start < 5) {
                            localvert_grid[y][x].blocks[z] = block_moved;
                        }

                        grid[x][y].blocks[z] = block_moved;

                        /*if the location becomes empty, don't worry about the rest*/
                        if (block_moved == -1) {
                            continue;
                        }

                        block[block_moved].x = local_block[block_moved].x;
                        block[block_moved].y = local_block[block_moved].y;
                        block[block_moved].z = local_block[block_moved].z;
                    } /* end of if(local_grid[x][y].blocks[z] != grid[x][y].blocks[z]) */
                } /* end of for(z = 0; z < grid[x][y].type->capacity; ++z) */
            } /* end of if(local_grid[x][y].type != EMPTY_TYPE) */
        } /* end of for(y = y_start; y < y_end; ++y) */
    } /* end of for(x = x_start; x < x_end; ++x) */
}  /* end of void update_from_local_to_global(local_block_t* local_block,) */

static void update_from_global_to_local_hori(local_block_t* local_block,
                                             grid_tile_t** local_grid,
                                             int x_start, int x_end,
                                             int y_start, int y_end)
{
    int x, y, z, block_moved;
    for (x = x_start; x < x_end; ++x) {
        for (y = y_start; y < y_end; ++y) {
            if (grid[x][y].type != EMPTY_TYPE) {
                local_grid[x][y].usage = grid[x][y].usage;

                for (z = 0; z < grid[x][y].type->capacity; ++z) {
                    if (grid[x][y].blocks[z] != local_grid[x][y].blocks[z]) {
                        //block has been moved
                        block_moved = grid[x][y].blocks[z];
                        local_grid[x][y].blocks[z] = block_moved;

                        if (block_moved == -1) {
                            continue;
                        }

                        local_block[block_moved].x = block[block_moved].x;
                        local_block[block_moved].y = block[block_moved].y;
                        local_block[block_moved].z = block[block_moved].z;
                    }
                }
            }
        }
    }
} /* end of static void update_from_global_to_local_hori(local_block_t* local_block,) */

static void update_from_global_to_local_vert(local_block_t* local_block,
                                             grid_tile_t** local_grid,
                                             int x_start, int x_end,
                                             int y_start, int y_end)
{
    int x, y, z, block_moved;
    for (y = y_start; y < y_end; ++y) {
        for (x = x_start; x < x_end; ++x) {
            if (localvert_grid[y][x].type != EMPTY_TYPE) {
                local_grid[x][y].usage = localvert_grid[y][x].usage;

                for (z = 0; z < localvert_grid[y][x].type->capacity; ++z) {
                    if (localvert_grid[y][x].blocks[z] != local_grid[x][y].blocks[z]) {
                        //block has been moved
                        block_moved = localvert_grid[y][x].blocks[z];
                        local_grid[x][y].blocks[z] = block_moved;

                        if (block_moved == -1) {
                            continue;
                        }

                        local_block[block_moved].x = block[block_moved].x;
                        local_block[block_moved].y = block[block_moved].y;
                        local_block[block_moved].z = block[block_moved].z;
                    }
                }
            }
        }
    }
}  /* end of static void update_from_global_to_local_vert(local_block_t* local_block,) */

static void update_from_global_to_local_grid_only(grid_tile_t** local_grid,
                                                  const int x_start,
                                                  const int x_end,
                                                  const int y_start,
                                                  const int y_end)
{
    int x, y, z;
    for (x = x_start; x < x_end; ++x) {
        for (y = y_start; y < y_end; ++y) {
            if (grid[x][y].type != EMPTY_TYPE) {
                local_grid[x][y].usage = grid[x][y].usage;

                for (z = 0; z < grid[x][y].type->capacity; ++z) {
                    if (grid[x][y].blocks[z] != local_grid[x][y].blocks[z]) {
                        //block has been moved
                        local_grid[x][y].blocks[z] = grid[x][y].blocks[z];
                    }
                }
            }
        }
    }
} /* end of void update_from_global_to_local_grid_only(grid_tile_t** local_grid,) */

static double my_difftime2(struct timeval* start,
                           struct timeval* end)
{
    long usec = end->tv_usec - start->tv_usec;
    long sec = end->tv_sec - start->tv_sec;
    if (usec < 0) {
        sec--;
        usec += 1000000;
    }

    double ret = (double)sec;
    ret += (double)usec / 1000000;

    return ret;
}
